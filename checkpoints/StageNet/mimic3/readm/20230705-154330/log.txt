2023-07-05 15:43:30 StageNet(
  (embeddings): ModuleDict(
    (conditions): Embedding(4031, 128, padding_idx=0)
    (procedures): Embedding(1276, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (stagenet): ModuleDict(
    (conditions): StageNetLayer(
      (kernel): Linear(in_features=129, out_features=1542, bias=True)
      (recurrent_kernel): Linear(in_features=385, out_features=1542, bias=True)
      (nn_scale): Linear(in_features=384, out_features=64, bias=True)
      (nn_rescale): Linear(in_features=64, out_features=384, bias=True)
      (nn_conv): Conv1d(384, 384, kernel_size=(10,), stride=(1,))
      (nn_dropconnect): Dropout(p=0.3, inplace=False)
      (nn_dropconnect_r): Dropout(p=0.3, inplace=False)
      (nn_dropout): Dropout(p=0.3, inplace=False)
      (nn_dropres): Dropout(p=0.3, inplace=False)
    )
    (procedures): StageNetLayer(
      (kernel): Linear(in_features=129, out_features=1542, bias=True)
      (recurrent_kernel): Linear(in_features=385, out_features=1542, bias=True)
      (nn_scale): Linear(in_features=384, out_features=64, bias=True)
      (nn_rescale): Linear(in_features=64, out_features=384, bias=True)
      (nn_conv): Conv1d(384, 384, kernel_size=(10,), stride=(1,))
      (nn_dropconnect): Dropout(p=0.3, inplace=False)
      (nn_dropconnect_r): Dropout(p=0.3, inplace=False)
      (nn_dropout): Dropout(p=0.3, inplace=False)
      (nn_dropres): Dropout(p=0.3, inplace=False)
    )
  )
  (fc): Linear(in_features=768, out_features=1, bias=True)
)
2023-07-05 15:43:30 Metrics: ['accuracy', 'pr_auc', 'roc_auc', 'f1']
2023-07-05 15:43:30 Device: cuda
2023-07-05 15:43:30 
2023-07-05 15:43:30 Training:
2023-07-05 15:43:30 Batch size: 32
2023-07-05 15:43:30 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-05 15:43:30 Optimizer params: {'lr': 0.001}
2023-07-05 15:43:30 Weight decay: 0.0
2023-07-05 15:43:30 Max grad norm: None
2023-07-05 15:43:30 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd720c69d50>
2023-07-05 15:43:30 Monitor: roc_auc
2023-07-05 15:43:30 Monitor criterion: max
2023-07-05 15:43:30 Epochs: 20
2023-07-05 15:43:30 
2023-07-05 15:43:38 --- Train epoch-0, step-247 ---
2023-07-05 15:43:38 loss: 0.6767
2023-07-05 15:43:38 --- Eval epoch-0, step-247 ---
2023-07-05 15:43:38 accuracy: 0.6043
2023-07-05 15:43:38 pr_auc: 0.6905
2023-07-05 15:43:38 roc_auc: 0.6473
2023-07-05 15:43:38 f1: 0.6648
2023-07-05 15:43:38 loss: 0.6598
2023-07-05 15:43:38 New best roc_auc score (0.6473) at epoch-0, step-247
2023-07-05 15:43:38 
2023-07-05 15:43:45 --- Train epoch-1, step-494 ---
2023-07-05 15:43:45 loss: 0.6400
2023-07-05 15:43:45 --- Eval epoch-1, step-494 ---
2023-07-05 15:43:45 accuracy: 0.6409
2023-07-05 15:43:45 pr_auc: 0.6879
2023-07-05 15:43:45 roc_auc: 0.6586
2023-07-05 15:43:45 f1: 0.6986
2023-07-05 15:43:45 loss: 0.6714
2023-07-05 15:43:45 New best roc_auc score (0.6586) at epoch-1, step-494
2023-07-05 15:43:45 
2023-07-05 15:43:51 --- Train epoch-2, step-741 ---
2023-07-05 15:43:51 loss: 0.5749
2023-07-05 15:43:52 --- Eval epoch-2, step-741 ---
2023-07-05 15:43:52 accuracy: 0.6226
2023-07-05 15:43:52 pr_auc: 0.6923
2023-07-05 15:43:52 roc_auc: 0.6551
2023-07-05 15:43:52 f1: 0.6692
2023-07-05 15:43:52 loss: 0.7145
2023-07-05 15:43:52 
2023-07-05 15:43:58 --- Train epoch-3, step-988 ---
2023-07-05 15:43:58 loss: 0.4890
2023-07-05 15:43:58 --- Eval epoch-3, step-988 ---
2023-07-05 15:43:58 accuracy: 0.6086
2023-07-05 15:43:58 pr_auc: 0.6872
2023-07-05 15:43:58 roc_auc: 0.6416
2023-07-05 15:43:58 f1: 0.6527
2023-07-05 15:43:58 loss: 0.8129
2023-07-05 15:43:58 
2023-07-05 15:44:05 --- Train epoch-4, step-1235 ---
2023-07-05 15:44:05 loss: 0.3821
2023-07-05 15:44:05 --- Eval epoch-4, step-1235 ---
2023-07-05 15:44:05 accuracy: 0.6054
2023-07-05 15:44:05 pr_auc: 0.6747
2023-07-05 15:44:05 roc_auc: 0.6321
2023-07-05 15:44:05 f1: 0.6326
2023-07-05 15:44:05 loss: 0.9954
2023-07-05 15:44:05 
2023-07-05 15:44:11 --- Train epoch-5, step-1482 ---
2023-07-05 15:44:11 loss: 0.2850
2023-07-05 15:44:11 --- Eval epoch-5, step-1482 ---
2023-07-05 15:44:11 accuracy: 0.5968
2023-07-05 15:44:11 pr_auc: 0.6706
2023-07-05 15:44:11 roc_auc: 0.6196
2023-07-05 15:44:11 f1: 0.6582
2023-07-05 15:44:11 loss: 1.2628
2023-07-05 15:44:11 
2023-07-05 15:44:17 --- Train epoch-6, step-1729 ---
2023-07-05 15:44:17 loss: 0.2060
2023-07-05 15:44:18 --- Eval epoch-6, step-1729 ---
2023-07-05 15:44:18 accuracy: 0.5785
2023-07-05 15:44:18 pr_auc: 0.6656
2023-07-05 15:44:18 roc_auc: 0.6087
2023-07-05 15:44:18 f1: 0.6238
2023-07-05 15:44:18 loss: 1.5811
2023-07-05 15:44:18 
2023-07-05 15:44:24 --- Train epoch-7, step-1976 ---
2023-07-05 15:44:24 loss: 0.1797
2023-07-05 15:44:24 --- Eval epoch-7, step-1976 ---
2023-07-05 15:44:24 accuracy: 0.5989
2023-07-05 15:44:24 pr_auc: 0.6758
2023-07-05 15:44:24 roc_auc: 0.6285
2023-07-05 15:44:24 f1: 0.6382
2023-07-05 15:44:24 loss: 1.6346
2023-07-05 15:44:24 
2023-07-05 15:44:30 --- Train epoch-8, step-2223 ---
2023-07-05 15:44:30 loss: 0.1360
2023-07-05 15:44:31 --- Eval epoch-8, step-2223 ---
2023-07-05 15:44:31 accuracy: 0.5892
2023-07-05 15:44:31 pr_auc: 0.6795
2023-07-05 15:44:31 roc_auc: 0.6301
2023-07-05 15:44:31 f1: 0.6306
2023-07-05 15:44:31 loss: 1.8161
2023-07-05 15:44:31 
2023-07-05 15:44:37 --- Train epoch-9, step-2470 ---
2023-07-05 15:44:37 loss: 0.1072
2023-07-05 15:44:37 --- Eval epoch-9, step-2470 ---
2023-07-05 15:44:37 accuracy: 0.5860
2023-07-05 15:44:37 pr_auc: 0.6703
2023-07-05 15:44:37 roc_auc: 0.6204
2023-07-05 15:44:37 f1: 0.6207
2023-07-05 15:44:37 loss: 2.0451
2023-07-05 15:44:37 
2023-07-05 15:44:43 --- Train epoch-10, step-2717 ---
2023-07-05 15:44:43 loss: 0.0988
2023-07-05 15:44:44 --- Eval epoch-10, step-2717 ---
2023-07-05 15:44:44 accuracy: 0.5882
2023-07-05 15:44:44 pr_auc: 0.6743
2023-07-05 15:44:44 roc_auc: 0.6232
2023-07-05 15:44:44 f1: 0.6263
2023-07-05 15:44:44 loss: 2.3785
2023-07-05 15:44:44 
2023-07-05 15:44:50 --- Train epoch-11, step-2964 ---
2023-07-05 15:44:50 loss: 0.0965
2023-07-05 15:44:50 --- Eval epoch-11, step-2964 ---
2023-07-05 15:44:50 accuracy: 0.5828
2023-07-05 15:44:50 pr_auc: 0.6662
2023-07-05 15:44:50 roc_auc: 0.6222
2023-07-05 15:44:50 f1: 0.6326
2023-07-05 15:44:50 loss: 2.3652
2023-07-05 15:44:50 
2023-07-05 15:44:57 --- Train epoch-12, step-3211 ---
2023-07-05 15:44:57 loss: 0.0946
2023-07-05 15:44:57 --- Eval epoch-12, step-3211 ---
2023-07-05 15:44:57 accuracy: 0.5806
2023-07-05 15:44:57 pr_auc: 0.6608
2023-07-05 15:44:57 roc_auc: 0.6171
2023-07-05 15:44:57 f1: 0.6321
2023-07-05 15:44:57 loss: 2.4986
2023-07-05 15:44:57 
2023-07-05 15:45:05 --- Train epoch-13, step-3458 ---
2023-07-05 15:45:05 loss: 0.0917
2023-07-05 15:45:05 --- Eval epoch-13, step-3458 ---
2023-07-05 15:45:05 accuracy: 0.5763
2023-07-05 15:45:05 pr_auc: 0.6677
2023-07-05 15:45:05 roc_auc: 0.6232
2023-07-05 15:45:05 f1: 0.6226
2023-07-05 15:45:05 loss: 2.5070
2023-07-05 15:45:05 
2023-07-05 15:45:12 --- Train epoch-14, step-3705 ---
2023-07-05 15:45:12 loss: 0.0799
2023-07-05 15:45:13 --- Eval epoch-14, step-3705 ---
2023-07-05 15:45:13 accuracy: 0.5817
2023-07-05 15:45:13 pr_auc: 0.6442
2023-07-05 15:45:13 roc_auc: 0.6068
2023-07-05 15:45:13 f1: 0.6285
2023-07-05 15:45:13 loss: 2.6829
2023-07-05 15:45:13 
2023-07-05 15:45:19 --- Train epoch-15, step-3952 ---
2023-07-05 15:45:19 loss: 0.0738
2023-07-05 15:45:19 --- Eval epoch-15, step-3952 ---
2023-07-05 15:45:19 accuracy: 0.5699
2023-07-05 15:45:19 pr_auc: 0.6519
2023-07-05 15:45:19 roc_auc: 0.6103
2023-07-05 15:45:19 f1: 0.6071
2023-07-05 15:45:19 loss: 2.6997
2023-07-05 15:45:19 
2023-07-05 15:45:26 --- Train epoch-16, step-4199 ---
2023-07-05 15:45:26 loss: 0.0580
2023-07-05 15:45:26 --- Eval epoch-16, step-4199 ---
2023-07-05 15:45:26 accuracy: 0.5720
2023-07-05 15:45:26 pr_auc: 0.6602
2023-07-05 15:45:26 roc_auc: 0.6118
2023-07-05 15:45:26 f1: 0.6188
2023-07-05 15:45:26 loss: 2.9675
2023-07-05 15:45:26 
2023-07-05 15:45:32 --- Train epoch-17, step-4446 ---
2023-07-05 15:45:32 loss: 0.0636
2023-07-05 15:45:33 --- Eval epoch-17, step-4446 ---
2023-07-05 15:45:33 accuracy: 0.5677
2023-07-05 15:45:33 pr_auc: 0.6496
2023-07-05 15:45:33 roc_auc: 0.6083
2023-07-05 15:45:33 f1: 0.6179
2023-07-05 15:45:33 loss: 2.9483
2023-07-05 15:45:33 
2023-07-05 15:45:39 --- Train epoch-18, step-4693 ---
2023-07-05 15:45:39 loss: 0.0580
2023-07-05 15:45:39 --- Eval epoch-18, step-4693 ---
2023-07-05 15:45:39 accuracy: 0.5828
2023-07-05 15:45:39 pr_auc: 0.6519
2023-07-05 15:45:39 roc_auc: 0.6146
2023-07-05 15:45:39 f1: 0.6326
2023-07-05 15:45:39 loss: 2.8757
2023-07-05 15:45:39 
2023-07-05 15:45:45 --- Train epoch-19, step-4940 ---
2023-07-05 15:45:45 loss: 0.0524
2023-07-05 15:45:46 --- Eval epoch-19, step-4940 ---
2023-07-05 15:45:46 accuracy: 0.5871
2023-07-05 15:45:46 pr_auc: 0.6458
2023-07-05 15:45:46 roc_auc: 0.6138
2023-07-05 15:45:46 f1: 0.6398
2023-07-05 15:45:46 loss: 3.0235
2023-07-05 15:45:46 Loaded best model
2023-07-05 15:45:48 StageNet(
  (embeddings): ModuleDict(
    (conditions): Embedding(4031, 128, padding_idx=0)
    (procedures): Embedding(1276, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (stagenet): ModuleDict(
    (conditions): StageNetLayer(
      (kernel): Linear(in_features=129, out_features=1542, bias=True)
      (recurrent_kernel): Linear(in_features=385, out_features=1542, bias=True)
      (nn_scale): Linear(in_features=384, out_features=64, bias=True)
      (nn_rescale): Linear(in_features=64, out_features=384, bias=True)
      (nn_conv): Conv1d(384, 384, kernel_size=(10,), stride=(1,))
      (nn_dropconnect): Dropout(p=0.3, inplace=False)
      (nn_dropconnect_r): Dropout(p=0.3, inplace=False)
      (nn_dropout): Dropout(p=0.3, inplace=False)
      (nn_dropres): Dropout(p=0.3, inplace=False)
    )
    (procedures): StageNetLayer(
      (kernel): Linear(in_features=129, out_features=1542, bias=True)
      (recurrent_kernel): Linear(in_features=385, out_features=1542, bias=True)
      (nn_scale): Linear(in_features=384, out_features=64, bias=True)
      (nn_rescale): Linear(in_features=64, out_features=384, bias=True)
      (nn_conv): Conv1d(384, 384, kernel_size=(10,), stride=(1,))
      (nn_dropconnect): Dropout(p=0.3, inplace=False)
      (nn_dropconnect_r): Dropout(p=0.3, inplace=False)
      (nn_dropout): Dropout(p=0.3, inplace=False)
      (nn_dropres): Dropout(p=0.3, inplace=False)
    )
  )
  (fc): Linear(in_features=768, out_features=1, bias=True)
)
2023-07-05 15:45:48 Metrics: ['accuracy', 'pr_auc', 'roc_auc', 'f1']
2023-07-05 15:45:48 Device: cuda
2023-07-05 15:45:48 
2023-07-05 15:45:48 Training:
2023-07-05 15:45:48 Batch size: 32
2023-07-05 15:45:48 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-05 15:45:48 Optimizer params: {'lr': 0.001}
2023-07-05 15:45:48 Weight decay: 0.0
2023-07-05 15:45:48 Max grad norm: None
2023-07-05 15:45:48 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd795a0f850>
2023-07-05 15:45:48 Monitor: roc_auc
2023-07-05 15:45:48 Monitor criterion: max
2023-07-05 15:45:48 Epochs: 20
2023-07-05 15:45:48 
2023-07-05 15:45:55 --- Train epoch-0, step-245 ---
2023-07-05 15:45:55 loss: 0.2810
2023-07-05 15:45:55 --- Eval epoch-0, step-245 ---
2023-07-05 15:45:55 accuracy: 0.9374
2023-07-05 15:45:55 pr_auc: 0.1164
2023-07-05 15:45:55 roc_auc: 0.6981
2023-07-05 15:45:55 f1: 0.0000
2023-07-05 15:45:55 loss: 0.2216
2023-07-05 15:45:55 New best roc_auc score (0.6981) at epoch-0, step-245
2023-07-05 15:45:55 
2023-07-05 15:46:01 --- Train epoch-1, step-490 ---
2023-07-05 15:46:01 loss: 0.2297
2023-07-05 15:46:02 --- Eval epoch-1, step-490 ---
2023-07-05 15:46:02 accuracy: 0.9374
2023-07-05 15:46:02 pr_auc: 0.1090
2023-07-05 15:46:02 roc_auc: 0.6874
2023-07-05 15:46:02 f1: 0.0000
2023-07-05 15:46:02 loss: 0.2400
2023-07-05 15:46:02 
2023-07-05 15:46:07 --- Train epoch-2, step-735 ---
2023-07-05 15:46:07 loss: 0.2045
2023-07-05 15:46:08 --- Eval epoch-2, step-735 ---
2023-07-05 15:46:08 accuracy: 0.9364
2023-07-05 15:46:08 pr_auc: 0.0998
2023-07-05 15:46:08 roc_auc: 0.6634
2023-07-05 15:46:08 f1: 0.0000
2023-07-05 15:46:08 loss: 0.2564
2023-07-05 15:46:08 
2023-07-05 15:46:14 --- Train epoch-3, step-980 ---
2023-07-05 15:46:14 loss: 0.1645
2023-07-05 15:46:14 --- Eval epoch-3, step-980 ---
2023-07-05 15:46:14 accuracy: 0.9262
2023-07-05 15:46:14 pr_auc: 0.0932
2023-07-05 15:46:14 roc_auc: 0.6449
2023-07-05 15:46:14 f1: 0.0270
2023-07-05 15:46:14 loss: 0.3099
2023-07-05 15:46:14 
2023-07-05 15:46:21 --- Train epoch-4, step-1225 ---
2023-07-05 15:46:21 loss: 0.1197
2023-07-05 15:46:21 --- Eval epoch-4, step-1225 ---
2023-07-05 15:46:21 accuracy: 0.9077
2023-07-05 15:46:21 pr_auc: 0.0848
2023-07-05 15:46:21 roc_auc: 0.6156
2023-07-05 15:46:21 f1: 0.0426
2023-07-05 15:46:21 loss: 0.3851
2023-07-05 15:46:21 
2023-07-05 15:46:27 --- Train epoch-5, step-1470 ---
2023-07-05 15:46:27 loss: 0.0749
2023-07-05 15:46:28 --- Eval epoch-5, step-1470 ---
2023-07-05 15:46:28 accuracy: 0.9138
2023-07-05 15:46:28 pr_auc: 0.0948
2023-07-05 15:46:28 roc_auc: 0.6149
2023-07-05 15:46:28 f1: 0.1250
2023-07-05 15:46:28 loss: 0.5488
2023-07-05 15:46:28 
2023-07-05 15:46:34 --- Train epoch-6, step-1715 ---
2023-07-05 15:46:34 loss: 0.0442
2023-07-05 15:46:35 --- Eval epoch-6, step-1715 ---
2023-07-05 15:46:35 accuracy: 0.9108
2023-07-05 15:46:35 pr_auc: 0.0923
2023-07-05 15:46:35 roc_auc: 0.6423
2023-07-05 15:46:35 f1: 0.0842
2023-07-05 15:46:35 loss: 0.6441
2023-07-05 15:46:35 
2023-07-05 15:46:40 --- Train epoch-7, step-1960 ---
2023-07-05 15:46:40 loss: 0.0224
2023-07-05 15:46:41 --- Eval epoch-7, step-1960 ---
2023-07-05 15:46:41 accuracy: 0.9138
2023-07-05 15:46:41 pr_auc: 0.1020
2023-07-05 15:46:41 roc_auc: 0.6281
2023-07-05 15:46:41 f1: 0.1064
2023-07-05 15:46:41 loss: 0.7757
2023-07-05 15:46:41 
2023-07-05 15:46:47 --- Train epoch-8, step-2205 ---
2023-07-05 15:46:47 loss: 0.0178
2023-07-05 15:46:48 --- Eval epoch-8, step-2205 ---
2023-07-05 15:46:48 accuracy: 0.9169
2023-07-05 15:46:48 pr_auc: 0.0998
2023-07-05 15:46:48 roc_auc: 0.6304
2023-07-05 15:46:48 f1: 0.0899
2023-07-05 15:46:48 loss: 0.8120
2023-07-05 15:46:48 
2023-07-05 15:46:53 --- Train epoch-9, step-2450 ---
2023-07-05 15:46:53 loss: 0.0186
2023-07-05 15:46:54 --- Eval epoch-9, step-2450 ---
2023-07-05 15:46:54 accuracy: 0.9097
2023-07-05 15:46:54 pr_auc: 0.1071
2023-07-05 15:46:54 roc_auc: 0.6507
2023-07-05 15:46:54 f1: 0.1200
2023-07-05 15:46:54 loss: 0.8448
2023-07-05 15:46:54 
2023-07-05 15:47:00 --- Train epoch-10, step-2695 ---
2023-07-05 15:47:00 loss: 0.0143
2023-07-05 15:47:01 --- Eval epoch-10, step-2695 ---
2023-07-05 15:47:01 accuracy: 0.9138
2023-07-05 15:47:01 pr_auc: 0.0945
2023-07-05 15:47:01 roc_auc: 0.6326
2023-07-05 15:47:01 f1: 0.0667
2023-07-05 15:47:01 loss: 0.9627
2023-07-05 15:47:01 
2023-07-05 15:47:06 --- Train epoch-11, step-2940 ---
2023-07-05 15:47:06 loss: 0.0140
2023-07-05 15:47:07 --- Eval epoch-11, step-2940 ---
2023-07-05 15:47:07 accuracy: 0.9046
2023-07-05 15:47:07 pr_auc: 0.0943
2023-07-05 15:47:07 roc_auc: 0.6440
2023-07-05 15:47:07 f1: 0.0606
2023-07-05 15:47:07 loss: 1.0371
2023-07-05 15:47:07 
2023-07-05 15:47:13 --- Train epoch-12, step-3185 ---
2023-07-05 15:47:13 loss: 0.0168
2023-07-05 15:47:14 --- Eval epoch-12, step-3185 ---
2023-07-05 15:47:14 accuracy: 0.9149
2023-07-05 15:47:14 pr_auc: 0.0931
2023-07-05 15:47:14 roc_auc: 0.6267
2023-07-05 15:47:14 f1: 0.0460
2023-07-05 15:47:14 loss: 1.0639
2023-07-05 15:47:14 
2023-07-05 15:47:20 --- Train epoch-13, step-3430 ---
2023-07-05 15:47:20 loss: 0.0155
2023-07-05 15:47:21 --- Eval epoch-13, step-3430 ---
2023-07-05 15:47:21 accuracy: 0.9169
2023-07-05 15:47:21 pr_auc: 0.0984
2023-07-05 15:47:21 roc_auc: 0.6298
2023-07-05 15:47:21 f1: 0.0471
2023-07-05 15:47:21 loss: 1.1451
2023-07-05 15:47:21 
2023-07-05 15:47:27 --- Train epoch-14, step-3675 ---
2023-07-05 15:47:27 loss: 0.0123
2023-07-05 15:47:28 --- Eval epoch-14, step-3675 ---
2023-07-05 15:47:28 accuracy: 0.9159
2023-07-05 15:47:28 pr_auc: 0.1025
2023-07-05 15:47:28 roc_auc: 0.6294
2023-07-05 15:47:28 f1: 0.0889
2023-07-05 15:47:28 loss: 1.1417
2023-07-05 15:47:28 
2023-07-05 15:47:35 --- Train epoch-15, step-3920 ---
2023-07-05 15:47:35 loss: 0.0113
2023-07-05 15:47:36 --- Eval epoch-15, step-3920 ---
2023-07-05 15:47:36 accuracy: 0.9087
2023-07-05 15:47:36 pr_auc: 0.1103
2023-07-05 15:47:36 roc_auc: 0.6414
2023-07-05 15:47:36 f1: 0.1359
2023-07-05 15:47:36 loss: 1.1514
2023-07-05 15:47:36 
2023-07-05 15:47:42 --- Train epoch-16, step-4165 ---
2023-07-05 15:47:42 loss: 0.0190
2023-07-05 15:47:43 --- Eval epoch-16, step-4165 ---
2023-07-05 15:47:43 accuracy: 0.9231
2023-07-05 15:47:43 pr_auc: 0.1121
2023-07-05 15:47:43 roc_auc: 0.6296
2023-07-05 15:47:43 f1: 0.0964
2023-07-05 15:47:43 loss: 1.2393
2023-07-05 15:47:43 
2023-07-05 15:47:49 --- Train epoch-17, step-4410 ---
2023-07-05 15:47:49 loss: 0.0180
2023-07-05 15:47:50 --- Eval epoch-17, step-4410 ---
2023-07-05 15:47:50 accuracy: 0.9169
2023-07-05 15:47:50 pr_auc: 0.1195
2023-07-05 15:47:50 roc_auc: 0.6372
2023-07-05 15:47:50 f1: 0.1290
2023-07-05 15:47:50 loss: 1.1088
2023-07-05 15:47:50 
2023-07-05 15:47:56 --- Train epoch-18, step-4655 ---
2023-07-05 15:47:56 loss: 0.0172
2023-07-05 15:47:56 --- Eval epoch-18, step-4655 ---
2023-07-05 15:47:56 accuracy: 0.9169
2023-07-05 15:47:56 pr_auc: 0.1108
2023-07-05 15:47:56 roc_auc: 0.6156
2023-07-05 15:47:56 f1: 0.1290
2023-07-05 15:47:56 loss: 1.1861
2023-07-05 15:47:56 
2023-07-05 15:48:02 --- Train epoch-19, step-4900 ---
2023-07-05 15:48:02 loss: 0.0116
2023-07-05 15:48:03 --- Eval epoch-19, step-4900 ---
2023-07-05 15:48:03 accuracy: 0.9149
2023-07-05 15:48:03 pr_auc: 0.1194
2023-07-05 15:48:03 roc_auc: 0.6077
2023-07-05 15:48:03 f1: 0.1443
2023-07-05 15:48:03 loss: 1.2326
2023-07-05 15:48:03 Loaded best model
2023-07-05 15:48:10 StageNet(
  (embeddings): ModuleDict(
    (conditions): Embedding(6664, 128, padding_idx=0)
    (procedures): Embedding(1980, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (stagenet): ModuleDict(
    (conditions): StageNetLayer(
      (kernel): Linear(in_features=129, out_features=1542, bias=True)
      (recurrent_kernel): Linear(in_features=385, out_features=1542, bias=True)
      (nn_scale): Linear(in_features=384, out_features=64, bias=True)
      (nn_rescale): Linear(in_features=64, out_features=384, bias=True)
      (nn_conv): Conv1d(384, 384, kernel_size=(10,), stride=(1,))
      (nn_dropconnect): Dropout(p=0.3, inplace=False)
      (nn_dropconnect_r): Dropout(p=0.3, inplace=False)
      (nn_dropout): Dropout(p=0.3, inplace=False)
      (nn_dropres): Dropout(p=0.3, inplace=False)
    )
    (procedures): StageNetLayer(
      (kernel): Linear(in_features=129, out_features=1542, bias=True)
      (recurrent_kernel): Linear(in_features=385, out_features=1542, bias=True)
      (nn_scale): Linear(in_features=384, out_features=64, bias=True)
      (nn_rescale): Linear(in_features=64, out_features=384, bias=True)
      (nn_conv): Conv1d(384, 384, kernel_size=(10,), stride=(1,))
      (nn_dropconnect): Dropout(p=0.3, inplace=False)
      (nn_dropconnect_r): Dropout(p=0.3, inplace=False)
      (nn_dropout): Dropout(p=0.3, inplace=False)
      (nn_dropres): Dropout(p=0.3, inplace=False)
    )
  )
  (fc): Linear(in_features=768, out_features=10, bias=True)
)
2023-07-05 15:48:10 Metrics: ['accuracy', 'f1_macro', 'roc_auc_weighted_ovo']
2023-07-05 15:48:10 Device: cuda
2023-07-05 15:48:10 
2023-07-05 15:48:10 Training:
2023-07-05 15:48:10 Batch size: 32
2023-07-05 15:48:10 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-05 15:48:10 Optimizer params: {'lr': 0.001}
2023-07-05 15:48:10 Weight decay: 0.0
2023-07-05 15:48:10 Max grad norm: None
2023-07-05 15:48:10 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd720b7fa90>
2023-07-05 15:48:10 Monitor: accuracy
2023-07-05 15:48:10 Monitor criterion: max
2023-07-05 15:48:10 Epochs: 20
2023-07-05 15:48:10 
2023-07-05 15:48:36 --- Train epoch-0, step-1108 ---
2023-07-05 15:48:36 loss: 1.7562
2023-07-05 15:48:39 --- Eval epoch-0, step-1108 ---
2023-07-05 15:48:39 accuracy: 0.3844
2023-07-05 15:48:39 f1_macro: 0.2271
2023-07-05 15:48:39 roc_auc_weighted_ovo: 0.7618
2023-07-05 15:48:39 loss: 1.7550
2023-07-05 15:48:39 New best accuracy score (0.3844) at epoch-0, step-1108
2023-07-05 15:48:39 
2023-07-05 15:49:06 --- Train epoch-1, step-2216 ---
2023-07-05 15:49:06 loss: 1.6116
2023-07-05 15:49:08 --- Eval epoch-1, step-2216 ---
2023-07-05 15:49:08 accuracy: 0.4071
2023-07-05 15:49:08 f1_macro: 0.2506
2023-07-05 15:49:08 roc_auc_weighted_ovo: 0.7769
2023-07-05 15:49:08 loss: 1.6796
2023-07-05 15:49:08 New best accuracy score (0.4071) at epoch-1, step-2216
2023-07-05 15:49:08 
2023-07-05 15:49:36 --- Train epoch-2, step-3324 ---
2023-07-05 15:49:36 loss: 1.5371
2023-07-05 15:49:38 --- Eval epoch-2, step-3324 ---
2023-07-05 15:49:38 accuracy: 0.4035
2023-07-05 15:49:38 f1_macro: 0.2524
2023-07-05 15:49:38 roc_auc_weighted_ovo: 0.7765
2023-07-05 15:49:38 loss: 1.7421
2023-07-05 15:49:38 
2023-07-05 15:50:09 --- Train epoch-3, step-4432 ---
2023-07-05 15:50:09 loss: 1.4751
2023-07-05 15:50:12 --- Eval epoch-3, step-4432 ---
2023-07-05 15:50:12 accuracy: 0.3972
2023-07-05 15:50:12 f1_macro: 0.2608
2023-07-05 15:50:12 roc_auc_weighted_ovo: 0.7779
2023-07-05 15:50:12 loss: 1.7917
2023-07-05 15:50:12 
2023-07-05 15:50:40 --- Train epoch-4, step-5540 ---
2023-07-05 15:50:40 loss: 1.4134
2023-07-05 15:50:42 --- Eval epoch-4, step-5540 ---
2023-07-05 15:50:42 accuracy: 0.3967
2023-07-05 15:50:42 f1_macro: 0.2599
2023-07-05 15:50:42 roc_auc_weighted_ovo: 0.7767
2023-07-05 15:50:42 loss: 1.8456
2023-07-05 15:50:42 
2023-07-05 15:51:10 --- Train epoch-5, step-6648 ---
2023-07-05 15:51:10 loss: 1.3507
2023-07-05 15:51:12 --- Eval epoch-5, step-6648 ---
2023-07-05 15:51:12 accuracy: 0.3911
2023-07-05 15:51:12 f1_macro: 0.2637
2023-07-05 15:51:12 roc_auc_weighted_ovo: 0.7771
2023-07-05 15:51:12 loss: 1.9397
2023-07-05 15:51:12 
2023-07-05 15:51:39 --- Train epoch-6, step-7756 ---
2023-07-05 15:51:39 loss: 1.2833
2023-07-05 15:51:42 --- Eval epoch-6, step-7756 ---
2023-07-05 15:51:42 accuracy: 0.3918
2023-07-05 15:51:42 f1_macro: 0.2682
2023-07-05 15:51:42 roc_auc_weighted_ovo: 0.7739
2023-07-05 15:51:42 loss: 1.9869
2023-07-05 15:51:42 
2023-07-05 15:52:10 --- Train epoch-7, step-8864 ---
2023-07-05 15:52:10 loss: 1.2187
2023-07-05 15:52:12 --- Eval epoch-7, step-8864 ---
2023-07-05 15:52:12 accuracy: 0.3751
2023-07-05 15:52:12 f1_macro: 0.2655
2023-07-05 15:52:12 roc_auc_weighted_ovo: 0.7695
2023-07-05 15:52:12 loss: 2.1630
2023-07-05 15:52:12 
2023-07-05 15:52:43 --- Train epoch-8, step-9972 ---
2023-07-05 15:52:43 loss: 1.1578
2023-07-05 15:52:46 --- Eval epoch-8, step-9972 ---
2023-07-05 15:52:46 accuracy: 0.3769
2023-07-05 15:52:46 f1_macro: 0.2650
2023-07-05 15:52:46 roc_auc_weighted_ovo: 0.7665
2023-07-05 15:52:46 loss: 2.2284
2023-07-05 15:52:46 
2023-07-05 15:53:13 --- Train epoch-9, step-11080 ---
2023-07-05 15:53:13 loss: 1.0976
2023-07-05 15:53:16 --- Eval epoch-9, step-11080 ---
2023-07-05 15:53:16 accuracy: 0.3697
2023-07-05 15:53:16 f1_macro: 0.2696
2023-07-05 15:53:16 roc_auc_weighted_ovo: 0.7668
2023-07-05 15:53:16 loss: 2.3104
2023-07-05 15:53:16 
2023-07-05 15:53:44 --- Train epoch-10, step-12188 ---
2023-07-05 15:53:44 loss: 1.0388
2023-07-05 15:53:46 --- Eval epoch-10, step-12188 ---
2023-07-05 15:53:46 accuracy: 0.3745
2023-07-05 15:53:46 f1_macro: 0.2740
2023-07-05 15:53:46 roc_auc_weighted_ovo: 0.7654
2023-07-05 15:53:46 loss: 2.4409
2023-07-05 15:53:46 
2023-07-05 15:54:13 --- Train epoch-11, step-13296 ---
2023-07-05 15:54:13 loss: 0.9923
2023-07-05 15:54:15 --- Eval epoch-11, step-13296 ---
2023-07-05 15:54:15 accuracy: 0.3704
2023-07-05 15:54:15 f1_macro: 0.2671
2023-07-05 15:54:15 roc_auc_weighted_ovo: 0.7627
2023-07-05 15:54:15 loss: 2.5610
2023-07-05 15:54:15 
2023-07-05 15:54:42 --- Train epoch-12, step-14404 ---
2023-07-05 15:54:42 loss: 0.9424
2023-07-05 15:54:44 --- Eval epoch-12, step-14404 ---
2023-07-05 15:54:44 accuracy: 0.3558
2023-07-05 15:54:44 f1_macro: 0.2520
2023-07-05 15:54:44 roc_auc_weighted_ovo: 0.7591
2023-07-05 15:54:44 loss: 2.7255
2023-07-05 15:54:44 
2023-07-05 15:55:15 --- Train epoch-13, step-15512 ---
2023-07-05 15:55:15 loss: 0.9029
2023-07-05 15:55:18 --- Eval epoch-13, step-15512 ---
2023-07-05 15:55:18 accuracy: 0.3700
2023-07-05 15:55:18 f1_macro: 0.2654
2023-07-05 15:55:18 roc_auc_weighted_ovo: 0.7611
2023-07-05 15:55:18 loss: 2.7552
2023-07-05 15:55:18 
2023-07-05 15:55:46 --- Train epoch-14, step-16620 ---
2023-07-05 15:55:46 loss: 0.8590
2023-07-05 15:55:48 --- Eval epoch-14, step-16620 ---
2023-07-05 15:55:48 accuracy: 0.3623
2023-07-05 15:55:48 f1_macro: 0.2623
2023-07-05 15:55:48 roc_auc_weighted_ovo: 0.7609
2023-07-05 15:55:48 loss: 2.8453
2023-07-05 15:55:48 
2023-07-05 15:56:15 --- Train epoch-15, step-17728 ---
2023-07-05 15:56:15 loss: 0.8177
2023-07-05 15:56:18 --- Eval epoch-15, step-17728 ---
2023-07-05 15:56:18 accuracy: 0.3684
2023-07-05 15:56:18 f1_macro: 0.2654
2023-07-05 15:56:18 roc_auc_weighted_ovo: 0.7572
2023-07-05 15:56:18 loss: 3.0053
2023-07-05 15:56:18 
2023-07-05 15:56:45 --- Train epoch-16, step-18836 ---
2023-07-05 15:56:45 loss: 0.7829
2023-07-05 15:56:47 --- Eval epoch-16, step-18836 ---
2023-07-05 15:56:47 accuracy: 0.3599
2023-07-05 15:56:47 f1_macro: 0.2616
2023-07-05 15:56:47 roc_auc_weighted_ovo: 0.7547
2023-07-05 15:56:47 loss: 3.1277
2023-07-05 15:56:47 
2023-07-05 15:57:15 --- Train epoch-17, step-19944 ---
2023-07-05 15:57:15 loss: 0.7434
2023-07-05 15:57:17 --- Eval epoch-17, step-19944 ---
2023-07-05 15:57:17 accuracy: 0.3617
2023-07-05 15:57:17 f1_macro: 0.2684
2023-07-05 15:57:17 roc_auc_weighted_ovo: 0.7544
2023-07-05 15:57:17 loss: 3.2688
2023-07-05 15:57:17 
2023-07-05 15:57:46 --- Train epoch-18, step-21052 ---
2023-07-05 15:57:46 loss: 0.7310
2023-07-05 15:57:49 --- Eval epoch-18, step-21052 ---
2023-07-05 15:57:49 accuracy: 0.3529
2023-07-05 15:57:49 f1_macro: 0.2626
2023-07-05 15:57:49 roc_auc_weighted_ovo: 0.7548
2023-07-05 15:57:49 loss: 3.2995
2023-07-05 15:57:49 
2023-07-05 15:58:16 --- Train epoch-19, step-22160 ---
2023-07-05 15:58:16 loss: 0.6987
2023-07-05 15:58:18 --- Eval epoch-19, step-22160 ---
2023-07-05 15:58:18 accuracy: 0.3601
2023-07-05 15:58:18 f1_macro: 0.2673
2023-07-05 15:58:18 roc_auc_weighted_ovo: 0.7549
2023-07-05 15:58:18 loss: 3.4262
2023-07-05 15:58:18 Loaded best model
2023-07-05 15:58:20 AdaCare(
  (embeddings): ModuleDict(
    (conditions): Embedding(4031, 128, padding_idx=0)
    (procedures): Embedding(1276, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (adacare): ModuleDict(
    (conditions): AdaCareLayer(
      (nn_conv1): CausalConv1d(128, 64, kernel_size=(2,), stride=(1,), padding=(1,))
      (nn_conv3): CausalConv1d(128, 64, kernel_size=(2,), stride=(1,), padding=(3,), dilation=(3,))
      (nn_conv5): CausalConv1d(128, 64, kernel_size=(2,), stride=(1,), padding=(5,), dilation=(5,))
      (nn_convse): Recalibration(
        (avg_pool): AdaptiveAvgPool1d(output_size=1)
        (nn_c): Linear(in_features=192, out_features=48, bias=True)
        (nn_rescale): Linear(in_features=48, out_features=192, bias=True)
        (sparsemax): Sparsemax()
        (softmax): Softmax(dim=1)
      )
      (nn_inputse): Recalibration(
        (avg_pool): AdaptiveAvgPool1d(output_size=1)
        (nn_c): Linear(in_features=128, out_features=32, bias=True)
        (nn_rescale): Linear(in_features=32, out_features=128, bias=True)
        (sparsemax): Sparsemax()
        (softmax): Softmax(dim=1)
      )
      (rnn): GRU(320, 128)
      (nn_dropout): Dropout(p=0.5, inplace=False)
      (relu): ReLU()
      (sigmoid): Sigmoid()
      (tanh): Tanh()
    )
    (procedures): AdaCareLayer(
      (nn_conv1): CausalConv1d(128, 64, kernel_size=(2,), stride=(1,), padding=(1,))
      (nn_conv3): CausalConv1d(128, 64, kernel_size=(2,), stride=(1,), padding=(3,), dilation=(3,))
      (nn_conv5): CausalConv1d(128, 64, kernel_size=(2,), stride=(1,), padding=(5,), dilation=(5,))
      (nn_convse): Recalibration(
        (avg_pool): AdaptiveAvgPool1d(output_size=1)
        (nn_c): Linear(in_features=192, out_features=48, bias=True)
        (nn_rescale): Linear(in_features=48, out_features=192, bias=True)
        (sparsemax): Sparsemax()
        (softmax): Softmax(dim=1)
      )
      (nn_inputse): Recalibration(
        (avg_pool): AdaptiveAvgPool1d(output_size=1)
        (nn_c): Linear(in_features=128, out_features=32, bias=True)
        (nn_rescale): Linear(in_features=32, out_features=128, bias=True)
        (sparsemax): Sparsemax()
        (softmax): Softmax(dim=1)
      )
      (rnn): GRU(320, 128)
      (nn_dropout): Dropout(p=0.5, inplace=False)
      (relu): ReLU()
      (sigmoid): Sigmoid()
      (tanh): Tanh()
    )
  )
  (fc): Linear(in_features=256, out_features=1, bias=True)
)
2023-07-05 15:58:20 Metrics: ['accuracy', 'pr_auc', 'roc_auc', 'f1']
2023-07-05 15:58:20 Device: cuda
2023-07-05 15:58:20 
2023-07-05 15:58:20 Training:
2023-07-05 15:58:20 Batch size: 32
2023-07-05 15:58:20 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-05 15:58:20 Optimizer params: {'lr': 0.001}
2023-07-05 15:58:20 Weight decay: 0.0
2023-07-05 15:58:20 Max grad norm: None
2023-07-05 15:58:20 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd795ed4290>
2023-07-05 15:58:20 Monitor: roc_auc
2023-07-05 15:58:20 Monitor criterion: max
2023-07-05 15:58:20 Epochs: 20
2023-07-05 15:58:20 
2023-07-05 15:58:23 --- Train epoch-0, step-242 ---
2023-07-05 15:58:23 loss: 0.6852
2023-07-05 15:58:23 --- Eval epoch-0, step-242 ---
2023-07-05 15:58:23 accuracy: 0.5763
2023-07-05 15:58:23 pr_auc: 0.6142
2023-07-05 15:58:23 roc_auc: 0.6055
2023-07-05 15:58:23 f1: 0.6764
2023-07-05 15:58:23 loss: 0.6882
2023-07-05 15:58:23 New best roc_auc score (0.6055) at epoch-0, step-242
2023-07-05 15:58:23 
2023-07-05 15:58:26 --- Train epoch-1, step-484 ---
2023-07-05 15:58:26 loss: 0.6391
2023-07-05 15:58:26 --- Eval epoch-1, step-484 ---
2023-07-05 15:58:26 accuracy: 0.5753
2023-07-05 15:58:26 pr_auc: 0.6133
2023-07-05 15:58:26 roc_auc: 0.6206
2023-07-05 15:58:26 f1: 0.6646
2023-07-05 15:58:26 loss: 0.6869
2023-07-05 15:58:26 New best roc_auc score (0.6206) at epoch-1, step-484
2023-07-05 15:58:26 
2023-07-05 15:58:29 --- Train epoch-2, step-726 ---
2023-07-05 15:58:29 loss: 0.5921
2023-07-05 15:58:29 --- Eval epoch-2, step-726 ---
2023-07-05 15:58:29 accuracy: 0.5924
2023-07-05 15:58:29 pr_auc: 0.6359
2023-07-05 15:58:29 roc_auc: 0.6247
2023-07-05 15:58:29 f1: 0.6262
2023-07-05 15:58:29 loss: 0.6995
2023-07-05 15:58:29 New best roc_auc score (0.6247) at epoch-2, step-726
2023-07-05 15:58:29 
2023-07-05 15:58:32 --- Train epoch-3, step-968 ---
2023-07-05 15:58:32 loss: 0.5156
2023-07-05 15:58:32 --- Eval epoch-3, step-968 ---
2023-07-05 15:58:32 accuracy: 0.5733
2023-07-05 15:58:32 pr_auc: 0.6214
2023-07-05 15:58:32 roc_auc: 0.6127
2023-07-05 15:58:32 f1: 0.6039
2023-07-05 15:58:32 loss: 0.7715
2023-07-05 15:58:32 
2023-07-05 15:58:35 --- Train epoch-4, step-1210 ---
2023-07-05 15:58:35 loss: 0.4087
2023-07-05 15:58:36 --- Eval epoch-4, step-1210 ---
2023-07-05 15:58:36 accuracy: 0.5914
2023-07-05 15:58:36 pr_auc: 0.6221
2023-07-05 15:58:36 roc_auc: 0.6156
2023-07-05 15:58:36 f1: 0.6297
2023-07-05 15:58:36 loss: 0.8565
2023-07-05 15:58:36 
2023-07-05 15:58:38 --- Train epoch-5, step-1452 ---
2023-07-05 15:58:38 loss: 0.2891
2023-07-05 15:58:39 --- Eval epoch-5, step-1452 ---
2023-07-05 15:58:39 accuracy: 0.5863
2023-07-05 15:58:39 pr_auc: 0.6183
2023-07-05 15:58:39 roc_auc: 0.6042
2023-07-05 15:58:39 f1: 0.5770
2023-07-05 15:58:39 loss: 1.0498
2023-07-05 15:58:39 
2023-07-05 15:58:42 --- Train epoch-6, step-1694 ---
2023-07-05 15:58:42 loss: 0.1772
2023-07-05 15:58:42 --- Eval epoch-6, step-1694 ---
2023-07-05 15:58:42 accuracy: 0.5833
2023-07-05 15:58:42 pr_auc: 0.6244
2023-07-05 15:58:42 roc_auc: 0.6141
2023-07-05 15:58:42 f1: 0.6210
2023-07-05 15:58:42 loss: 1.2682
2023-07-05 15:58:42 
2023-07-05 15:58:45 --- Train epoch-7, step-1936 ---
2023-07-05 15:58:45 loss: 0.0968
2023-07-05 15:58:45 --- Eval epoch-7, step-1936 ---
2023-07-05 15:58:45 accuracy: 0.5663
2023-07-05 15:58:45 pr_auc: 0.6141
2023-07-05 15:58:45 roc_auc: 0.5982
2023-07-05 15:58:45 f1: 0.6080
2023-07-05 15:58:45 loss: 1.7257
2023-07-05 15:58:45 
2023-07-05 15:58:48 --- Train epoch-8, step-2178 ---
2023-07-05 15:58:48 loss: 0.0595
2023-07-05 15:58:48 --- Eval epoch-8, step-2178 ---
2023-07-05 15:58:48 accuracy: 0.5683
2023-07-05 15:58:48 pr_auc: 0.6043
2023-07-05 15:58:48 roc_auc: 0.5971
2023-07-05 15:58:48 f1: 0.6062
2023-07-05 15:58:48 loss: 1.9844
2023-07-05 15:58:48 
2023-07-05 15:58:51 --- Train epoch-9, step-2420 ---
2023-07-05 15:58:51 loss: 0.0463
2023-07-05 15:58:51 --- Eval epoch-9, step-2420 ---
2023-07-05 15:58:51 accuracy: 0.5693
2023-07-05 15:58:51 pr_auc: 0.6104
2023-07-05 15:58:51 roc_auc: 0.5925
2023-07-05 15:58:51 f1: 0.5823
2023-07-05 15:58:51 loss: 2.1044
2023-07-05 15:58:51 
2023-07-05 15:58:54 --- Train epoch-10, step-2662 ---
2023-07-05 15:58:54 loss: 0.0286
2023-07-05 15:58:54 --- Eval epoch-10, step-2662 ---
2023-07-05 15:58:54 accuracy: 0.5833
2023-07-05 15:58:54 pr_auc: 0.6254
2023-07-05 15:58:54 roc_auc: 0.6128
2023-07-05 15:58:54 f1: 0.6382
2023-07-05 15:58:54 loss: 2.3877
2023-07-05 15:58:54 
2023-07-05 15:58:57 --- Train epoch-11, step-2904 ---
2023-07-05 15:58:57 loss: 0.0284
2023-07-05 15:58:57 --- Eval epoch-11, step-2904 ---
2023-07-05 15:58:57 accuracy: 0.5863
2023-07-05 15:58:57 pr_auc: 0.6303
2023-07-05 15:58:57 roc_auc: 0.6091
2023-07-05 15:58:57 f1: 0.6091
2023-07-05 15:58:57 loss: 2.3953
2023-07-05 15:58:57 
2023-07-05 15:59:00 --- Train epoch-12, step-3146 ---
2023-07-05 15:59:00 loss: 0.0294
2023-07-05 15:59:00 --- Eval epoch-12, step-3146 ---
2023-07-05 15:59:00 accuracy: 0.5683
2023-07-05 15:59:00 pr_auc: 0.6310
2023-07-05 15:59:00 roc_auc: 0.6022
2023-07-05 15:59:00 f1: 0.6221
2023-07-05 15:59:00 loss: 2.5734
2023-07-05 15:59:00 
2023-07-05 15:59:03 --- Train epoch-13, step-3388 ---
2023-07-05 15:59:03 loss: 0.0239
2023-07-05 15:59:03 --- Eval epoch-13, step-3388 ---
2023-07-05 15:59:03 accuracy: 0.5612
2023-07-05 15:59:03 pr_auc: 0.6086
2023-07-05 15:59:03 roc_auc: 0.5836
2023-07-05 15:59:03 f1: 0.6088
2023-07-05 15:59:03 loss: 2.7306
2023-07-05 15:59:03 
2023-07-05 15:59:07 --- Train epoch-14, step-3630 ---
2023-07-05 15:59:07 loss: 0.0370
2023-07-05 15:59:07 --- Eval epoch-14, step-3630 ---
2023-07-05 15:59:07 accuracy: 0.5492
2023-07-05 15:59:07 pr_auc: 0.6152
2023-07-05 15:59:07 roc_auc: 0.5795
2023-07-05 15:59:07 f1: 0.5784
2023-07-05 15:59:07 loss: 2.7121
2023-07-05 15:59:07 
2023-07-05 15:59:10 --- Train epoch-15, step-3872 ---
2023-07-05 15:59:10 loss: 0.0243
2023-07-05 15:59:10 --- Eval epoch-15, step-3872 ---
2023-07-05 15:59:10 accuracy: 0.5602
2023-07-05 15:59:10 pr_auc: 0.6098
2023-07-05 15:59:10 roc_auc: 0.5892
2023-07-05 15:59:10 f1: 0.6068
2023-07-05 15:59:10 loss: 2.8697
2023-07-05 15:59:10 
2023-07-05 15:59:13 --- Train epoch-16, step-4114 ---
2023-07-05 15:59:13 loss: 0.0168
2023-07-05 15:59:13 --- Eval epoch-16, step-4114 ---
2023-07-05 15:59:13 accuracy: 0.5582
2023-07-05 15:59:13 pr_auc: 0.6103
2023-07-05 15:59:13 roc_auc: 0.5886
2023-07-05 15:59:13 f1: 0.5911
2023-07-05 15:59:13 loss: 2.9058
2023-07-05 15:59:13 
2023-07-05 15:59:16 --- Train epoch-17, step-4356 ---
2023-07-05 15:59:16 loss: 0.0136
2023-07-05 15:59:16 --- Eval epoch-17, step-4356 ---
2023-07-05 15:59:16 accuracy: 0.5663
2023-07-05 15:59:16 pr_auc: 0.6192
2023-07-05 15:59:16 roc_auc: 0.5950
2023-07-05 15:59:16 f1: 0.5909
2023-07-05 15:59:16 loss: 2.8335
2023-07-05 15:59:16 
2023-07-05 15:59:19 --- Train epoch-18, step-4598 ---
2023-07-05 15:59:19 loss: 0.0225
2023-07-05 15:59:19 --- Eval epoch-18, step-4598 ---
2023-07-05 15:59:19 accuracy: 0.5693
2023-07-05 15:59:19 pr_auc: 0.6190
2023-07-05 15:59:19 roc_auc: 0.5980
2023-07-05 15:59:19 f1: 0.6082
2023-07-05 15:59:19 loss: 2.9865
2023-07-05 15:59:19 
2023-07-05 15:59:22 --- Train epoch-19, step-4840 ---
2023-07-05 15:59:22 loss: 0.0306
2023-07-05 15:59:23 --- Eval epoch-19, step-4840 ---
2023-07-05 15:59:23 accuracy: 0.5572
2023-07-05 15:59:23 pr_auc: 0.6161
2023-07-05 15:59:23 roc_auc: 0.5881
2023-07-05 15:59:23 f1: 0.5867
2023-07-05 15:59:23 loss: 3.0033
2023-07-05 15:59:23 Loaded best model
2023-07-05 15:59:24 AdaCare(
  (embeddings): ModuleDict(
    (conditions): Embedding(4031, 128, padding_idx=0)
    (procedures): Embedding(1276, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (adacare): ModuleDict(
    (conditions): AdaCareLayer(
      (nn_conv1): CausalConv1d(128, 64, kernel_size=(2,), stride=(1,), padding=(1,))
      (nn_conv3): CausalConv1d(128, 64, kernel_size=(2,), stride=(1,), padding=(3,), dilation=(3,))
      (nn_conv5): CausalConv1d(128, 64, kernel_size=(2,), stride=(1,), padding=(5,), dilation=(5,))
      (nn_convse): Recalibration(
        (avg_pool): AdaptiveAvgPool1d(output_size=1)
        (nn_c): Linear(in_features=192, out_features=48, bias=True)
        (nn_rescale): Linear(in_features=48, out_features=192, bias=True)
        (sparsemax): Sparsemax()
        (softmax): Softmax(dim=1)
      )
      (nn_inputse): Recalibration(
        (avg_pool): AdaptiveAvgPool1d(output_size=1)
        (nn_c): Linear(in_features=128, out_features=32, bias=True)
        (nn_rescale): Linear(in_features=32, out_features=128, bias=True)
        (sparsemax): Sparsemax()
        (softmax): Softmax(dim=1)
      )
      (rnn): GRU(320, 128)
      (nn_dropout): Dropout(p=0.5, inplace=False)
      (relu): ReLU()
      (sigmoid): Sigmoid()
      (tanh): Tanh()
    )
    (procedures): AdaCareLayer(
      (nn_conv1): CausalConv1d(128, 64, kernel_size=(2,), stride=(1,), padding=(1,))
      (nn_conv3): CausalConv1d(128, 64, kernel_size=(2,), stride=(1,), padding=(3,), dilation=(3,))
      (nn_conv5): CausalConv1d(128, 64, kernel_size=(2,), stride=(1,), padding=(5,), dilation=(5,))
      (nn_convse): Recalibration(
        (avg_pool): AdaptiveAvgPool1d(output_size=1)
        (nn_c): Linear(in_features=192, out_features=48, bias=True)
        (nn_rescale): Linear(in_features=48, out_features=192, bias=True)
        (sparsemax): Sparsemax()
        (softmax): Softmax(dim=1)
      )
      (nn_inputse): Recalibration(
        (avg_pool): AdaptiveAvgPool1d(output_size=1)
        (nn_c): Linear(in_features=128, out_features=32, bias=True)
        (nn_rescale): Linear(in_features=32, out_features=128, bias=True)
        (sparsemax): Sparsemax()
        (softmax): Softmax(dim=1)
      )
      (rnn): GRU(320, 128)
      (nn_dropout): Dropout(p=0.5, inplace=False)
      (relu): ReLU()
      (sigmoid): Sigmoid()
      (tanh): Tanh()
    )
  )
  (fc): Linear(in_features=256, out_features=1, bias=True)
)
2023-07-05 15:59:24 Metrics: ['accuracy', 'pr_auc', 'roc_auc', 'f1']
2023-07-05 15:59:24 Device: cuda
2023-07-05 15:59:24 
2023-07-05 15:59:24 Training:
2023-07-05 15:59:24 Batch size: 32
2023-07-05 15:59:24 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-05 15:59:24 Optimizer params: {'lr': 0.001}
2023-07-05 15:59:24 Weight decay: 0.0
2023-07-05 15:59:24 Max grad norm: None
2023-07-05 15:59:24 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd79593ead0>
2023-07-05 15:59:24 Monitor: roc_auc
2023-07-05 15:59:24 Monitor criterion: max
2023-07-05 15:59:24 Epochs: 20
2023-07-05 15:59:24 
2023-07-05 15:59:27 --- Train epoch-0, step-244 ---
2023-07-05 15:59:27 loss: 0.2647
2023-07-05 15:59:27 --- Eval epoch-0, step-244 ---
2023-07-05 15:59:27 accuracy: 0.9322
2023-07-05 15:59:27 pr_auc: 0.1244
2023-07-05 15:59:27 roc_auc: 0.6430
2023-07-05 15:59:27 f1: 0.0000
2023-07-05 15:59:27 loss: 0.2479
2023-07-05 15:59:27 New best roc_auc score (0.6430) at epoch-0, step-244
2023-07-05 15:59:27 
2023-07-05 15:59:30 --- Train epoch-1, step-488 ---
2023-07-05 15:59:30 loss: 0.2437
2023-07-05 15:59:30 --- Eval epoch-1, step-488 ---
2023-07-05 15:59:30 accuracy: 0.9322
2023-07-05 15:59:30 pr_auc: 0.1233
2023-07-05 15:59:30 roc_auc: 0.6484
2023-07-05 15:59:30 f1: 0.0000
2023-07-05 15:59:30 loss: 0.2411
2023-07-05 15:59:30 New best roc_auc score (0.6484) at epoch-1, step-488
2023-07-05 15:59:30 
2023-07-05 15:59:33 --- Train epoch-2, step-732 ---
2023-07-05 15:59:33 loss: 0.2157
2023-07-05 15:59:33 --- Eval epoch-2, step-732 ---
2023-07-05 15:59:33 accuracy: 0.9311
2023-07-05 15:59:33 pr_auc: 0.1230
2023-07-05 15:59:33 roc_auc: 0.6688
2023-07-05 15:59:33 f1: 0.0000
2023-07-05 15:59:33 loss: 0.2439
2023-07-05 15:59:33 New best roc_auc score (0.6688) at epoch-2, step-732
2023-07-05 15:59:33 
2023-07-05 15:59:36 --- Train epoch-3, step-976 ---
2023-07-05 15:59:36 loss: 0.1671
2023-07-05 15:59:37 --- Eval epoch-3, step-976 ---
2023-07-05 15:59:37 accuracy: 0.9280
2023-07-05 15:59:37 pr_auc: 0.0967
2023-07-05 15:59:37 roc_auc: 0.5915
2023-07-05 15:59:37 f1: 0.0282
2023-07-05 15:59:37 loss: 0.2867
2023-07-05 15:59:37 
2023-07-05 15:59:40 --- Train epoch-4, step-1220 ---
2023-07-05 15:59:40 loss: 0.1116
2023-07-05 15:59:40 --- Eval epoch-4, step-1220 ---
2023-07-05 15:59:40 accuracy: 0.9228
2023-07-05 15:59:40 pr_auc: 0.1017
2023-07-05 15:59:40 roc_auc: 0.6056
2023-07-05 15:59:40 f1: 0.0000
2023-07-05 15:59:40 loss: 0.3192
2023-07-05 15:59:40 
2023-07-05 15:59:43 --- Train epoch-5, step-1464 ---
2023-07-05 15:59:43 loss: 0.0601
2023-07-05 15:59:44 --- Eval epoch-5, step-1464 ---
2023-07-05 15:59:44 accuracy: 0.9196
2023-07-05 15:59:44 pr_auc: 0.1023
2023-07-05 15:59:44 roc_auc: 0.5843
2023-07-05 15:59:44 f1: 0.0723
2023-07-05 15:59:44 loss: 0.4054
2023-07-05 15:59:44 
2023-07-05 15:59:47 --- Train epoch-6, step-1708 ---
2023-07-05 15:59:47 loss: 0.0255
2023-07-05 15:59:47 --- Eval epoch-6, step-1708 ---
2023-07-05 15:59:47 accuracy: 0.9113
2023-07-05 15:59:47 pr_auc: 0.0923
2023-07-05 15:59:47 roc_auc: 0.5868
2023-07-05 15:59:47 f1: 0.0449
2023-07-05 15:59:47 loss: 0.4897
2023-07-05 15:59:47 
2023-07-05 15:59:50 --- Train epoch-7, step-1952 ---
2023-07-05 15:59:50 loss: 0.0091
2023-07-05 15:59:50 --- Eval epoch-7, step-1952 ---
2023-07-05 15:59:50 accuracy: 0.9175
2023-07-05 15:59:50 pr_auc: 0.0999
2023-07-05 15:59:50 roc_auc: 0.6084
2023-07-05 15:59:50 f1: 0.0247
2023-07-05 15:59:50 loss: 0.5461
2023-07-05 15:59:50 
2023-07-05 15:59:53 --- Train epoch-8, step-2196 ---
2023-07-05 15:59:53 loss: 0.0030
2023-07-05 15:59:54 --- Eval epoch-8, step-2196 ---
2023-07-05 15:59:54 accuracy: 0.9217
2023-07-05 15:59:54 pr_auc: 0.0970
2023-07-05 15:59:54 roc_auc: 0.6029
2023-07-05 15:59:54 f1: 0.0000
2023-07-05 15:59:54 loss: 0.5960
2023-07-05 15:59:54 
2023-07-05 15:59:57 --- Train epoch-9, step-2440 ---
2023-07-05 15:59:57 loss: 0.0013
2023-07-05 15:59:57 --- Eval epoch-9, step-2440 ---
2023-07-05 15:59:57 accuracy: 0.9134
2023-07-05 15:59:57 pr_auc: 0.0972
2023-07-05 15:59:57 roc_auc: 0.6041
2023-07-05 15:59:57 f1: 0.0235
2023-07-05 15:59:57 loss: 0.6211
2023-07-05 15:59:57 
2023-07-05 16:00:00 --- Train epoch-10, step-2684 ---
2023-07-05 16:00:00 loss: 0.0006
2023-07-05 16:00:00 --- Eval epoch-10, step-2684 ---
2023-07-05 16:00:00 accuracy: 0.9196
2023-07-05 16:00:00 pr_auc: 0.0993
2023-07-05 16:00:00 roc_auc: 0.6056
2023-07-05 16:00:00 f1: 0.0494
2023-07-05 16:00:00 loss: 0.6539
2023-07-05 16:00:00 
2023-07-05 16:00:03 --- Train epoch-11, step-2928 ---
2023-07-05 16:00:03 loss: 0.0003
2023-07-05 16:00:03 --- Eval epoch-11, step-2928 ---
2023-07-05 16:00:03 accuracy: 0.9196
2023-07-05 16:00:03 pr_auc: 0.0987
2023-07-05 16:00:03 roc_auc: 0.6026
2023-07-05 16:00:03 f1: 0.0494
2023-07-05 16:00:03 loss: 0.6824
2023-07-05 16:00:03 
2023-07-05 16:00:07 --- Train epoch-12, step-3172 ---
2023-07-05 16:00:07 loss: 0.0002
2023-07-05 16:00:07 --- Eval epoch-12, step-3172 ---
2023-07-05 16:00:07 accuracy: 0.9186
2023-07-05 16:00:07 pr_auc: 0.0984
2023-07-05 16:00:07 roc_auc: 0.5988
2023-07-05 16:00:07 f1: 0.0488
2023-07-05 16:00:07 loss: 0.7063
2023-07-05 16:00:07 
2023-07-05 16:00:10 --- Train epoch-13, step-3416 ---
2023-07-05 16:00:10 loss: 0.0002
2023-07-05 16:00:10 --- Eval epoch-13, step-3416 ---
2023-07-05 16:00:10 accuracy: 0.9175
2023-07-05 16:00:10 pr_auc: 0.0981
2023-07-05 16:00:10 roc_auc: 0.5972
2023-07-05 16:00:10 f1: 0.0482
2023-07-05 16:00:10 loss: 0.7187
2023-07-05 16:00:10 
2023-07-05 16:00:13 --- Train epoch-14, step-3660 ---
2023-07-05 16:00:13 loss: 0.0001
2023-07-05 16:00:13 --- Eval epoch-14, step-3660 ---
2023-07-05 16:00:13 accuracy: 0.9207
2023-07-05 16:00:13 pr_auc: 0.0978
2023-07-05 16:00:13 roc_auc: 0.5975
2023-07-05 16:00:13 f1: 0.0500
2023-07-05 16:00:13 loss: 0.7346
2023-07-05 16:00:13 
2023-07-05 16:00:16 --- Train epoch-15, step-3904 ---
2023-07-05 16:00:16 loss: 0.0001
2023-07-05 16:00:16 --- Eval epoch-15, step-3904 ---
2023-07-05 16:00:16 accuracy: 0.9196
2023-07-05 16:00:16 pr_auc: 0.0977
2023-07-05 16:00:16 roc_auc: 0.5995
2023-07-05 16:00:16 f1: 0.0494
2023-07-05 16:00:16 loss: 0.7459
2023-07-05 16:00:16 
2023-07-05 16:00:19 --- Train epoch-16, step-4148 ---
2023-07-05 16:00:19 loss: 0.0001
2023-07-05 16:00:19 --- Eval epoch-16, step-4148 ---
2023-07-05 16:00:19 accuracy: 0.9144
2023-07-05 16:00:19 pr_auc: 0.0992
2023-07-05 16:00:19 roc_auc: 0.5949
2023-07-05 16:00:19 f1: 0.0465
2023-07-05 16:00:19 loss: 0.7560
2023-07-05 16:00:19 
2023-07-05 16:00:22 --- Train epoch-17, step-4392 ---
2023-07-05 16:00:22 loss: 0.0001
2023-07-05 16:00:23 --- Eval epoch-17, step-4392 ---
2023-07-05 16:00:23 accuracy: 0.9217
2023-07-05 16:00:23 pr_auc: 0.0979
2023-07-05 16:00:23 roc_auc: 0.5964
2023-07-05 16:00:23 f1: 0.0506
2023-07-05 16:00:23 loss: 0.7743
2023-07-05 16:00:23 
2023-07-05 16:00:26 --- Train epoch-18, step-4636 ---
2023-07-05 16:00:26 loss: 0.0001
2023-07-05 16:00:26 --- Eval epoch-18, step-4636 ---
2023-07-05 16:00:26 accuracy: 0.9207
2023-07-05 16:00:26 pr_auc: 0.0958
2023-07-05 16:00:26 roc_auc: 0.5937
2023-07-05 16:00:26 f1: 0.0256
2023-07-05 16:00:26 loss: 0.7899
2023-07-05 16:00:26 
2023-07-05 16:00:29 --- Train epoch-19, step-4880 ---
2023-07-05 16:00:29 loss: 0.0000
2023-07-05 16:00:29 --- Eval epoch-19, step-4880 ---
2023-07-05 16:00:29 accuracy: 0.9207
2023-07-05 16:00:29 pr_auc: 0.0960
2023-07-05 16:00:29 roc_auc: 0.5946
2023-07-05 16:00:29 f1: 0.0256
2023-07-05 16:00:29 loss: 0.7988
2023-07-05 16:00:29 Loaded best model
2023-07-05 16:00:36 AdaCare(
  (embeddings): ModuleDict(
    (conditions): Embedding(6664, 128, padding_idx=0)
    (procedures): Embedding(1980, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (adacare): ModuleDict(
    (conditions): AdaCareLayer(
      (nn_conv1): CausalConv1d(128, 64, kernel_size=(2,), stride=(1,), padding=(1,))
      (nn_conv3): CausalConv1d(128, 64, kernel_size=(2,), stride=(1,), padding=(3,), dilation=(3,))
      (nn_conv5): CausalConv1d(128, 64, kernel_size=(2,), stride=(1,), padding=(5,), dilation=(5,))
      (nn_convse): Recalibration(
        (avg_pool): AdaptiveAvgPool1d(output_size=1)
        (nn_c): Linear(in_features=192, out_features=48, bias=True)
        (nn_rescale): Linear(in_features=48, out_features=192, bias=True)
        (sparsemax): Sparsemax()
        (softmax): Softmax(dim=1)
      )
      (nn_inputse): Recalibration(
        (avg_pool): AdaptiveAvgPool1d(output_size=1)
        (nn_c): Linear(in_features=128, out_features=32, bias=True)
        (nn_rescale): Linear(in_features=32, out_features=128, bias=True)
        (sparsemax): Sparsemax()
        (softmax): Softmax(dim=1)
      )
      (rnn): GRU(320, 128)
      (nn_dropout): Dropout(p=0.5, inplace=False)
      (relu): ReLU()
      (sigmoid): Sigmoid()
      (tanh): Tanh()
    )
    (procedures): AdaCareLayer(
      (nn_conv1): CausalConv1d(128, 64, kernel_size=(2,), stride=(1,), padding=(1,))
      (nn_conv3): CausalConv1d(128, 64, kernel_size=(2,), stride=(1,), padding=(3,), dilation=(3,))
      (nn_conv5): CausalConv1d(128, 64, kernel_size=(2,), stride=(1,), padding=(5,), dilation=(5,))
      (nn_convse): Recalibration(
        (avg_pool): AdaptiveAvgPool1d(output_size=1)
        (nn_c): Linear(in_features=192, out_features=48, bias=True)
        (nn_rescale): Linear(in_features=48, out_features=192, bias=True)
        (sparsemax): Sparsemax()
        (softmax): Softmax(dim=1)
      )
      (nn_inputse): Recalibration(
        (avg_pool): AdaptiveAvgPool1d(output_size=1)
        (nn_c): Linear(in_features=128, out_features=32, bias=True)
        (nn_rescale): Linear(in_features=32, out_features=128, bias=True)
        (sparsemax): Sparsemax()
        (softmax): Softmax(dim=1)
      )
      (rnn): GRU(320, 128)
      (nn_dropout): Dropout(p=0.5, inplace=False)
      (relu): ReLU()
      (sigmoid): Sigmoid()
      (tanh): Tanh()
    )
  )
  (fc): Linear(in_features=256, out_features=10, bias=True)
)
2023-07-05 16:00:36 Metrics: ['accuracy', 'f1_macro', 'roc_auc_weighted_ovo']
2023-07-05 16:00:36 Device: cuda
2023-07-05 16:00:36 
2023-07-05 16:00:36 Training:
2023-07-05 16:00:36 Batch size: 32
2023-07-05 16:00:36 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-05 16:00:36 Optimizer params: {'lr': 0.001}
2023-07-05 16:00:36 Weight decay: 0.0
2023-07-05 16:00:36 Max grad norm: None
2023-07-05 16:00:36 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd7205fa110>
2023-07-05 16:00:36 Monitor: accuracy
2023-07-05 16:00:36 Monitor criterion: max
2023-07-05 16:00:36 Epochs: 20
2023-07-05 16:00:36 
2023-07-05 16:00:49 --- Train epoch-0, step-1109 ---
2023-07-05 16:00:49 loss: 1.7731
2023-07-05 16:00:50 --- Eval epoch-0, step-1109 ---
2023-07-05 16:00:50 accuracy: 0.3977
2023-07-05 16:00:50 f1_macro: 0.2498
2023-07-05 16:00:50 roc_auc_weighted_ovo: 0.7743
2023-07-05 16:00:50 loss: 1.6615
2023-07-05 16:00:50 New best accuracy score (0.3977) at epoch-0, step-1109
2023-07-05 16:00:50 
2023-07-05 16:01:04 --- Train epoch-1, step-2218 ---
2023-07-05 16:01:04 loss: 1.6262
2023-07-05 16:01:04 --- Eval epoch-1, step-2218 ---
2023-07-05 16:01:04 accuracy: 0.4026
2023-07-05 16:01:04 f1_macro: 0.2435
2023-07-05 16:01:04 roc_auc_weighted_ovo: 0.7844
2023-07-05 16:01:04 loss: 1.6230
2023-07-05 16:01:04 New best accuracy score (0.4026) at epoch-1, step-2218
2023-07-05 16:01:04 
2023-07-05 16:01:18 --- Train epoch-2, step-3327 ---
2023-07-05 16:01:18 loss: 1.5496
2023-07-05 16:01:19 --- Eval epoch-2, step-3327 ---
2023-07-05 16:01:19 accuracy: 0.4068
2023-07-05 16:01:19 f1_macro: 0.2531
2023-07-05 16:01:19 roc_auc_weighted_ovo: 0.7888
2023-07-05 16:01:19 loss: 1.6079
2023-07-05 16:01:19 New best accuracy score (0.4068) at epoch-2, step-3327
2023-07-05 16:01:19 
2023-07-05 16:01:33 --- Train epoch-3, step-4436 ---
2023-07-05 16:01:33 loss: 1.4794
2023-07-05 16:01:33 --- Eval epoch-3, step-4436 ---
2023-07-05 16:01:33 accuracy: 0.4084
2023-07-05 16:01:33 f1_macro: 0.2535
2023-07-05 16:01:33 roc_auc_weighted_ovo: 0.7881
2023-07-05 16:01:33 loss: 1.6275
2023-07-05 16:01:33 New best accuracy score (0.4084) at epoch-3, step-4436
2023-07-05 16:01:33 
2023-07-05 16:01:47 --- Train epoch-4, step-5545 ---
2023-07-05 16:01:47 loss: 1.4085
2023-07-05 16:01:48 --- Eval epoch-4, step-5545 ---
2023-07-05 16:01:48 accuracy: 0.3955
2023-07-05 16:01:48 f1_macro: 0.2563
2023-07-05 16:01:48 roc_auc_weighted_ovo: 0.7865
2023-07-05 16:01:48 loss: 1.6691
2023-07-05 16:01:48 
2023-07-05 16:02:01 --- Train epoch-5, step-6654 ---
2023-07-05 16:02:01 loss: 1.3305
2023-07-05 16:02:02 --- Eval epoch-5, step-6654 ---
2023-07-05 16:02:02 accuracy: 0.4048
2023-07-05 16:02:02 f1_macro: 0.2684
2023-07-05 16:02:02 roc_auc_weighted_ovo: 0.7835
2023-07-05 16:02:02 loss: 1.7110
2023-07-05 16:02:02 
2023-07-05 16:02:15 --- Train epoch-6, step-7763 ---
2023-07-05 16:02:15 loss: 1.2452
2023-07-05 16:02:16 --- Eval epoch-6, step-7763 ---
2023-07-05 16:02:16 accuracy: 0.3846
2023-07-05 16:02:16 f1_macro: 0.2612
2023-07-05 16:02:16 roc_auc_weighted_ovo: 0.7790
2023-07-05 16:02:16 loss: 1.8098
2023-07-05 16:02:16 
2023-07-05 16:02:30 --- Train epoch-7, step-8872 ---
2023-07-05 16:02:30 loss: 1.1591
2023-07-05 16:02:31 --- Eval epoch-7, step-8872 ---
2023-07-05 16:02:31 accuracy: 0.3719
2023-07-05 16:02:31 f1_macro: 0.2575
2023-07-05 16:02:31 roc_auc_weighted_ovo: 0.7729
2023-07-05 16:02:31 loss: 1.9171
2023-07-05 16:02:31 
2023-07-05 16:02:44 --- Train epoch-8, step-9981 ---
2023-07-05 16:02:44 loss: 1.0744
2023-07-05 16:02:45 --- Eval epoch-8, step-9981 ---
2023-07-05 16:02:45 accuracy: 0.3815
2023-07-05 16:02:45 f1_macro: 0.2685
2023-07-05 16:02:45 roc_auc_weighted_ovo: 0.7678
2023-07-05 16:02:45 loss: 2.0247
2023-07-05 16:02:45 
2023-07-05 16:02:59 --- Train epoch-9, step-11090 ---
2023-07-05 16:02:59 loss: 0.9990
2023-07-05 16:02:59 --- Eval epoch-9, step-11090 ---
2023-07-05 16:02:59 accuracy: 0.3661
2023-07-05 16:02:59 f1_macro: 0.2620
2023-07-05 16:02:59 roc_auc_weighted_ovo: 0.7648
2023-07-05 16:02:59 loss: 2.1964
2023-07-05 16:02:59 
2023-07-05 16:03:13 --- Train epoch-10, step-12199 ---
2023-07-05 16:03:13 loss: 0.9216
2023-07-05 16:03:14 --- Eval epoch-10, step-12199 ---
2023-07-05 16:03:14 accuracy: 0.3679
2023-07-05 16:03:14 f1_macro: 0.2682
2023-07-05 16:03:14 roc_auc_weighted_ovo: 0.7647
2023-07-05 16:03:14 loss: 2.2808
2023-07-05 16:03:14 
2023-07-05 16:03:28 --- Train epoch-11, step-13308 ---
2023-07-05 16:03:28 loss: 0.8625
2023-07-05 16:03:28 --- Eval epoch-11, step-13308 ---
2023-07-05 16:03:28 accuracy: 0.3563
2023-07-05 16:03:28 f1_macro: 0.2525
2023-07-05 16:03:28 roc_auc_weighted_ovo: 0.7582
2023-07-05 16:03:28 loss: 2.4620
2023-07-05 16:03:28 
2023-07-05 16:03:42 --- Train epoch-12, step-14417 ---
2023-07-05 16:03:42 loss: 0.7959
2023-07-05 16:03:42 --- Eval epoch-12, step-14417 ---
2023-07-05 16:03:42 accuracy: 0.3483
2023-07-05 16:03:42 f1_macro: 0.2496
2023-07-05 16:03:42 roc_auc_weighted_ovo: 0.7559
2023-07-05 16:03:42 loss: 2.6101
2023-07-05 16:03:42 
2023-07-05 16:03:56 --- Train epoch-13, step-15526 ---
2023-07-05 16:03:56 loss: 0.7360
2023-07-05 16:03:56 --- Eval epoch-13, step-15526 ---
2023-07-05 16:03:56 accuracy: 0.3450
2023-07-05 16:03:56 f1_macro: 0.2514
2023-07-05 16:03:56 roc_auc_weighted_ovo: 0.7550
2023-07-05 16:03:56 loss: 2.7763
2023-07-05 16:03:56 
2023-07-05 16:04:10 --- Train epoch-14, step-16635 ---
2023-07-05 16:04:10 loss: 0.6963
2023-07-05 16:04:10 --- Eval epoch-14, step-16635 ---
2023-07-05 16:04:10 accuracy: 0.3405
2023-07-05 16:04:10 f1_macro: 0.2583
2023-07-05 16:04:10 roc_auc_weighted_ovo: 0.7546
2023-07-05 16:04:10 loss: 2.8580
2023-07-05 16:04:10 
2023-07-05 16:04:27 --- Train epoch-15, step-17744 ---
2023-07-05 16:04:27 loss: 0.6526
2023-07-05 16:04:28 --- Eval epoch-15, step-17744 ---
2023-07-05 16:04:28 accuracy: 0.3427
2023-07-05 16:04:28 f1_macro: 0.2460
2023-07-05 16:04:28 roc_auc_weighted_ovo: 0.7506
2023-07-05 16:04:28 loss: 3.0367
2023-07-05 16:04:28 
2023-07-05 16:04:42 --- Train epoch-16, step-18853 ---
2023-07-05 16:04:42 loss: 0.6006
2023-07-05 16:04:43 --- Eval epoch-16, step-18853 ---
2023-07-05 16:04:43 accuracy: 0.3470
2023-07-05 16:04:43 f1_macro: 0.2576
2023-07-05 16:04:43 roc_auc_weighted_ovo: 0.7529
2023-07-05 16:04:43 loss: 3.1330
2023-07-05 16:04:43 
2023-07-05 16:04:57 --- Train epoch-17, step-19962 ---
2023-07-05 16:04:57 loss: 0.5641
2023-07-05 16:04:58 --- Eval epoch-17, step-19962 ---
2023-07-05 16:04:58 accuracy: 0.3396
2023-07-05 16:04:58 f1_macro: 0.2513
2023-07-05 16:04:58 roc_auc_weighted_ovo: 0.7513
2023-07-05 16:04:58 loss: 3.2738
2023-07-05 16:04:58 
2023-07-05 16:05:11 --- Train epoch-18, step-21071 ---
2023-07-05 16:05:11 loss: 0.5319
2023-07-05 16:05:12 --- Eval epoch-18, step-21071 ---
2023-07-05 16:05:12 accuracy: 0.3392
2023-07-05 16:05:12 f1_macro: 0.2550
2023-07-05 16:05:12 roc_auc_weighted_ovo: 0.7499
2023-07-05 16:05:12 loss: 3.3804
2023-07-05 16:05:12 
2023-07-05 16:05:25 --- Train epoch-19, step-22180 ---
2023-07-05 16:05:25 loss: 0.5026
2023-07-05 16:05:26 --- Eval epoch-19, step-22180 ---
2023-07-05 16:05:26 accuracy: 0.3463
2023-07-05 16:05:26 f1_macro: 0.2488
2023-07-05 16:05:26 roc_auc_weighted_ovo: 0.7492
2023-07-05 16:05:26 loss: 3.4642
2023-07-05 16:05:26 Loaded best model
2023-07-05 16:05:28 Transformer(
  (embeddings): ModuleDict(
    (conditions): Embedding(4031, 128, padding_idx=0)
    (procedures): Embedding(1276, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (transformer): ModuleDict(
    (conditions): TransformerLayer(
      (transformer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=128, out_features=128, bias=False)
              (1): Linear(in_features=128, out_features=128, bias=False)
              (2): Linear(in_features=128, out_features=128, bias=False)
            )
            (output_linear): Linear(in_features=128, out_features=128, bias=False)
            (attention): Attention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=128, out_features=512, bias=True)
            (w_2): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.5, inplace=False)
            (activation): GELU(approximate=none)
          )
          (input_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (output_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (procedures): TransformerLayer(
      (transformer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=128, out_features=128, bias=False)
              (1): Linear(in_features=128, out_features=128, bias=False)
              (2): Linear(in_features=128, out_features=128, bias=False)
            )
            (output_linear): Linear(in_features=128, out_features=128, bias=False)
            (attention): Attention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=128, out_features=512, bias=True)
            (w_2): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.5, inplace=False)
            (activation): GELU(approximate=none)
          )
          (input_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (output_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (fc): Linear(in_features=256, out_features=1, bias=True)
)
2023-07-05 16:05:28 Metrics: ['accuracy', 'pr_auc', 'roc_auc', 'f1']
2023-07-05 16:05:28 Device: cuda
2023-07-05 16:05:28 
2023-07-05 16:05:28 Training:
2023-07-05 16:05:28 Batch size: 32
2023-07-05 16:05:28 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-05 16:05:28 Optimizer params: {'lr': 0.001}
2023-07-05 16:05:28 Weight decay: 0.0
2023-07-05 16:05:28 Max grad norm: None
2023-07-05 16:05:28 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd7210bced0>
2023-07-05 16:05:28 Monitor: roc_auc
2023-07-05 16:05:28 Monitor criterion: max
2023-07-05 16:05:28 Epochs: 20
2023-07-05 16:05:28 
2023-07-05 16:05:30 --- Train epoch-0, step-243 ---
2023-07-05 16:05:30 loss: 0.8649
2023-07-05 16:05:31 --- Eval epoch-0, step-243 ---
2023-07-05 16:05:31 accuracy: 0.5970
2023-07-05 16:05:31 pr_auc: 0.6502
2023-07-05 16:05:31 roc_auc: 0.6033
2023-07-05 16:05:31 f1: 0.6753
2023-07-05 16:05:31 loss: 0.6759
2023-07-05 16:05:31 New best roc_auc score (0.6033) at epoch-0, step-243
2023-07-05 16:05:31 
2023-07-05 16:05:33 --- Train epoch-1, step-486 ---
2023-07-05 16:05:33 loss: 0.6798
2023-07-05 16:05:33 --- Eval epoch-1, step-486 ---
2023-07-05 16:05:33 accuracy: 0.5859
2023-07-05 16:05:33 pr_auc: 0.6546
2023-07-05 16:05:33 roc_auc: 0.6114
2023-07-05 16:05:33 f1: 0.6741
2023-07-05 16:05:33 loss: 0.6773
2023-07-05 16:05:33 New best roc_auc score (0.6114) at epoch-1, step-486
2023-07-05 16:05:33 
2023-07-05 16:05:36 --- Train epoch-2, step-729 ---
2023-07-05 16:05:36 loss: 0.6661
2023-07-05 16:05:36 --- Eval epoch-2, step-729 ---
2023-07-05 16:05:36 accuracy: 0.5789
2023-07-05 16:05:36 pr_auc: 0.6694
2023-07-05 16:05:36 roc_auc: 0.6233
2023-07-05 16:05:36 f1: 0.6242
2023-07-05 16:05:36 loss: 0.6700
2023-07-05 16:05:36 New best roc_auc score (0.6233) at epoch-2, step-729
2023-07-05 16:05:36 
2023-07-05 16:05:38 --- Train epoch-3, step-972 ---
2023-07-05 16:05:38 loss: 0.6519
2023-07-05 16:05:38 --- Eval epoch-3, step-972 ---
2023-07-05 16:05:38 accuracy: 0.5990
2023-07-05 16:05:38 pr_auc: 0.6942
2023-07-05 16:05:38 roc_auc: 0.6428
2023-07-05 16:05:38 f1: 0.6122
2023-07-05 16:05:38 loss: 0.6808
2023-07-05 16:05:38 New best roc_auc score (0.6428) at epoch-3, step-972
2023-07-05 16:05:38 
2023-07-05 16:05:40 --- Train epoch-4, step-1215 ---
2023-07-05 16:05:40 loss: 0.6327
2023-07-05 16:05:41 --- Eval epoch-4, step-1215 ---
2023-07-05 16:05:41 accuracy: 0.6040
2023-07-05 16:05:41 pr_auc: 0.7093
2023-07-05 16:05:41 roc_auc: 0.6511
2023-07-05 16:05:41 f1: 0.6311
2023-07-05 16:05:41 loss: 0.6598
2023-07-05 16:05:41 New best roc_auc score (0.6511) at epoch-4, step-1215
2023-07-05 16:05:41 
2023-07-05 16:05:43 --- Train epoch-5, step-1458 ---
2023-07-05 16:05:43 loss: 0.6158
2023-07-05 16:05:43 --- Eval epoch-5, step-1458 ---
2023-07-05 16:05:43 accuracy: 0.5980
2023-07-05 16:05:43 pr_auc: 0.6956
2023-07-05 16:05:43 roc_auc: 0.6436
2023-07-05 16:05:43 f1: 0.6403
2023-07-05 16:05:43 loss: 0.6893
2023-07-05 16:05:43 
2023-07-05 16:05:45 --- Train epoch-6, step-1701 ---
2023-07-05 16:05:45 loss: 0.5866
2023-07-05 16:05:46 --- Eval epoch-6, step-1701 ---
2023-07-05 16:05:46 accuracy: 0.6151
2023-07-05 16:05:46 pr_auc: 0.7052
2023-07-05 16:05:46 roc_auc: 0.6532
2023-07-05 16:05:46 f1: 0.6724
2023-07-05 16:05:46 loss: 0.6937
2023-07-05 16:05:46 New best roc_auc score (0.6532) at epoch-6, step-1701
2023-07-05 16:05:46 
2023-07-05 16:05:48 --- Train epoch-7, step-1944 ---
2023-07-05 16:05:48 loss: 0.5579
2023-07-05 16:05:48 --- Eval epoch-7, step-1944 ---
2023-07-05 16:05:48 accuracy: 0.5970
2023-07-05 16:05:48 pr_auc: 0.6924
2023-07-05 16:05:48 roc_auc: 0.6336
2023-07-05 16:05:48 f1: 0.6404
2023-07-05 16:05:48 loss: 0.7129
2023-07-05 16:05:48 
2023-07-05 16:05:50 --- Train epoch-8, step-2187 ---
2023-07-05 16:05:50 loss: 0.5206
2023-07-05 16:05:51 --- Eval epoch-8, step-2187 ---
2023-07-05 16:05:51 accuracy: 0.6080
2023-07-05 16:05:51 pr_auc: 0.7059
2023-07-05 16:05:51 roc_auc: 0.6434
2023-07-05 16:05:51 f1: 0.6389
2023-07-05 16:05:51 loss: 0.7257
2023-07-05 16:05:51 
2023-07-05 16:05:53 --- Train epoch-9, step-2430 ---
2023-07-05 16:05:53 loss: 0.4787
2023-07-05 16:05:53 --- Eval epoch-9, step-2430 ---
2023-07-05 16:05:53 accuracy: 0.5950
2023-07-05 16:05:53 pr_auc: 0.6981
2023-07-05 16:05:53 roc_auc: 0.6378
2023-07-05 16:05:53 f1: 0.6279
2023-07-05 16:05:53 loss: 0.8141
2023-07-05 16:05:53 
2023-07-05 16:05:55 --- Train epoch-10, step-2673 ---
2023-07-05 16:05:55 loss: 0.4314
2023-07-05 16:05:55 --- Eval epoch-10, step-2673 ---
2023-07-05 16:05:55 accuracy: 0.5970
2023-07-05 16:05:55 pr_auc: 0.6998
2023-07-05 16:05:55 roc_auc: 0.6367
2023-07-05 16:05:55 f1: 0.6461
2023-07-05 16:05:55 loss: 0.8296
2023-07-05 16:05:55 
2023-07-05 16:05:58 --- Train epoch-11, step-2916 ---
2023-07-05 16:05:58 loss: 0.3928
2023-07-05 16:05:58 --- Eval epoch-11, step-2916 ---
2023-07-05 16:05:58 accuracy: 0.5879
2023-07-05 16:05:58 pr_auc: 0.6826
2023-07-05 16:05:58 roc_auc: 0.6219
2023-07-05 16:05:58 f1: 0.6378
2023-07-05 16:05:58 loss: 0.9467
2023-07-05 16:05:58 
2023-07-05 16:06:00 --- Train epoch-12, step-3159 ---
2023-07-05 16:06:00 loss: 0.3416
2023-07-05 16:06:00 --- Eval epoch-12, step-3159 ---
2023-07-05 16:06:00 accuracy: 0.5940
2023-07-05 16:06:00 pr_auc: 0.6894
2023-07-05 16:06:00 roc_auc: 0.6305
2023-07-05 16:06:00 f1: 0.6085
2023-07-05 16:06:00 loss: 1.0391
2023-07-05 16:06:00 
2023-07-05 16:06:03 --- Train epoch-13, step-3402 ---
2023-07-05 16:06:03 loss: 0.3036
2023-07-05 16:06:03 --- Eval epoch-13, step-3402 ---
2023-07-05 16:06:03 accuracy: 0.5889
2023-07-05 16:06:03 pr_auc: 0.6750
2023-07-05 16:06:03 roc_auc: 0.6223
2023-07-05 16:06:03 f1: 0.6188
2023-07-05 16:06:03 loss: 1.1311
2023-07-05 16:06:03 
2023-07-05 16:06:06 --- Train epoch-14, step-3645 ---
2023-07-05 16:06:06 loss: 0.2659
2023-07-05 16:06:06 --- Eval epoch-14, step-3645 ---
2023-07-05 16:06:06 accuracy: 0.6010
2023-07-05 16:06:06 pr_auc: 0.6896
2023-07-05 16:06:06 roc_auc: 0.6348
2023-07-05 16:06:06 f1: 0.6477
2023-07-05 16:06:06 loss: 1.2260
2023-07-05 16:06:06 
2023-07-05 16:06:08 --- Train epoch-15, step-3888 ---
2023-07-05 16:06:08 loss: 0.2288
2023-07-05 16:06:08 --- Eval epoch-15, step-3888 ---
2023-07-05 16:06:08 accuracy: 0.5980
2023-07-05 16:06:08 pr_auc: 0.6912
2023-07-05 16:06:08 roc_auc: 0.6321
2023-07-05 16:06:08 f1: 0.6503
2023-07-05 16:06:08 loss: 1.3827
2023-07-05 16:06:08 
2023-07-05 16:06:11 --- Train epoch-16, step-4131 ---
2023-07-05 16:06:11 loss: 0.2036
2023-07-05 16:06:11 --- Eval epoch-16, step-4131 ---
2023-07-05 16:06:11 accuracy: 0.5839
2023-07-05 16:06:11 pr_auc: 0.6813
2023-07-05 16:06:11 roc_auc: 0.6225
2023-07-05 16:06:11 f1: 0.6230
2023-07-05 16:06:11 loss: 1.4274
2023-07-05 16:06:11 
2023-07-05 16:06:13 --- Train epoch-17, step-4374 ---
2023-07-05 16:06:13 loss: 0.1729
2023-07-05 16:06:14 --- Eval epoch-17, step-4374 ---
2023-07-05 16:06:14 accuracy: 0.5859
2023-07-05 16:06:14 pr_auc: 0.6855
2023-07-05 16:06:14 roc_auc: 0.6286
2023-07-05 16:06:14 f1: 0.6178
2023-07-05 16:06:14 loss: 1.6495
2023-07-05 16:06:14 
2023-07-05 16:06:16 --- Train epoch-18, step-4617 ---
2023-07-05 16:06:16 loss: 0.1685
2023-07-05 16:06:16 --- Eval epoch-18, step-4617 ---
2023-07-05 16:06:16 accuracy: 0.5799
2023-07-05 16:06:16 pr_auc: 0.6693
2023-07-05 16:06:16 roc_auc: 0.6129
2023-07-05 16:06:16 f1: 0.6200
2023-07-05 16:06:16 loss: 1.8386
2023-07-05 16:06:16 
2023-07-05 16:06:18 --- Train epoch-19, step-4860 ---
2023-07-05 16:06:18 loss: 0.1525
2023-07-05 16:06:18 --- Eval epoch-19, step-4860 ---
2023-07-05 16:06:18 accuracy: 0.6020
2023-07-05 16:06:18 pr_auc: 0.6853
2023-07-05 16:06:18 roc_auc: 0.6249
2023-07-05 16:06:18 f1: 0.6610
2023-07-05 16:06:18 loss: 1.8517
2023-07-05 16:06:18 Loaded best model
2023-07-05 16:06:20 Transformer(
  (embeddings): ModuleDict(
    (conditions): Embedding(4031, 128, padding_idx=0)
    (procedures): Embedding(1276, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (transformer): ModuleDict(
    (conditions): TransformerLayer(
      (transformer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=128, out_features=128, bias=False)
              (1): Linear(in_features=128, out_features=128, bias=False)
              (2): Linear(in_features=128, out_features=128, bias=False)
            )
            (output_linear): Linear(in_features=128, out_features=128, bias=False)
            (attention): Attention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=128, out_features=512, bias=True)
            (w_2): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.5, inplace=False)
            (activation): GELU(approximate=none)
          )
          (input_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (output_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (procedures): TransformerLayer(
      (transformer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=128, out_features=128, bias=False)
              (1): Linear(in_features=128, out_features=128, bias=False)
              (2): Linear(in_features=128, out_features=128, bias=False)
            )
            (output_linear): Linear(in_features=128, out_features=128, bias=False)
            (attention): Attention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=128, out_features=512, bias=True)
            (w_2): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.5, inplace=False)
            (activation): GELU(approximate=none)
          )
          (input_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (output_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (fc): Linear(in_features=256, out_features=1, bias=True)
)
2023-07-05 16:06:20 Metrics: ['accuracy', 'pr_auc', 'roc_auc', 'f1']
2023-07-05 16:06:20 Device: cuda
2023-07-05 16:06:20 
2023-07-05 16:06:20 Training:
2023-07-05 16:06:20 Batch size: 32
2023-07-05 16:06:20 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-05 16:06:20 Optimizer params: {'lr': 0.001}
2023-07-05 16:06:20 Weight decay: 0.0
2023-07-05 16:06:20 Max grad norm: None
2023-07-05 16:06:20 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd7202d1890>
2023-07-05 16:06:20 Monitor: roc_auc
2023-07-05 16:06:20 Monitor criterion: max
2023-07-05 16:06:20 Epochs: 20
2023-07-05 16:06:20 
2023-07-05 16:06:22 --- Train epoch-0, step-241 ---
2023-07-05 16:06:22 loss: 0.3616
2023-07-05 16:06:22 --- Eval epoch-0, step-241 ---
2023-07-05 16:06:22 accuracy: 0.9387
2023-07-05 16:06:22 pr_auc: 0.0802
2023-07-05 16:06:22 roc_auc: 0.5838
2023-07-05 16:06:22 f1: 0.0000
2023-07-05 16:06:22 loss: 0.2377
2023-07-05 16:06:22 New best roc_auc score (0.5838) at epoch-0, step-241
2023-07-05 16:06:22 
2023-07-05 16:06:25 --- Train epoch-1, step-482 ---
2023-07-05 16:06:25 loss: 0.2719
2023-07-05 16:06:25 --- Eval epoch-1, step-482 ---
2023-07-05 16:06:25 accuracy: 0.9387
2023-07-05 16:06:25 pr_auc: 0.1191
2023-07-05 16:06:25 roc_auc: 0.6321
2023-07-05 16:06:25 f1: 0.0000
2023-07-05 16:06:25 loss: 0.2254
2023-07-05 16:06:25 New best roc_auc score (0.6321) at epoch-1, step-482
2023-07-05 16:06:25 
2023-07-05 16:06:27 --- Train epoch-2, step-723 ---
2023-07-05 16:06:27 loss: 0.2456
2023-07-05 16:06:27 --- Eval epoch-2, step-723 ---
2023-07-05 16:06:27 accuracy: 0.9387
2023-07-05 16:06:27 pr_auc: 0.1377
2023-07-05 16:06:27 roc_auc: 0.6489
2023-07-05 16:06:27 f1: 0.0000
2023-07-05 16:06:27 loss: 0.2273
2023-07-05 16:06:27 New best roc_auc score (0.6489) at epoch-2, step-723
2023-07-05 16:06:27 
2023-07-05 16:06:29 --- Train epoch-3, step-964 ---
2023-07-05 16:06:29 loss: 0.2284
2023-07-05 16:06:29 --- Eval epoch-3, step-964 ---
2023-07-05 16:06:29 accuracy: 0.9387
2023-07-05 16:06:29 pr_auc: 0.1052
2023-07-05 16:06:29 roc_auc: 0.6049
2023-07-05 16:06:29 f1: 0.0323
2023-07-05 16:06:30 loss: 0.2469
2023-07-05 16:06:30 
2023-07-05 16:06:32 --- Train epoch-4, step-1205 ---
2023-07-05 16:06:32 loss: 0.2164
2023-07-05 16:06:32 --- Eval epoch-4, step-1205 ---
2023-07-05 16:06:32 accuracy: 0.9367
2023-07-05 16:06:32 pr_auc: 0.0803
2023-07-05 16:06:32 roc_auc: 0.5863
2023-07-05 16:06:32 f1: 0.0000
2023-07-05 16:06:32 loss: 0.2595
2023-07-05 16:06:32 
2023-07-05 16:06:34 --- Train epoch-5, step-1446 ---
2023-07-05 16:06:34 loss: 0.1943
2023-07-05 16:06:34 --- Eval epoch-5, step-1446 ---
2023-07-05 16:06:34 accuracy: 0.9336
2023-07-05 16:06:34 pr_auc: 0.0916
2023-07-05 16:06:34 roc_auc: 0.5706
2023-07-05 16:06:34 f1: 0.0580
2023-07-05 16:06:34 loss: 0.3048
2023-07-05 16:06:34 
2023-07-05 16:06:37 --- Train epoch-6, step-1687 ---
2023-07-05 16:06:37 loss: 0.1680
2023-07-05 16:06:37 --- Eval epoch-6, step-1687 ---
2023-07-05 16:06:37 accuracy: 0.9275
2023-07-05 16:06:37 pr_auc: 0.0939
2023-07-05 16:06:37 roc_auc: 0.5599
2023-07-05 16:06:37 f1: 0.0533
2023-07-05 16:06:37 loss: 0.3190
2023-07-05 16:06:37 
2023-07-05 16:06:39 --- Train epoch-7, step-1928 ---
2023-07-05 16:06:39 loss: 0.1420
2023-07-05 16:06:39 --- Eval epoch-7, step-1928 ---
2023-07-05 16:06:39 accuracy: 0.9173
2023-07-05 16:06:39 pr_auc: 0.0793
2023-07-05 16:06:39 roc_auc: 0.5745
2023-07-05 16:06:39 f1: 0.0000
2023-07-05 16:06:39 loss: 0.3670
2023-07-05 16:06:39 
2023-07-05 16:06:42 --- Train epoch-8, step-2169 ---
2023-07-05 16:06:42 loss: 0.1111
2023-07-05 16:06:42 --- Eval epoch-8, step-2169 ---
2023-07-05 16:06:42 accuracy: 0.9152
2023-07-05 16:06:42 pr_auc: 0.0782
2023-07-05 16:06:42 roc_auc: 0.5549
2023-07-05 16:06:42 f1: 0.0879
2023-07-05 16:06:42 loss: 0.4400
2023-07-05 16:06:42 
2023-07-05 16:06:44 --- Train epoch-9, step-2410 ---
2023-07-05 16:06:44 loss: 0.0904
2023-07-05 16:06:44 --- Eval epoch-9, step-2410 ---
2023-07-05 16:06:44 accuracy: 0.9193
2023-07-05 16:06:44 pr_auc: 0.0985
2023-07-05 16:06:44 roc_auc: 0.5745
2023-07-05 16:06:44 f1: 0.0482
2023-07-05 16:06:44 loss: 0.5106
2023-07-05 16:06:44 
2023-07-05 16:06:47 --- Train epoch-10, step-2651 ---
2023-07-05 16:06:47 loss: 0.0729
2023-07-05 16:06:47 --- Eval epoch-10, step-2651 ---
2023-07-05 16:06:47 accuracy: 0.9152
2023-07-05 16:06:47 pr_auc: 0.0891
2023-07-05 16:06:47 roc_auc: 0.5838
2023-07-05 16:06:47 f1: 0.0674
2023-07-05 16:06:47 loss: 0.5737
2023-07-05 16:06:47 
2023-07-05 16:06:49 --- Train epoch-11, step-2892 ---
2023-07-05 16:06:49 loss: 0.0541
2023-07-05 16:06:49 --- Eval epoch-11, step-2892 ---
2023-07-05 16:06:49 accuracy: 0.9162
2023-07-05 16:06:49 pr_auc: 0.0874
2023-07-05 16:06:49 roc_auc: 0.5561
2023-07-05 16:06:49 f1: 0.0465
2023-07-05 16:06:49 loss: 0.7129
2023-07-05 16:06:49 
2023-07-05 16:06:52 --- Train epoch-12, step-3133 ---
2023-07-05 16:06:52 loss: 0.0501
2023-07-05 16:06:52 --- Eval epoch-12, step-3133 ---
2023-07-05 16:06:52 accuracy: 0.9101
2023-07-05 16:06:52 pr_auc: 0.0793
2023-07-05 16:06:52 roc_auc: 0.5561
2023-07-05 16:06:52 f1: 0.0638
2023-07-05 16:06:52 loss: 0.7781
2023-07-05 16:06:52 
2023-07-05 16:06:54 --- Train epoch-13, step-3374 ---
2023-07-05 16:06:54 loss: 0.0393
2023-07-05 16:06:54 --- Eval epoch-13, step-3374 ---
2023-07-05 16:06:54 accuracy: 0.9183
2023-07-05 16:06:54 pr_auc: 0.0783
2023-07-05 16:06:54 roc_auc: 0.5515
2023-07-05 16:06:54 f1: 0.0909
2023-07-05 16:06:54 loss: 0.9498
2023-07-05 16:06:54 
2023-07-05 16:06:57 --- Train epoch-14, step-3615 ---
2023-07-05 16:06:57 loss: 0.0351
2023-07-05 16:06:57 --- Eval epoch-14, step-3615 ---
2023-07-05 16:06:57 accuracy: 0.8917
2023-07-05 16:06:57 pr_auc: 0.0738
2023-07-05 16:06:57 roc_auc: 0.5548
2023-07-05 16:06:57 f1: 0.0702
2023-07-05 16:06:57 loss: 0.9520
2023-07-05 16:06:57 
2023-07-05 16:06:59 --- Train epoch-15, step-3856 ---
2023-07-05 16:06:59 loss: 0.0324
2023-07-05 16:06:59 --- Eval epoch-15, step-3856 ---
2023-07-05 16:06:59 accuracy: 0.9050
2023-07-05 16:06:59 pr_auc: 0.0755
2023-07-05 16:06:59 roc_auc: 0.5798
2023-07-05 16:06:59 f1: 0.0412
2023-07-05 16:06:59 loss: 0.9797
2023-07-05 16:06:59 
2023-07-05 16:07:02 --- Train epoch-16, step-4097 ---
2023-07-05 16:07:02 loss: 0.0331
2023-07-05 16:07:02 --- Eval epoch-16, step-4097 ---
2023-07-05 16:07:02 accuracy: 0.9091
2023-07-05 16:07:02 pr_auc: 0.0977
2023-07-05 16:07:02 roc_auc: 0.5825
2023-07-05 16:07:02 f1: 0.0825
2023-07-05 16:07:02 loss: 1.0053
2023-07-05 16:07:02 
2023-07-05 16:07:04 --- Train epoch-17, step-4338 ---
2023-07-05 16:07:04 loss: 0.0222
2023-07-05 16:07:04 --- Eval epoch-17, step-4338 ---
2023-07-05 16:07:04 accuracy: 0.8999
2023-07-05 16:07:04 pr_auc: 0.0794
2023-07-05 16:07:04 roc_auc: 0.5625
2023-07-05 16:07:04 f1: 0.0577
2023-07-05 16:07:04 loss: 1.1297
2023-07-05 16:07:04 
2023-07-05 16:07:07 --- Train epoch-18, step-4579 ---
2023-07-05 16:07:07 loss: 0.0247
2023-07-05 16:07:07 --- Eval epoch-18, step-4579 ---
2023-07-05 16:07:07 accuracy: 0.9152
2023-07-05 16:07:07 pr_auc: 0.0772
2023-07-05 16:07:07 roc_auc: 0.5621
2023-07-05 16:07:07 f1: 0.0460
2023-07-05 16:07:07 loss: 1.2463
2023-07-05 16:07:07 
2023-07-05 16:07:09 --- Train epoch-19, step-4820 ---
2023-07-05 16:07:09 loss: 0.0215
2023-07-05 16:07:09 --- Eval epoch-19, step-4820 ---
2023-07-05 16:07:09 accuracy: 0.9111
2023-07-05 16:07:09 pr_auc: 0.0758
2023-07-05 16:07:09 roc_auc: 0.5515
2023-07-05 16:07:09 f1: 0.0645
2023-07-05 16:07:09 loss: 1.5343
2023-07-05 16:07:09 Loaded best model
2023-07-05 16:07:15 Transformer(
  (embeddings): ModuleDict(
    (conditions): Embedding(6664, 128, padding_idx=0)
    (procedures): Embedding(1980, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (transformer): ModuleDict(
    (conditions): TransformerLayer(
      (transformer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=128, out_features=128, bias=False)
              (1): Linear(in_features=128, out_features=128, bias=False)
              (2): Linear(in_features=128, out_features=128, bias=False)
            )
            (output_linear): Linear(in_features=128, out_features=128, bias=False)
            (attention): Attention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=128, out_features=512, bias=True)
            (w_2): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.5, inplace=False)
            (activation): GELU(approximate=none)
          )
          (input_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (output_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (procedures): TransformerLayer(
      (transformer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=128, out_features=128, bias=False)
              (1): Linear(in_features=128, out_features=128, bias=False)
              (2): Linear(in_features=128, out_features=128, bias=False)
            )
            (output_linear): Linear(in_features=128, out_features=128, bias=False)
            (attention): Attention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=128, out_features=512, bias=True)
            (w_2): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.5, inplace=False)
            (activation): GELU(approximate=none)
          )
          (input_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (output_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (fc): Linear(in_features=256, out_features=10, bias=True)
)
2023-07-05 16:07:15 Metrics: ['accuracy', 'f1_macro', 'roc_auc_weighted_ovo']
2023-07-05 16:07:15 Device: cuda
2023-07-05 16:07:15 
2023-07-05 16:07:15 Training:
2023-07-05 16:07:15 Batch size: 32
2023-07-05 16:07:15 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-05 16:07:15 Optimizer params: {'lr': 0.001}
2023-07-05 16:07:15 Weight decay: 0.0
2023-07-05 16:07:15 Max grad norm: None
2023-07-05 16:07:15 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd7212df390>
2023-07-05 16:07:15 Monitor: accuracy
2023-07-05 16:07:15 Monitor criterion: max
2023-07-05 16:07:15 Epochs: 20
2023-07-05 16:07:15 
2023-07-05 16:07:26 --- Train epoch-0, step-1112 ---
2023-07-05 16:07:26 loss: 2.0453
2023-07-05 16:07:27 --- Eval epoch-0, step-1112 ---
2023-07-05 16:07:27 accuracy: 0.3753
2023-07-05 16:07:27 f1_macro: 0.2333
2023-07-05 16:07:27 roc_auc_weighted_ovo: 0.7578
2023-07-05 16:07:27 loss: 1.7058
2023-07-05 16:07:27 New best accuracy score (0.3753) at epoch-0, step-1112
2023-07-05 16:07:27 
2023-07-05 16:07:40 --- Train epoch-1, step-2224 ---
2023-07-05 16:07:40 loss: 1.7929
2023-07-05 16:07:40 --- Eval epoch-1, step-2224 ---
2023-07-05 16:07:40 accuracy: 0.3939
2023-07-05 16:07:40 f1_macro: 0.2572
2023-07-05 16:07:40 roc_auc_weighted_ovo: 0.7722
2023-07-05 16:07:40 loss: 1.6638
2023-07-05 16:07:40 New best accuracy score (0.3939) at epoch-1, step-2224
2023-07-05 16:07:40 
2023-07-05 16:07:51 --- Train epoch-2, step-3336 ---
2023-07-05 16:07:51 loss: 1.7443
2023-07-05 16:07:52 --- Eval epoch-2, step-3336 ---
2023-07-05 16:07:52 accuracy: 0.3941
2023-07-05 16:07:52 f1_macro: 0.2555
2023-07-05 16:07:52 roc_auc_weighted_ovo: 0.7755
2023-07-05 16:07:52 loss: 1.6653
2023-07-05 16:07:52 New best accuracy score (0.3941) at epoch-2, step-3336
2023-07-05 16:07:52 
2023-07-05 16:08:03 --- Train epoch-3, step-4448 ---
2023-07-05 16:08:03 loss: 1.7022
2023-07-05 16:08:03 --- Eval epoch-3, step-4448 ---
2023-07-05 16:08:03 accuracy: 0.3960
2023-07-05 16:08:03 f1_macro: 0.2610
2023-07-05 16:08:03 roc_auc_weighted_ovo: 0.7854
2023-07-05 16:08:03 loss: 1.6347
2023-07-05 16:08:03 New best accuracy score (0.3960) at epoch-3, step-4448
2023-07-05 16:08:03 
2023-07-05 16:08:14 --- Train epoch-4, step-5560 ---
2023-07-05 16:08:14 loss: 1.6729
2023-07-05 16:08:15 --- Eval epoch-4, step-5560 ---
2023-07-05 16:08:15 accuracy: 0.4016
2023-07-05 16:08:15 f1_macro: 0.2582
2023-07-05 16:08:15 roc_auc_weighted_ovo: 0.7842
2023-07-05 16:08:15 loss: 1.6442
2023-07-05 16:08:15 New best accuracy score (0.4016) at epoch-4, step-5560
2023-07-05 16:08:15 
2023-07-05 16:08:25 --- Train epoch-5, step-6672 ---
2023-07-05 16:08:25 loss: 1.6477
2023-07-05 16:08:26 --- Eval epoch-5, step-6672 ---
2023-07-05 16:08:26 accuracy: 0.3869
2023-07-05 16:08:26 f1_macro: 0.2507
2023-07-05 16:08:26 roc_auc_weighted_ovo: 0.7857
2023-07-05 16:08:26 loss: 1.6482
2023-07-05 16:08:26 
2023-07-05 16:08:37 --- Train epoch-6, step-7784 ---
2023-07-05 16:08:37 loss: 1.6205
2023-07-05 16:08:37 --- Eval epoch-6, step-7784 ---
2023-07-05 16:08:37 accuracy: 0.4000
2023-07-05 16:08:37 f1_macro: 0.2769
2023-07-05 16:08:37 roc_auc_weighted_ovo: 0.7847
2023-07-05 16:08:37 loss: 1.6450
2023-07-05 16:08:37 
2023-07-05 16:08:48 --- Train epoch-7, step-8896 ---
2023-07-05 16:08:48 loss: 1.5987
2023-07-05 16:08:49 --- Eval epoch-7, step-8896 ---
2023-07-05 16:08:49 accuracy: 0.3925
2023-07-05 16:08:49 f1_macro: 0.2669
2023-07-05 16:08:49 roc_auc_weighted_ovo: 0.7883
2023-07-05 16:08:49 loss: 1.6441
2023-07-05 16:08:49 
2023-07-05 16:08:59 --- Train epoch-8, step-10008 ---
2023-07-05 16:08:59 loss: 1.5759
2023-07-05 16:09:00 --- Eval epoch-8, step-10008 ---
2023-07-05 16:09:00 accuracy: 0.4016
2023-07-05 16:09:00 f1_macro: 0.2633
2023-07-05 16:09:00 roc_auc_weighted_ovo: 0.7851
2023-07-05 16:09:00 loss: 1.6478
2023-07-05 16:09:00 
2023-07-05 16:09:10 --- Train epoch-9, step-11120 ---
2023-07-05 16:09:10 loss: 1.5462
2023-07-05 16:09:11 --- Eval epoch-9, step-11120 ---
2023-07-05 16:09:11 accuracy: 0.3955
2023-07-05 16:09:11 f1_macro: 0.2699
2023-07-05 16:09:11 roc_auc_weighted_ovo: 0.7882
2023-07-05 16:09:11 loss: 1.6523
2023-07-05 16:09:11 
2023-07-05 16:09:22 --- Train epoch-10, step-12232 ---
2023-07-05 16:09:22 loss: 1.5240
2023-07-05 16:09:22 --- Eval epoch-10, step-12232 ---
2023-07-05 16:09:22 accuracy: 0.3862
2023-07-05 16:09:22 f1_macro: 0.2619
2023-07-05 16:09:22 roc_auc_weighted_ovo: 0.7867
2023-07-05 16:09:22 loss: 1.6580
2023-07-05 16:09:22 
2023-07-05 16:09:33 --- Train epoch-11, step-13344 ---
2023-07-05 16:09:33 loss: 1.4903
2023-07-05 16:09:34 --- Eval epoch-11, step-13344 ---
2023-07-05 16:09:34 accuracy: 0.3898
2023-07-05 16:09:34 f1_macro: 0.2548
2023-07-05 16:09:34 roc_auc_weighted_ovo: 0.7857
2023-07-05 16:09:34 loss: 1.6669
2023-07-05 16:09:34 
2023-07-05 16:09:44 --- Train epoch-12, step-14456 ---
2023-07-05 16:09:44 loss: 1.4695
2023-07-05 16:09:45 --- Eval epoch-12, step-14456 ---
2023-07-05 16:09:45 accuracy: 0.3910
2023-07-05 16:09:45 f1_macro: 0.2718
2023-07-05 16:09:45 roc_auc_weighted_ovo: 0.7873
2023-07-05 16:09:45 loss: 1.6882
2023-07-05 16:09:45 
2023-07-05 16:09:56 --- Train epoch-13, step-15568 ---
2023-07-05 16:09:56 loss: 1.4357
2023-07-05 16:09:56 --- Eval epoch-13, step-15568 ---
2023-07-05 16:09:56 accuracy: 0.3919
2023-07-05 16:09:56 f1_macro: 0.2686
2023-07-05 16:09:56 roc_auc_weighted_ovo: 0.7835
2023-07-05 16:09:56 loss: 1.7109
2023-07-05 16:09:56 
2023-07-05 16:10:08 --- Train epoch-14, step-16680 ---
2023-07-05 16:10:08 loss: 1.4007
2023-07-05 16:10:09 --- Eval epoch-14, step-16680 ---
2023-07-05 16:10:09 accuracy: 0.3828
2023-07-05 16:10:09 f1_macro: 0.2770
2023-07-05 16:10:09 roc_auc_weighted_ovo: 0.7832
2023-07-05 16:10:09 loss: 1.7417
2023-07-05 16:10:09 
2023-07-05 16:10:20 --- Train epoch-15, step-17792 ---
2023-07-05 16:10:20 loss: 1.3761
2023-07-05 16:10:20 --- Eval epoch-15, step-17792 ---
2023-07-05 16:10:20 accuracy: 0.3832
2023-07-05 16:10:20 f1_macro: 0.2626
2023-07-05 16:10:20 roc_auc_weighted_ovo: 0.7819
2023-07-05 16:10:20 loss: 1.7458
2023-07-05 16:10:20 
2023-07-05 16:10:31 --- Train epoch-16, step-18904 ---
2023-07-05 16:10:31 loss: 1.3383
2023-07-05 16:10:32 --- Eval epoch-16, step-18904 ---
2023-07-05 16:10:32 accuracy: 0.3796
2023-07-05 16:10:32 f1_macro: 0.2657
2023-07-05 16:10:32 roc_auc_weighted_ovo: 0.7763
2023-07-05 16:10:32 loss: 1.8272
2023-07-05 16:10:32 
2023-07-05 16:10:45 --- Train epoch-17, step-20016 ---
2023-07-05 16:10:45 loss: 1.3091
2023-07-05 16:10:46 --- Eval epoch-17, step-20016 ---
2023-07-05 16:10:46 accuracy: 0.3823
2023-07-05 16:10:46 f1_macro: 0.2696
2023-07-05 16:10:46 roc_auc_weighted_ovo: 0.7789
2023-07-05 16:10:46 loss: 1.8108
2023-07-05 16:10:46 
2023-07-05 16:10:57 --- Train epoch-18, step-21128 ---
2023-07-05 16:10:57 loss: 1.2725
2023-07-05 16:10:57 --- Eval epoch-18, step-21128 ---
2023-07-05 16:10:57 accuracy: 0.3791
2023-07-05 16:10:57 f1_macro: 0.2681
2023-07-05 16:10:57 roc_auc_weighted_ovo: 0.7749
2023-07-05 16:10:57 loss: 1.8755
2023-07-05 16:10:57 
2023-07-05 16:11:08 --- Train epoch-19, step-22240 ---
2023-07-05 16:11:08 loss: 1.2548
2023-07-05 16:11:09 --- Eval epoch-19, step-22240 ---
2023-07-05 16:11:09 accuracy: 0.3723
2023-07-05 16:11:09 f1_macro: 0.2624
2023-07-05 16:11:09 roc_auc_weighted_ovo: 0.7738
2023-07-05 16:11:09 loss: 1.8599
2023-07-05 16:11:09 Loaded best model
2023-07-05 16:11:10 RNN(
  (embeddings): ModuleDict(
    (conditions): Embedding(4031, 128, padding_idx=0)
    (procedures): Embedding(1276, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (rnn): ModuleDict(
    (conditions): RNNLayer(
      (dropout_layer): Dropout(p=0.5, inplace=False)
      (rnn): GRU(128, 128, batch_first=True)
    )
    (procedures): RNNLayer(
      (dropout_layer): Dropout(p=0.5, inplace=False)
      (rnn): GRU(128, 128, batch_first=True)
    )
  )
  (fc): Linear(in_features=256, out_features=1, bias=True)
)
2023-07-05 16:11:10 Metrics: ['accuracy', 'pr_auc', 'roc_auc', 'f1']
2023-07-05 16:11:10 Device: cuda
2023-07-05 16:11:10 
2023-07-05 16:11:10 Training:
2023-07-05 16:11:10 Batch size: 32
2023-07-05 16:11:10 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-05 16:11:10 Optimizer params: {'lr': 0.001}
2023-07-05 16:11:10 Weight decay: 0.0
2023-07-05 16:11:10 Max grad norm: None
2023-07-05 16:11:10 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd7213bfa10>
2023-07-05 16:11:10 Monitor: roc_auc
2023-07-05 16:11:10 Monitor criterion: max
2023-07-05 16:11:10 Epochs: 20
2023-07-05 16:11:10 
2023-07-05 16:11:12 --- Train epoch-0, step-243 ---
2023-07-05 16:11:12 loss: 0.6848
2023-07-05 16:11:12 --- Eval epoch-0, step-243 ---
2023-07-05 16:11:12 accuracy: 0.5873
2023-07-05 16:11:12 pr_auc: 0.6495
2023-07-05 16:11:12 roc_auc: 0.6184
2023-07-05 16:11:12 f1: 0.6569
2023-07-05 16:11:12 loss: 0.6610
2023-07-05 16:11:12 New best roc_auc score (0.6184) at epoch-0, step-243
2023-07-05 16:11:12 
2023-07-05 16:11:13 --- Train epoch-1, step-486 ---
2023-07-05 16:11:13 loss: 0.6634
2023-07-05 16:11:13 --- Eval epoch-1, step-486 ---
2023-07-05 16:11:13 accuracy: 0.5936
2023-07-05 16:11:13 pr_auc: 0.6412
2023-07-05 16:11:13 roc_auc: 0.6279
2023-07-05 16:11:13 f1: 0.6689
2023-07-05 16:11:13 loss: 0.6610
2023-07-05 16:11:13 New best roc_auc score (0.6279) at epoch-1, step-486
2023-07-05 16:11:13 
2023-07-05 16:11:15 --- Train epoch-2, step-729 ---
2023-07-05 16:11:15 loss: 0.6501
2023-07-05 16:11:15 --- Eval epoch-2, step-729 ---
2023-07-05 16:11:15 accuracy: 0.6123
2023-07-05 16:11:15 pr_auc: 0.6645
2023-07-05 16:11:15 roc_auc: 0.6418
2023-07-05 16:11:15 f1: 0.6804
2023-07-05 16:11:15 loss: 0.6568
2023-07-05 16:11:15 New best roc_auc score (0.6418) at epoch-2, step-729
2023-07-05 16:11:15 
2023-07-05 16:11:16 --- Train epoch-3, step-972 ---
2023-07-05 16:11:16 loss: 0.6316
2023-07-05 16:11:16 --- Eval epoch-3, step-972 ---
2023-07-05 16:11:16 accuracy: 0.5988
2023-07-05 16:11:16 pr_auc: 0.6635
2023-07-05 16:11:16 roc_auc: 0.6359
2023-07-05 16:11:16 f1: 0.6572
2023-07-05 16:11:16 loss: 0.6643
2023-07-05 16:11:16 
2023-07-05 16:11:18 --- Train epoch-4, step-1215 ---
2023-07-05 16:11:18 loss: 0.6250
2023-07-05 16:11:18 --- Eval epoch-4, step-1215 ---
2023-07-05 16:11:18 accuracy: 0.5988
2023-07-05 16:11:18 pr_auc: 0.6717
2023-07-05 16:11:18 roc_auc: 0.6399
2023-07-05 16:11:18 f1: 0.6386
2023-07-05 16:11:18 loss: 0.6630
2023-07-05 16:11:18 
2023-07-05 16:11:19 --- Train epoch-5, step-1458 ---
2023-07-05 16:11:19 loss: 0.6040
2023-07-05 16:11:19 --- Eval epoch-5, step-1458 ---
2023-07-05 16:11:19 accuracy: 0.6071
2023-07-05 16:11:19 pr_auc: 0.6721
2023-07-05 16:11:19 roc_auc: 0.6396
2023-07-05 16:11:19 f1: 0.6667
2023-07-05 16:11:19 loss: 0.6745
2023-07-05 16:11:19 
2023-07-05 16:11:21 --- Train epoch-6, step-1701 ---
2023-07-05 16:11:21 loss: 0.5894
2023-07-05 16:11:21 --- Eval epoch-6, step-1701 ---
2023-07-05 16:11:21 accuracy: 0.6143
2023-07-05 16:11:21 pr_auc: 0.6670
2023-07-05 16:11:21 roc_auc: 0.6391
2023-07-05 16:11:21 f1: 0.6630
2023-07-05 16:11:21 loss: 0.6912
2023-07-05 16:11:21 
2023-07-05 16:11:22 --- Train epoch-7, step-1944 ---
2023-07-05 16:11:22 loss: 0.5732
2023-07-05 16:11:22 --- Eval epoch-7, step-1944 ---
2023-07-05 16:11:22 accuracy: 0.6123
2023-07-05 16:11:22 pr_auc: 0.6708
2023-07-05 16:11:22 roc_auc: 0.6433
2023-07-05 16:11:22 f1: 0.6667
2023-07-05 16:11:22 loss: 0.6863
2023-07-05 16:11:22 New best roc_auc score (0.6433) at epoch-7, step-1944
2023-07-05 16:11:22 
2023-07-05 16:11:24 --- Train epoch-8, step-2187 ---
2023-07-05 16:11:24 loss: 0.5681
2023-07-05 16:11:24 --- Eval epoch-8, step-2187 ---
2023-07-05 16:11:24 accuracy: 0.6206
2023-07-05 16:11:24 pr_auc: 0.6779
2023-07-05 16:11:24 roc_auc: 0.6515
2023-07-05 16:11:24 f1: 0.6807
2023-07-05 16:11:24 loss: 0.6930
2023-07-05 16:11:24 New best roc_auc score (0.6515) at epoch-8, step-2187
2023-07-05 16:11:24 
2023-07-05 16:11:25 --- Train epoch-9, step-2430 ---
2023-07-05 16:11:25 loss: 0.5520
2023-07-05 16:11:25 --- Eval epoch-9, step-2430 ---
2023-07-05 16:11:25 accuracy: 0.6227
2023-07-05 16:11:25 pr_auc: 0.6833
2023-07-05 16:11:25 roc_auc: 0.6571
2023-07-05 16:11:25 f1: 0.6768
2023-07-05 16:11:25 loss: 0.6872
2023-07-05 16:11:25 New best roc_auc score (0.6571) at epoch-9, step-2430
2023-07-05 16:11:25 
2023-07-05 16:11:27 --- Train epoch-10, step-2673 ---
2023-07-05 16:11:27 loss: 0.5336
2023-07-05 16:11:27 --- Eval epoch-10, step-2673 ---
2023-07-05 16:11:27 accuracy: 0.6195
2023-07-05 16:11:27 pr_auc: 0.6796
2023-07-05 16:11:27 roc_auc: 0.6535
2023-07-05 16:11:27 f1: 0.6667
2023-07-05 16:11:27 loss: 0.7047
2023-07-05 16:11:27 
2023-07-05 16:11:28 --- Train epoch-11, step-2916 ---
2023-07-05 16:11:28 loss: 0.5287
2023-07-05 16:11:29 --- Eval epoch-11, step-2916 ---
2023-07-05 16:11:29 accuracy: 0.6237
2023-07-05 16:11:29 pr_auc: 0.6790
2023-07-05 16:11:29 roc_auc: 0.6466
2023-07-05 16:11:29 f1: 0.6750
2023-07-05 16:11:29 loss: 0.7121
2023-07-05 16:11:29 
2023-07-05 16:11:30 --- Train epoch-12, step-3159 ---
2023-07-05 16:11:30 loss: 0.5174
2023-07-05 16:11:30 --- Eval epoch-12, step-3159 ---
2023-07-05 16:11:30 accuracy: 0.6081
2023-07-05 16:11:30 pr_auc: 0.6799
2023-07-05 16:11:30 roc_auc: 0.6408
2023-07-05 16:11:30 f1: 0.6607
2023-07-05 16:11:30 loss: 0.7230
2023-07-05 16:11:30 
2023-07-05 16:11:31 --- Train epoch-13, step-3402 ---
2023-07-05 16:11:31 loss: 0.5087
2023-07-05 16:11:32 --- Eval epoch-13, step-3402 ---
2023-07-05 16:11:32 accuracy: 0.6143
2023-07-05 16:11:32 pr_auc: 0.6728
2023-07-05 16:11:32 roc_auc: 0.6415
2023-07-05 16:11:32 f1: 0.6673
2023-07-05 16:11:32 loss: 0.7331
2023-07-05 16:11:32 
2023-07-05 16:11:33 --- Train epoch-14, step-3645 ---
2023-07-05 16:11:33 loss: 0.4964
2023-07-05 16:11:33 --- Eval epoch-14, step-3645 ---
2023-07-05 16:11:33 accuracy: 0.6029
2023-07-05 16:11:33 pr_auc: 0.6742
2023-07-05 16:11:33 roc_auc: 0.6381
2023-07-05 16:11:33 f1: 0.6631
2023-07-05 16:11:33 loss: 0.7454
2023-07-05 16:11:33 
2023-07-05 16:11:35 --- Train epoch-15, step-3888 ---
2023-07-05 16:11:35 loss: 0.4751
2023-07-05 16:11:35 --- Eval epoch-15, step-3888 ---
2023-07-05 16:11:35 accuracy: 0.5946
2023-07-05 16:11:35 pr_auc: 0.6727
2023-07-05 16:11:35 roc_auc: 0.6373
2023-07-05 16:11:35 f1: 0.6701
2023-07-05 16:11:35 loss: 0.7725
2023-07-05 16:11:35 
2023-07-05 16:11:36 --- Train epoch-16, step-4131 ---
2023-07-05 16:11:36 loss: 0.4676
2023-07-05 16:11:36 --- Eval epoch-16, step-4131 ---
2023-07-05 16:11:36 accuracy: 0.6154
2023-07-05 16:11:36 pr_auc: 0.6749
2023-07-05 16:11:36 roc_auc: 0.6368
2023-07-05 16:11:36 f1: 0.6673
2023-07-05 16:11:36 loss: 0.7624
2023-07-05 16:11:36 
2023-07-05 16:11:38 --- Train epoch-17, step-4374 ---
2023-07-05 16:11:38 loss: 0.4639
2023-07-05 16:11:38 --- Eval epoch-17, step-4374 ---
2023-07-05 16:11:38 accuracy: 0.6040
2023-07-05 16:11:38 pr_auc: 0.6755
2023-07-05 16:11:38 roc_auc: 0.6367
2023-07-05 16:11:38 f1: 0.6678
2023-07-05 16:11:38 loss: 0.7679
2023-07-05 16:11:38 
2023-07-05 16:11:39 --- Train epoch-18, step-4617 ---
2023-07-05 16:11:39 loss: 0.4543
2023-07-05 16:11:40 --- Eval epoch-18, step-4617 ---
2023-07-05 16:11:40 accuracy: 0.6008
2023-07-05 16:11:40 pr_auc: 0.6772
2023-07-05 16:11:40 roc_auc: 0.6383
2023-07-05 16:11:40 f1: 0.6528
2023-07-05 16:11:40 loss: 0.7695
2023-07-05 16:11:40 
2023-07-05 16:11:41 --- Train epoch-19, step-4860 ---
2023-07-05 16:11:41 loss: 0.4501
2023-07-05 16:11:41 --- Eval epoch-19, step-4860 ---
2023-07-05 16:11:41 accuracy: 0.6071
2023-07-05 16:11:41 pr_auc: 0.6687
2023-07-05 16:11:41 roc_auc: 0.6327
2023-07-05 16:11:41 f1: 0.6420
2023-07-05 16:11:41 loss: 0.7866
2023-07-05 16:11:41 Loaded best model
2023-07-05 16:11:43 RNN(
  (embeddings): ModuleDict(
    (conditions): Embedding(4031, 128, padding_idx=0)
    (procedures): Embedding(1276, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (rnn): ModuleDict(
    (conditions): RNNLayer(
      (dropout_layer): Dropout(p=0.5, inplace=False)
      (rnn): GRU(128, 128, batch_first=True)
    )
    (procedures): RNNLayer(
      (dropout_layer): Dropout(p=0.5, inplace=False)
      (rnn): GRU(128, 128, batch_first=True)
    )
  )
  (fc): Linear(in_features=256, out_features=1, bias=True)
)
2023-07-05 16:11:43 Metrics: ['accuracy', 'pr_auc', 'roc_auc', 'f1']
2023-07-05 16:11:43 Device: cuda
2023-07-05 16:11:43 
2023-07-05 16:11:43 Training:
2023-07-05 16:11:43 Batch size: 32
2023-07-05 16:11:43 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-05 16:11:43 Optimizer params: {'lr': 0.001}
2023-07-05 16:11:43 Weight decay: 0.0
2023-07-05 16:11:43 Max grad norm: None
2023-07-05 16:11:43 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd721294650>
2023-07-05 16:11:43 Monitor: roc_auc
2023-07-05 16:11:43 Monitor criterion: max
2023-07-05 16:11:43 Epochs: 20
2023-07-05 16:11:43 
2023-07-05 16:11:44 --- Train epoch-0, step-240 ---
2023-07-05 16:11:44 loss: 0.2844
2023-07-05 16:11:44 --- Eval epoch-0, step-240 ---
2023-07-05 16:11:44 accuracy: 0.9191
2023-07-05 16:11:44 pr_auc: 0.0781
2023-07-05 16:11:44 roc_auc: 0.5102
2023-07-05 16:11:44 f1: 0.0000
2023-07-05 16:11:44 loss: 0.3225
2023-07-05 16:11:44 New best roc_auc score (0.5102) at epoch-0, step-240
2023-07-05 16:11:44 
2023-07-05 16:11:46 --- Train epoch-1, step-480 ---
2023-07-05 16:11:46 loss: 0.2439
2023-07-05 16:11:46 --- Eval epoch-1, step-480 ---
2023-07-05 16:11:46 accuracy: 0.9191
2023-07-05 16:11:46 pr_auc: 0.0858
2023-07-05 16:11:46 roc_auc: 0.5396
2023-07-05 16:11:46 f1: 0.0000
2023-07-05 16:11:46 loss: 0.2954
2023-07-05 16:11:46 New best roc_auc score (0.5396) at epoch-1, step-480
2023-07-05 16:11:46 
2023-07-05 16:11:47 --- Train epoch-2, step-720 ---
2023-07-05 16:11:47 loss: 0.2299
2023-07-05 16:11:47 --- Eval epoch-2, step-720 ---
2023-07-05 16:11:47 accuracy: 0.9191
2023-07-05 16:11:47 pr_auc: 0.0903
2023-07-05 16:11:47 roc_auc: 0.5460
2023-07-05 16:11:47 f1: 0.0000
2023-07-05 16:11:47 loss: 0.3006
2023-07-05 16:11:47 New best roc_auc score (0.5460) at epoch-2, step-720
2023-07-05 16:11:47 
2023-07-05 16:11:49 --- Train epoch-3, step-960 ---
2023-07-05 16:11:49 loss: 0.2185
2023-07-05 16:11:49 --- Eval epoch-3, step-960 ---
2023-07-05 16:11:49 accuracy: 0.9180
2023-07-05 16:11:49 pr_auc: 0.0916
2023-07-05 16:11:49 roc_auc: 0.5433
2023-07-05 16:11:49 f1: 0.0000
2023-07-05 16:11:49 loss: 0.3040
2023-07-05 16:11:49 
2023-07-05 16:11:50 --- Train epoch-4, step-1200 ---
2023-07-05 16:11:50 loss: 0.2126
2023-07-05 16:11:50 --- Eval epoch-4, step-1200 ---
2023-07-05 16:11:50 accuracy: 0.9180
2023-07-05 16:11:50 pr_auc: 0.0978
2023-07-05 16:11:50 roc_auc: 0.5658
2023-07-05 16:11:50 f1: 0.0000
2023-07-05 16:11:50 loss: 0.2959
2023-07-05 16:11:50 New best roc_auc score (0.5658) at epoch-4, step-1200
2023-07-05 16:11:50 
2023-07-05 16:11:52 --- Train epoch-5, step-1440 ---
2023-07-05 16:11:52 loss: 0.1986
2023-07-05 16:11:52 --- Eval epoch-5, step-1440 ---
2023-07-05 16:11:52 accuracy: 0.9180
2023-07-05 16:11:52 pr_auc: 0.1044
2023-07-05 16:11:52 roc_auc: 0.6081
2023-07-05 16:11:52 f1: 0.0000
2023-07-05 16:11:52 loss: 0.3008
2023-07-05 16:11:52 New best roc_auc score (0.6081) at epoch-5, step-1440
2023-07-05 16:11:52 
2023-07-05 16:11:53 --- Train epoch-6, step-1680 ---
2023-07-05 16:11:53 loss: 0.1849
2023-07-05 16:11:53 --- Eval epoch-6, step-1680 ---
2023-07-05 16:11:53 accuracy: 0.9170
2023-07-05 16:11:53 pr_auc: 0.1086
2023-07-05 16:11:53 roc_auc: 0.6071
2023-07-05 16:11:53 f1: 0.0241
2023-07-05 16:11:53 loss: 0.3091
2023-07-05 16:11:53 
2023-07-05 16:11:55 --- Train epoch-7, step-1920 ---
2023-07-05 16:11:55 loss: 0.1770
2023-07-05 16:11:55 --- Eval epoch-7, step-1920 ---
2023-07-05 16:11:55 accuracy: 0.9170
2023-07-05 16:11:55 pr_auc: 0.0986
2023-07-05 16:11:55 roc_auc: 0.5822
2023-07-05 16:11:55 f1: 0.0000
2023-07-05 16:11:55 loss: 0.3293
2023-07-05 16:11:55 
2023-07-05 16:11:56 --- Train epoch-8, step-2160 ---
2023-07-05 16:11:56 loss: 0.1692
2023-07-05 16:11:56 --- Eval epoch-8, step-2160 ---
2023-07-05 16:11:56 accuracy: 0.9170
2023-07-05 16:11:56 pr_auc: 0.1008
2023-07-05 16:11:56 roc_auc: 0.5859
2023-07-05 16:11:56 f1: 0.0000
2023-07-05 16:11:56 loss: 0.3389
2023-07-05 16:11:56 
2023-07-05 16:11:58 --- Train epoch-9, step-2400 ---
2023-07-05 16:11:58 loss: 0.1591
2023-07-05 16:11:58 --- Eval epoch-9, step-2400 ---
2023-07-05 16:11:58 accuracy: 0.9160
2023-07-05 16:11:58 pr_auc: 0.1033
2023-07-05 16:11:58 roc_auc: 0.5877
2023-07-05 16:11:58 f1: 0.0000
2023-07-05 16:11:58 loss: 0.3463
2023-07-05 16:11:58 
2023-07-05 16:11:59 --- Train epoch-10, step-2640 ---
2023-07-05 16:11:59 loss: 0.1563
2023-07-05 16:12:00 --- Eval epoch-10, step-2640 ---
2023-07-05 16:12:00 accuracy: 0.9150
2023-07-05 16:12:00 pr_auc: 0.1084
2023-07-05 16:12:00 roc_auc: 0.6030
2023-07-05 16:12:00 f1: 0.0000
2023-07-05 16:12:00 loss: 0.3447
2023-07-05 16:12:00 
2023-07-05 16:12:01 --- Train epoch-11, step-2880 ---
2023-07-05 16:12:01 loss: 0.1424
2023-07-05 16:12:01 --- Eval epoch-11, step-2880 ---
2023-07-05 16:12:01 accuracy: 0.9160
2023-07-05 16:12:01 pr_auc: 0.1032
2023-07-05 16:12:01 roc_auc: 0.6055
2023-07-05 16:12:01 f1: 0.0000
2023-07-05 16:12:01 loss: 0.3609
2023-07-05 16:12:01 
2023-07-05 16:12:03 --- Train epoch-12, step-3120 ---
2023-07-05 16:12:03 loss: 0.1389
2023-07-05 16:12:03 --- Eval epoch-12, step-3120 ---
2023-07-05 16:12:03 accuracy: 0.9150
2023-07-05 16:12:03 pr_auc: 0.1046
2023-07-05 16:12:03 roc_auc: 0.6076
2023-07-05 16:12:03 f1: 0.0000
2023-07-05 16:12:03 loss: 0.3718
2023-07-05 16:12:03 
2023-07-05 16:12:04 --- Train epoch-13, step-3360 ---
2023-07-05 16:12:04 loss: 0.1317
2023-07-05 16:12:04 --- Eval epoch-13, step-3360 ---
2023-07-05 16:12:04 accuracy: 0.9129
2023-07-05 16:12:04 pr_auc: 0.1068
2023-07-05 16:12:04 roc_auc: 0.6109
2023-07-05 16:12:04 f1: 0.0000
2023-07-05 16:12:04 loss: 0.3915
2023-07-05 16:12:04 New best roc_auc score (0.6109) at epoch-13, step-3360
2023-07-05 16:12:04 
2023-07-05 16:12:06 --- Train epoch-14, step-3600 ---
2023-07-05 16:12:06 loss: 0.1233
2023-07-05 16:12:06 --- Eval epoch-14, step-3600 ---
2023-07-05 16:12:06 accuracy: 0.9119
2023-07-05 16:12:06 pr_auc: 0.1036
2023-07-05 16:12:06 roc_auc: 0.6094
2023-07-05 16:12:06 f1: 0.0000
2023-07-05 16:12:06 loss: 0.3860
2023-07-05 16:12:06 
2023-07-05 16:12:08 --- Train epoch-15, step-3840 ---
2023-07-05 16:12:08 loss: 0.1194
2023-07-05 16:12:08 --- Eval epoch-15, step-3840 ---
2023-07-05 16:12:08 accuracy: 0.9088
2023-07-05 16:12:08 pr_auc: 0.1034
2023-07-05 16:12:08 roc_auc: 0.5980
2023-07-05 16:12:08 f1: 0.0000
2023-07-05 16:12:08 loss: 0.4041
2023-07-05 16:12:08 
2023-07-05 16:12:09 --- Train epoch-16, step-4080 ---
2023-07-05 16:12:09 loss: 0.1123
2023-07-05 16:12:09 --- Eval epoch-16, step-4080 ---
2023-07-05 16:12:09 accuracy: 0.9098
2023-07-05 16:12:09 pr_auc: 0.1054
2023-07-05 16:12:09 roc_auc: 0.5955
2023-07-05 16:12:09 f1: 0.0222
2023-07-05 16:12:09 loss: 0.4284
2023-07-05 16:12:09 
2023-07-05 16:12:10 --- Train epoch-17, step-4320 ---
2023-07-05 16:12:10 loss: 0.1034
2023-07-05 16:12:10 --- Eval epoch-17, step-4320 ---
2023-07-05 16:12:10 accuracy: 0.9057
2023-07-05 16:12:10 pr_auc: 0.1128
2023-07-05 16:12:10 roc_auc: 0.5996
2023-07-05 16:12:10 f1: 0.0213
2023-07-05 16:12:10 loss: 0.4320
2023-07-05 16:12:10 
2023-07-05 16:12:12 --- Train epoch-18, step-4560 ---
2023-07-05 16:12:12 loss: 0.1014
2023-07-05 16:12:12 --- Eval epoch-18, step-4560 ---
2023-07-05 16:12:12 accuracy: 0.9139
2023-07-05 16:12:12 pr_auc: 0.1170
2023-07-05 16:12:12 roc_auc: 0.6125
2023-07-05 16:12:12 f1: 0.0455
2023-07-05 16:12:12 loss: 0.4368
2023-07-05 16:12:12 New best roc_auc score (0.6125) at epoch-18, step-4560
2023-07-05 16:12:12 
2023-07-05 16:12:13 --- Train epoch-19, step-4800 ---
2023-07-05 16:12:13 loss: 0.1056
2023-07-05 16:12:13 --- Eval epoch-19, step-4800 ---
2023-07-05 16:12:13 accuracy: 0.9160
2023-07-05 16:12:13 pr_auc: 0.1089
2023-07-05 16:12:13 roc_auc: 0.6108
2023-07-05 16:12:13 f1: 0.0465
2023-07-05 16:12:13 loss: 0.4539
2023-07-05 16:12:13 Loaded best model
2023-07-05 16:12:20 RNN(
  (embeddings): ModuleDict(
    (conditions): Embedding(6664, 128, padding_idx=0)
    (procedures): Embedding(1980, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (rnn): ModuleDict(
    (conditions): RNNLayer(
      (dropout_layer): Dropout(p=0.5, inplace=False)
      (rnn): GRU(128, 128, batch_first=True)
    )
    (procedures): RNNLayer(
      (dropout_layer): Dropout(p=0.5, inplace=False)
      (rnn): GRU(128, 128, batch_first=True)
    )
  )
  (fc): Linear(in_features=256, out_features=10, bias=True)
)
2023-07-05 16:12:20 Metrics: ['accuracy', 'f1_macro', 'roc_auc_weighted_ovo']
2023-07-05 16:12:20 Device: cuda
2023-07-05 16:12:20 
2023-07-05 16:12:20 Training:
2023-07-05 16:12:20 Batch size: 32
2023-07-05 16:12:20 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-05 16:12:20 Optimizer params: {'lr': 0.001}
2023-07-05 16:12:20 Weight decay: 0.0
2023-07-05 16:12:20 Max grad norm: None
2023-07-05 16:12:20 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd721462790>
2023-07-05 16:12:20 Monitor: accuracy
2023-07-05 16:12:20 Monitor criterion: max
2023-07-05 16:12:20 Epochs: 20
2023-07-05 16:12:20 
2023-07-05 16:12:27 --- Train epoch-0, step-1110 ---
2023-07-05 16:12:27 loss: 1.8214
2023-07-05 16:12:27 --- Eval epoch-0, step-1110 ---
2023-07-05 16:12:27 accuracy: 0.3817
2023-07-05 16:12:27 f1_macro: 0.2328
2023-07-05 16:12:27 roc_auc_weighted_ovo: 0.7568
2023-07-05 16:12:27 loss: 1.7108
2023-07-05 16:12:27 New best accuracy score (0.3817) at epoch-0, step-1110
2023-07-05 16:12:27 
2023-07-05 16:12:33 --- Train epoch-1, step-2220 ---
2023-07-05 16:12:33 loss: 1.6972
2023-07-05 16:12:34 --- Eval epoch-1, step-2220 ---
2023-07-05 16:12:34 accuracy: 0.3891
2023-07-05 16:12:34 f1_macro: 0.2383
2023-07-05 16:12:34 roc_auc_weighted_ovo: 0.7718
2023-07-05 16:12:34 loss: 1.6718
2023-07-05 16:12:34 New best accuracy score (0.3891) at epoch-1, step-2220
2023-07-05 16:12:34 
2023-07-05 16:12:40 --- Train epoch-2, step-3330 ---
2023-07-05 16:12:40 loss: 1.6480
2023-07-05 16:12:41 --- Eval epoch-2, step-3330 ---
2023-07-05 16:12:41 accuracy: 0.3898
2023-07-05 16:12:41 f1_macro: 0.2526
2023-07-05 16:12:41 roc_auc_weighted_ovo: 0.7772
2023-07-05 16:12:41 loss: 1.6629
2023-07-05 16:12:41 New best accuracy score (0.3898) at epoch-2, step-3330
2023-07-05 16:12:41 
2023-07-05 16:12:47 --- Train epoch-3, step-4440 ---
2023-07-05 16:12:47 loss: 1.6152
2023-07-05 16:12:48 --- Eval epoch-3, step-4440 ---
2023-07-05 16:12:48 accuracy: 0.3941
2023-07-05 16:12:48 f1_macro: 0.2556
2023-07-05 16:12:48 roc_auc_weighted_ovo: 0.7826
2023-07-05 16:12:48 loss: 1.6538
2023-07-05 16:12:48 New best accuracy score (0.3941) at epoch-3, step-4440
2023-07-05 16:12:48 
2023-07-05 16:12:55 --- Train epoch-4, step-5550 ---
2023-07-05 16:12:55 loss: 1.5923
2023-07-05 16:12:55 --- Eval epoch-4, step-5550 ---
2023-07-05 16:12:55 accuracy: 0.3973
2023-07-05 16:12:55 f1_macro: 0.2689
2023-07-05 16:12:55 roc_auc_weighted_ovo: 0.7870
2023-07-05 16:12:55 loss: 1.6368
2023-07-05 16:12:55 New best accuracy score (0.3973) at epoch-4, step-5550
2023-07-05 16:12:55 
2023-07-05 16:13:02 --- Train epoch-5, step-6660 ---
2023-07-05 16:13:02 loss: 1.5742
2023-07-05 16:13:02 --- Eval epoch-5, step-6660 ---
2023-07-05 16:13:02 accuracy: 0.3989
2023-07-05 16:13:02 f1_macro: 0.2642
2023-07-05 16:13:02 roc_auc_weighted_ovo: 0.7887
2023-07-05 16:13:02 loss: 1.6355
2023-07-05 16:13:02 New best accuracy score (0.3989) at epoch-5, step-6660
2023-07-05 16:13:02 
2023-07-05 16:13:09 --- Train epoch-6, step-7770 ---
2023-07-05 16:13:09 loss: 1.5581
2023-07-05 16:13:09 --- Eval epoch-6, step-7770 ---
2023-07-05 16:13:09 accuracy: 0.3932
2023-07-05 16:13:09 f1_macro: 0.2615
2023-07-05 16:13:09 roc_auc_weighted_ovo: 0.7896
2023-07-05 16:13:09 loss: 1.6367
2023-07-05 16:13:09 
2023-07-05 16:13:16 --- Train epoch-7, step-8880 ---
2023-07-05 16:13:16 loss: 1.5386
2023-07-05 16:13:16 --- Eval epoch-7, step-8880 ---
2023-07-05 16:13:16 accuracy: 0.3973
2023-07-05 16:13:16 f1_macro: 0.2669
2023-07-05 16:13:16 roc_auc_weighted_ovo: 0.7892
2023-07-05 16:13:16 loss: 1.6330
2023-07-05 16:13:16 
2023-07-05 16:13:23 --- Train epoch-8, step-9990 ---
2023-07-05 16:13:23 loss: 1.5292
2023-07-05 16:13:24 --- Eval epoch-8, step-9990 ---
2023-07-05 16:13:24 accuracy: 0.4068
2023-07-05 16:13:24 f1_macro: 0.2817
2023-07-05 16:13:24 roc_auc_weighted_ovo: 0.7905
2023-07-05 16:13:24 loss: 1.6384
2023-07-05 16:13:24 New best accuracy score (0.4068) at epoch-8, step-9990
2023-07-05 16:13:24 
2023-07-05 16:13:30 --- Train epoch-9, step-11100 ---
2023-07-05 16:13:30 loss: 1.5186
2023-07-05 16:13:30 --- Eval epoch-9, step-11100 ---
2023-07-05 16:13:30 accuracy: 0.4075
2023-07-05 16:13:30 f1_macro: 0.2754
2023-07-05 16:13:30 roc_auc_weighted_ovo: 0.7920
2023-07-05 16:13:30 loss: 1.6320
2023-07-05 16:13:30 New best accuracy score (0.4075) at epoch-9, step-11100
2023-07-05 16:13:30 
2023-07-05 16:13:37 --- Train epoch-10, step-12210 ---
2023-07-05 16:13:37 loss: 1.5063
2023-07-05 16:13:38 --- Eval epoch-10, step-12210 ---
2023-07-05 16:13:38 accuracy: 0.3980
2023-07-05 16:13:38 f1_macro: 0.2815
2023-07-05 16:13:38 roc_auc_weighted_ovo: 0.7938
2023-07-05 16:13:38 loss: 1.6379
2023-07-05 16:13:38 
2023-07-05 16:13:45 --- Train epoch-11, step-13320 ---
2023-07-05 16:13:45 loss: 1.4952
2023-07-05 16:13:46 --- Eval epoch-11, step-13320 ---
2023-07-05 16:13:46 accuracy: 0.4023
2023-07-05 16:13:46 f1_macro: 0.2772
2023-07-05 16:13:46 roc_auc_weighted_ovo: 0.7935
2023-07-05 16:13:46 loss: 1.6350
2023-07-05 16:13:46 
2023-07-05 16:13:54 --- Train epoch-12, step-14430 ---
2023-07-05 16:13:54 loss: 1.4885
2023-07-05 16:13:54 --- Eval epoch-12, step-14430 ---
2023-07-05 16:13:54 accuracy: 0.4027
2023-07-05 16:13:54 f1_macro: 0.2780
2023-07-05 16:13:54 roc_auc_weighted_ovo: 0.7919
2023-07-05 16:13:54 loss: 1.6392
2023-07-05 16:13:54 
2023-07-05 16:14:01 --- Train epoch-13, step-15540 ---
2023-07-05 16:14:01 loss: 1.4777
2023-07-05 16:14:01 --- Eval epoch-13, step-15540 ---
2023-07-05 16:14:01 accuracy: 0.4002
2023-07-05 16:14:01 f1_macro: 0.2772
2023-07-05 16:14:01 roc_auc_weighted_ovo: 0.7931
2023-07-05 16:14:01 loss: 1.6467
2023-07-05 16:14:01 
2023-07-05 16:14:08 --- Train epoch-14, step-16650 ---
2023-07-05 16:14:08 loss: 1.4715
2023-07-05 16:14:08 --- Eval epoch-14, step-16650 ---
2023-07-05 16:14:08 accuracy: 0.4023
2023-07-05 16:14:08 f1_macro: 0.2780
2023-07-05 16:14:08 roc_auc_weighted_ovo: 0.7924
2023-07-05 16:14:08 loss: 1.6457
2023-07-05 16:14:08 
2023-07-05 16:14:15 --- Train epoch-15, step-17760 ---
2023-07-05 16:14:15 loss: 1.4593
2023-07-05 16:14:15 --- Eval epoch-15, step-17760 ---
2023-07-05 16:14:15 accuracy: 0.3948
2023-07-05 16:14:15 f1_macro: 0.2791
2023-07-05 16:14:15 roc_auc_weighted_ovo: 0.7920
2023-07-05 16:14:15 loss: 1.6469
2023-07-05 16:14:15 
2023-07-05 16:14:22 --- Train epoch-16, step-18870 ---
2023-07-05 16:14:22 loss: 1.4496
2023-07-05 16:14:23 --- Eval epoch-16, step-18870 ---
2023-07-05 16:14:23 accuracy: 0.3930
2023-07-05 16:14:23 f1_macro: 0.2759
2023-07-05 16:14:23 roc_auc_weighted_ovo: 0.7918
2023-07-05 16:14:23 loss: 1.6479
2023-07-05 16:14:23 
2023-07-05 16:14:29 --- Train epoch-17, step-19980 ---
2023-07-05 16:14:29 loss: 1.4461
2023-07-05 16:14:30 --- Eval epoch-17, step-19980 ---
2023-07-05 16:14:30 accuracy: 0.3941
2023-07-05 16:14:30 f1_macro: 0.2691
2023-07-05 16:14:30 roc_auc_weighted_ovo: 0.7899
2023-07-05 16:14:30 loss: 1.6576
2023-07-05 16:14:30 
2023-07-05 16:14:36 --- Train epoch-18, step-21090 ---
2023-07-05 16:14:36 loss: 1.4374
2023-07-05 16:14:36 --- Eval epoch-18, step-21090 ---
2023-07-05 16:14:36 accuracy: 0.3998
2023-07-05 16:14:36 f1_macro: 0.2780
2023-07-05 16:14:36 roc_auc_weighted_ovo: 0.7930
2023-07-05 16:14:36 loss: 1.6527
2023-07-05 16:14:36 
2023-07-05 16:14:43 --- Train epoch-19, step-22200 ---
2023-07-05 16:14:43 loss: 1.4327
2023-07-05 16:14:44 --- Eval epoch-19, step-22200 ---
2023-07-05 16:14:44 accuracy: 0.3900
2023-07-05 16:14:44 f1_macro: 0.2726
2023-07-05 16:14:44 roc_auc_weighted_ovo: 0.7907
2023-07-05 16:14:44 loss: 1.6593
2023-07-05 16:14:44 Loaded best model
2023-07-05 16:14:46 ConCare(
  (embeddings): ModuleDict(
    (conditions): Embedding(4031, 128, padding_idx=0)
    (procedures): Embedding(1276, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (concare): ModuleDict(
    (conditions): ConCareLayer(
      (PositionalEncoding): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (GRUs): ModuleList(
        (0): GRU(1, 128, batch_first=True)
        (1): GRU(1, 128, batch_first=True)
        (2): GRU(1, 128, batch_first=True)
        (3): GRU(1, 128, batch_first=True)
        (4): GRU(1, 128, batch_first=True)
        (5): GRU(1, 128, batch_first=True)
        (6): GRU(1, 128, batch_first=True)
        (7): GRU(1, 128, batch_first=True)
        (8): GRU(1, 128, batch_first=True)
        (9): GRU(1, 128, batch_first=True)
        (10): GRU(1, 128, batch_first=True)
        (11): GRU(1, 128, batch_first=True)
        (12): GRU(1, 128, batch_first=True)
        (13): GRU(1, 128, batch_first=True)
        (14): GRU(1, 128, batch_first=True)
        (15): GRU(1, 128, batch_first=True)
        (16): GRU(1, 128, batch_first=True)
        (17): GRU(1, 128, batch_first=True)
        (18): GRU(1, 128, batch_first=True)
        (19): GRU(1, 128, batch_first=True)
        (20): GRU(1, 128, batch_first=True)
        (21): GRU(1, 128, batch_first=True)
        (22): GRU(1, 128, batch_first=True)
        (23): GRU(1, 128, batch_first=True)
        (24): GRU(1, 128, batch_first=True)
        (25): GRU(1, 128, batch_first=True)
        (26): GRU(1, 128, batch_first=True)
        (27): GRU(1, 128, batch_first=True)
        (28): GRU(1, 128, batch_first=True)
        (29): GRU(1, 128, batch_first=True)
        (30): GRU(1, 128, batch_first=True)
        (31): GRU(1, 128, batch_first=True)
        (32): GRU(1, 128, batch_first=True)
        (33): GRU(1, 128, batch_first=True)
        (34): GRU(1, 128, batch_first=True)
        (35): GRU(1, 128, batch_first=True)
        (36): GRU(1, 128, batch_first=True)
        (37): GRU(1, 128, batch_first=True)
        (38): GRU(1, 128, batch_first=True)
        (39): GRU(1, 128, batch_first=True)
        (40): GRU(1, 128, batch_first=True)
        (41): GRU(1, 128, batch_first=True)
        (42): GRU(1, 128, batch_first=True)
        (43): GRU(1, 128, batch_first=True)
        (44): GRU(1, 128, batch_first=True)
        (45): GRU(1, 128, batch_first=True)
        (46): GRU(1, 128, batch_first=True)
        (47): GRU(1, 128, batch_first=True)
        (48): GRU(1, 128, batch_first=True)
        (49): GRU(1, 128, batch_first=True)
        (50): GRU(1, 128, batch_first=True)
        (51): GRU(1, 128, batch_first=True)
        (52): GRU(1, 128, batch_first=True)
        (53): GRU(1, 128, batch_first=True)
        (54): GRU(1, 128, batch_first=True)
        (55): GRU(1, 128, batch_first=True)
        (56): GRU(1, 128, batch_first=True)
        (57): GRU(1, 128, batch_first=True)
        (58): GRU(1, 128, batch_first=True)
        (59): GRU(1, 128, batch_first=True)
        (60): GRU(1, 128, batch_first=True)
        (61): GRU(1, 128, batch_first=True)
        (62): GRU(1, 128, batch_first=True)
        (63): GRU(1, 128, batch_first=True)
        (64): GRU(1, 128, batch_first=True)
        (65): GRU(1, 128, batch_first=True)
        (66): GRU(1, 128, batch_first=True)
        (67): GRU(1, 128, batch_first=True)
        (68): GRU(1, 128, batch_first=True)
        (69): GRU(1, 128, batch_first=True)
        (70): GRU(1, 128, batch_first=True)
        (71): GRU(1, 128, batch_first=True)
        (72): GRU(1, 128, batch_first=True)
        (73): GRU(1, 128, batch_first=True)
        (74): GRU(1, 128, batch_first=True)
        (75): GRU(1, 128, batch_first=True)
        (76): GRU(1, 128, batch_first=True)
        (77): GRU(1, 128, batch_first=True)
        (78): GRU(1, 128, batch_first=True)
        (79): GRU(1, 128, batch_first=True)
        (80): GRU(1, 128, batch_first=True)
        (81): GRU(1, 128, batch_first=True)
        (82): GRU(1, 128, batch_first=True)
        (83): GRU(1, 128, batch_first=True)
        (84): GRU(1, 128, batch_first=True)
        (85): GRU(1, 128, batch_first=True)
        (86): GRU(1, 128, batch_first=True)
        (87): GRU(1, 128, batch_first=True)
        (88): GRU(1, 128, batch_first=True)
        (89): GRU(1, 128, batch_first=True)
        (90): GRU(1, 128, batch_first=True)
        (91): GRU(1, 128, batch_first=True)
        (92): GRU(1, 128, batch_first=True)
        (93): GRU(1, 128, batch_first=True)
        (94): GRU(1, 128, batch_first=True)
        (95): GRU(1, 128, batch_first=True)
        (96): GRU(1, 128, batch_first=True)
        (97): GRU(1, 128, batch_first=True)
        (98): GRU(1, 128, batch_first=True)
        (99): GRU(1, 128, batch_first=True)
        (100): GRU(1, 128, batch_first=True)
        (101): GRU(1, 128, batch_first=True)
        (102): GRU(1, 128, batch_first=True)
        (103): GRU(1, 128, batch_first=True)
        (104): GRU(1, 128, batch_first=True)
        (105): GRU(1, 128, batch_first=True)
        (106): GRU(1, 128, batch_first=True)
        (107): GRU(1, 128, batch_first=True)
        (108): GRU(1, 128, batch_first=True)
        (109): GRU(1, 128, batch_first=True)
        (110): GRU(1, 128, batch_first=True)
        (111): GRU(1, 128, batch_first=True)
        (112): GRU(1, 128, batch_first=True)
        (113): GRU(1, 128, batch_first=True)
        (114): GRU(1, 128, batch_first=True)
        (115): GRU(1, 128, batch_first=True)
        (116): GRU(1, 128, batch_first=True)
        (117): GRU(1, 128, batch_first=True)
        (118): GRU(1, 128, batch_first=True)
        (119): GRU(1, 128, batch_first=True)
        (120): GRU(1, 128, batch_first=True)
        (121): GRU(1, 128, batch_first=True)
        (122): GRU(1, 128, batch_first=True)
        (123): GRU(1, 128, batch_first=True)
        (124): GRU(1, 128, batch_first=True)
        (125): GRU(1, 128, batch_first=True)
        (126): GRU(1, 128, batch_first=True)
        (127): GRU(1, 128, batch_first=True)
      )
      (LastStepAttentions): ModuleList(
        (0): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (1): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (2): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (3): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (4): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (5): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (6): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (7): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (8): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (9): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (10): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (11): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (12): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (13): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (14): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (15): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (16): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (17): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (18): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (19): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (20): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (21): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (22): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (23): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (24): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (25): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (26): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (27): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (28): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (29): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (30): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (31): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (32): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (33): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (34): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (35): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (36): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (37): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (38): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (39): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (40): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (41): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (42): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (43): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (44): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (45): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (46): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (47): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (48): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (49): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (50): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (51): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (52): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (53): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (54): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (55): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (56): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (57): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (58): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (59): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (60): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (61): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (62): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (63): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (64): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (65): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (66): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (67): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (68): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (69): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (70): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (71): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (72): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (73): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (74): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (75): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (76): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (77): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (78): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (79): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (80): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (81): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (82): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (83): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (84): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (85): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (86): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (87): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (88): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (89): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (90): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (91): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (92): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (93): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (94): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (95): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (96): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (97): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (98): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (99): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (100): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (101): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (102): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (103): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (104): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (105): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (106): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (107): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (108): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (109): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (110): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (111): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (112): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (113): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (114): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (115): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (116): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (117): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (118): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (119): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (120): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (121): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (122): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (123): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (124): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (125): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (126): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (127): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
      )
      (FinalAttentionQKV): FinalAttentionQKV(
        (W_q): Linear(in_features=128, out_features=128, bias=True)
        (W_k): Linear(in_features=128, out_features=128, bias=True)
        (W_v): Linear(in_features=128, out_features=128, bias=True)
        (W_out): Linear(in_features=128, out_features=1, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (tanh): Tanh()
        (softmax): Softmax(dim=1)
        (sigmoid): Sigmoid()
      )
      (MultiHeadedAttention): MultiHeadedAttention(
        (linears): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Linear(in_features=128, out_features=128, bias=True)
          (2): Linear(in_features=128, out_features=128, bias=True)
        )
        (final_linear): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (SublayerConnection): SublayerConnection(
        (norm): LayerNorm()
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (PositionwiseFeedForward): PositionwiseFeedForward(
        (w_1): Linear(in_features=128, out_features=64, bias=True)
        (w_2): Linear(in_features=64, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.5, inplace=False)
      (tanh): Tanh()
      (softmax): Softmax(dim=None)
      (sigmoid): Sigmoid()
      (relu): ReLU()
    )
    (procedures): ConCareLayer(
      (PositionalEncoding): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (GRUs): ModuleList(
        (0): GRU(1, 128, batch_first=True)
        (1): GRU(1, 128, batch_first=True)
        (2): GRU(1, 128, batch_first=True)
        (3): GRU(1, 128, batch_first=True)
        (4): GRU(1, 128, batch_first=True)
        (5): GRU(1, 128, batch_first=True)
        (6): GRU(1, 128, batch_first=True)
        (7): GRU(1, 128, batch_first=True)
        (8): GRU(1, 128, batch_first=True)
        (9): GRU(1, 128, batch_first=True)
        (10): GRU(1, 128, batch_first=True)
        (11): GRU(1, 128, batch_first=True)
        (12): GRU(1, 128, batch_first=True)
        (13): GRU(1, 128, batch_first=True)
        (14): GRU(1, 128, batch_first=True)
        (15): GRU(1, 128, batch_first=True)
        (16): GRU(1, 128, batch_first=True)
        (17): GRU(1, 128, batch_first=True)
        (18): GRU(1, 128, batch_first=True)
        (19): GRU(1, 128, batch_first=True)
        (20): GRU(1, 128, batch_first=True)
        (21): GRU(1, 128, batch_first=True)
        (22): GRU(1, 128, batch_first=True)
        (23): GRU(1, 128, batch_first=True)
        (24): GRU(1, 128, batch_first=True)
        (25): GRU(1, 128, batch_first=True)
        (26): GRU(1, 128, batch_first=True)
        (27): GRU(1, 128, batch_first=True)
        (28): GRU(1, 128, batch_first=True)
        (29): GRU(1, 128, batch_first=True)
        (30): GRU(1, 128, batch_first=True)
        (31): GRU(1, 128, batch_first=True)
        (32): GRU(1, 128, batch_first=True)
        (33): GRU(1, 128, batch_first=True)
        (34): GRU(1, 128, batch_first=True)
        (35): GRU(1, 128, batch_first=True)
        (36): GRU(1, 128, batch_first=True)
        (37): GRU(1, 128, batch_first=True)
        (38): GRU(1, 128, batch_first=True)
        (39): GRU(1, 128, batch_first=True)
        (40): GRU(1, 128, batch_first=True)
        (41): GRU(1, 128, batch_first=True)
        (42): GRU(1, 128, batch_first=True)
        (43): GRU(1, 128, batch_first=True)
        (44): GRU(1, 128, batch_first=True)
        (45): GRU(1, 128, batch_first=True)
        (46): GRU(1, 128, batch_first=True)
        (47): GRU(1, 128, batch_first=True)
        (48): GRU(1, 128, batch_first=True)
        (49): GRU(1, 128, batch_first=True)
        (50): GRU(1, 128, batch_first=True)
        (51): GRU(1, 128, batch_first=True)
        (52): GRU(1, 128, batch_first=True)
        (53): GRU(1, 128, batch_first=True)
        (54): GRU(1, 128, batch_first=True)
        (55): GRU(1, 128, batch_first=True)
        (56): GRU(1, 128, batch_first=True)
        (57): GRU(1, 128, batch_first=True)
        (58): GRU(1, 128, batch_first=True)
        (59): GRU(1, 128, batch_first=True)
        (60): GRU(1, 128, batch_first=True)
        (61): GRU(1, 128, batch_first=True)
        (62): GRU(1, 128, batch_first=True)
        (63): GRU(1, 128, batch_first=True)
        (64): GRU(1, 128, batch_first=True)
        (65): GRU(1, 128, batch_first=True)
        (66): GRU(1, 128, batch_first=True)
        (67): GRU(1, 128, batch_first=True)
        (68): GRU(1, 128, batch_first=True)
        (69): GRU(1, 128, batch_first=True)
        (70): GRU(1, 128, batch_first=True)
        (71): GRU(1, 128, batch_first=True)
        (72): GRU(1, 128, batch_first=True)
        (73): GRU(1, 128, batch_first=True)
        (74): GRU(1, 128, batch_first=True)
        (75): GRU(1, 128, batch_first=True)
        (76): GRU(1, 128, batch_first=True)
        (77): GRU(1, 128, batch_first=True)
        (78): GRU(1, 128, batch_first=True)
        (79): GRU(1, 128, batch_first=True)
        (80): GRU(1, 128, batch_first=True)
        (81): GRU(1, 128, batch_first=True)
        (82): GRU(1, 128, batch_first=True)
        (83): GRU(1, 128, batch_first=True)
        (84): GRU(1, 128, batch_first=True)
        (85): GRU(1, 128, batch_first=True)
        (86): GRU(1, 128, batch_first=True)
        (87): GRU(1, 128, batch_first=True)
        (88): GRU(1, 128, batch_first=True)
        (89): GRU(1, 128, batch_first=True)
        (90): GRU(1, 128, batch_first=True)
        (91): GRU(1, 128, batch_first=True)
        (92): GRU(1, 128, batch_first=True)
        (93): GRU(1, 128, batch_first=True)
        (94): GRU(1, 128, batch_first=True)
        (95): GRU(1, 128, batch_first=True)
        (96): GRU(1, 128, batch_first=True)
        (97): GRU(1, 128, batch_first=True)
        (98): GRU(1, 128, batch_first=True)
        (99): GRU(1, 128, batch_first=True)
        (100): GRU(1, 128, batch_first=True)
        (101): GRU(1, 128, batch_first=True)
        (102): GRU(1, 128, batch_first=True)
        (103): GRU(1, 128, batch_first=True)
        (104): GRU(1, 128, batch_first=True)
        (105): GRU(1, 128, batch_first=True)
        (106): GRU(1, 128, batch_first=True)
        (107): GRU(1, 128, batch_first=True)
        (108): GRU(1, 128, batch_first=True)
        (109): GRU(1, 128, batch_first=True)
        (110): GRU(1, 128, batch_first=True)
        (111): GRU(1, 128, batch_first=True)
        (112): GRU(1, 128, batch_first=True)
        (113): GRU(1, 128, batch_first=True)
        (114): GRU(1, 128, batch_first=True)
        (115): GRU(1, 128, batch_first=True)
        (116): GRU(1, 128, batch_first=True)
        (117): GRU(1, 128, batch_first=True)
        (118): GRU(1, 128, batch_first=True)
        (119): GRU(1, 128, batch_first=True)
        (120): GRU(1, 128, batch_first=True)
        (121): GRU(1, 128, batch_first=True)
        (122): GRU(1, 128, batch_first=True)
        (123): GRU(1, 128, batch_first=True)
        (124): GRU(1, 128, batch_first=True)
        (125): GRU(1, 128, batch_first=True)
        (126): GRU(1, 128, batch_first=True)
        (127): GRU(1, 128, batch_first=True)
      )
      (LastStepAttentions): ModuleList(
        (0): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (1): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (2): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (3): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (4): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (5): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (6): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (7): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (8): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (9): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (10): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (11): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (12): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (13): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (14): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (15): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (16): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (17): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (18): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (19): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (20): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (21): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (22): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (23): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (24): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (25): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (26): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (27): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (28): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (29): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (30): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (31): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (32): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (33): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (34): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (35): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (36): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (37): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (38): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (39): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (40): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (41): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (42): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (43): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (44): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (45): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (46): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (47): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (48): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (49): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (50): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (51): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (52): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (53): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (54): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (55): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (56): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (57): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (58): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (59): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (60): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (61): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (62): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (63): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (64): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (65): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (66): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (67): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (68): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (69): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (70): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (71): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (72): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (73): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (74): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (75): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (76): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (77): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (78): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (79): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (80): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (81): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (82): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (83): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (84): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (85): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (86): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (87): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (88): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (89): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (90): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (91): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (92): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (93): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (94): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (95): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (96): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (97): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (98): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (99): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (100): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (101): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (102): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (103): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (104): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (105): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (106): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (107): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (108): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (109): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (110): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (111): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (112): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (113): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (114): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (115): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (116): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (117): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (118): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (119): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (120): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (121): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (122): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (123): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (124): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (125): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (126): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (127): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
      )
      (FinalAttentionQKV): FinalAttentionQKV(
        (W_q): Linear(in_features=128, out_features=128, bias=True)
        (W_k): Linear(in_features=128, out_features=128, bias=True)
        (W_v): Linear(in_features=128, out_features=128, bias=True)
        (W_out): Linear(in_features=128, out_features=1, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (tanh): Tanh()
        (softmax): Softmax(dim=1)
        (sigmoid): Sigmoid()
      )
      (MultiHeadedAttention): MultiHeadedAttention(
        (linears): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Linear(in_features=128, out_features=128, bias=True)
          (2): Linear(in_features=128, out_features=128, bias=True)
        )
        (final_linear): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (SublayerConnection): SublayerConnection(
        (norm): LayerNorm()
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (PositionwiseFeedForward): PositionwiseFeedForward(
        (w_1): Linear(in_features=128, out_features=64, bias=True)
        (w_2): Linear(in_features=64, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.5, inplace=False)
      (tanh): Tanh()
      (softmax): Softmax(dim=None)
      (sigmoid): Sigmoid()
      (relu): ReLU()
    )
  )
  (fc): Linear(in_features=256, out_features=1, bias=True)
)
2023-07-05 16:14:46 Metrics: ['accuracy', 'pr_auc', 'roc_auc', 'f1']
2023-07-05 16:14:46 Device: cuda
2023-07-05 16:14:46 
2023-07-05 16:14:46 Training:
2023-07-05 16:14:46 Batch size: 32
2023-07-05 16:14:46 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-05 16:14:46 Optimizer params: {'lr': 0.001}
2023-07-05 16:14:46 Weight decay: 0.0
2023-07-05 16:14:46 Max grad norm: None
2023-07-05 16:14:46 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd7958f73d0>
2023-07-05 16:14:46 Monitor: roc_auc
2023-07-05 16:14:46 Monitor criterion: max
2023-07-05 16:14:46 Epochs: 20
2023-07-05 16:14:46 
2023-07-05 16:18:26 --- Train epoch-0, step-243 ---
2023-07-05 16:18:26 loss: 0.6915
2023-07-05 16:18:33 --- Eval epoch-0, step-243 ---
2023-07-05 16:18:33 accuracy: 0.5597
2023-07-05 16:18:33 pr_auc: 0.6133
2023-07-05 16:18:33 roc_auc: 0.5750
2023-07-05 16:18:33 f1: 0.6745
2023-07-05 16:18:33 loss: 0.6820
2023-07-05 16:18:33 New best roc_auc score (0.5750) at epoch-0, step-243
2023-07-05 16:18:33 
2023-07-05 16:22:03 --- Train epoch-1, step-486 ---
2023-07-05 16:22:03 loss: 0.6858
2023-07-05 16:22:11 --- Eval epoch-1, step-486 ---
2023-07-05 16:22:11 accuracy: 0.5757
2023-07-05 16:22:11 pr_auc: 0.6391
2023-07-05 16:22:11 roc_auc: 0.5921
2023-07-05 16:22:11 f1: 0.7133
2023-07-05 16:22:11 loss: 0.6761
2023-07-05 16:22:11 New best roc_auc score (0.5921) at epoch-1, step-486
2023-07-05 16:22:12 
2023-07-05 16:25:43 --- Train epoch-2, step-729 ---
2023-07-05 16:25:43 loss: 0.6714
2023-07-05 16:25:51 --- Eval epoch-2, step-729 ---
2023-07-05 16:25:51 accuracy: 0.6034
2023-07-05 16:25:51 pr_auc: 0.6588
2023-07-05 16:25:51 roc_auc: 0.6336
2023-07-05 16:25:51 f1: 0.7005
2023-07-05 16:25:51 loss: 0.6643
2023-07-05 16:25:51 New best roc_auc score (0.6336) at epoch-2, step-729
2023-07-05 16:25:51 
2023-07-05 16:29:23 --- Train epoch-3, step-972 ---
2023-07-05 16:29:23 loss: 0.6414
2023-07-05 16:29:32 --- Eval epoch-3, step-972 ---
2023-07-05 16:29:32 accuracy: 0.5938
2023-07-05 16:29:32 pr_auc: 0.6667
2023-07-05 16:29:32 roc_auc: 0.6366
2023-07-05 16:29:32 f1: 0.6019
2023-07-05 16:29:32 loss: 0.6780
2023-07-05 16:29:32 New best roc_auc score (0.6366) at epoch-3, step-972
2023-07-05 16:29:32 
2023-07-05 16:33:04 --- Train epoch-4, step-1215 ---
2023-07-05 16:33:04 loss: 0.6155
2023-07-05 16:33:12 --- Eval epoch-4, step-1215 ---
2023-07-05 16:33:12 accuracy: 0.6045
2023-07-05 16:33:12 pr_auc: 0.6666
2023-07-05 16:33:12 roc_auc: 0.6278
2023-07-05 16:33:12 f1: 0.6941
2023-07-05 16:33:12 loss: 0.6845
2023-07-05 16:33:12 
2023-07-05 16:36:41 --- Train epoch-5, step-1458 ---
2023-07-05 16:36:41 loss: 0.5946
2023-07-05 16:36:49 --- Eval epoch-5, step-1458 ---
2023-07-05 16:36:49 accuracy: 0.6066
2023-07-05 16:36:49 pr_auc: 0.6659
2023-07-05 16:36:49 roc_auc: 0.6291
2023-07-05 16:36:49 f1: 0.6933
2023-07-05 16:36:49 loss: 0.6945
2023-07-05 16:36:49 
2023-07-05 16:40:22 --- Train epoch-6, step-1701 ---
2023-07-05 16:40:22 loss: 0.5692
2023-07-05 16:40:29 --- Eval epoch-6, step-1701 ---
2023-07-05 16:40:29 accuracy: 0.6087
2023-07-05 16:40:29 pr_auc: 0.6587
2023-07-05 16:40:29 roc_auc: 0.6252
2023-07-05 16:40:29 f1: 0.6767
2023-07-05 16:40:29 loss: 0.6897
2023-07-05 16:40:29 
2023-07-05 16:44:00 --- Train epoch-7, step-1944 ---
2023-07-05 16:44:00 loss: 0.5393
2023-07-05 16:44:08 --- Eval epoch-7, step-1944 ---
2023-07-05 16:44:08 accuracy: 0.5959
2023-07-05 16:44:08 pr_auc: 0.6636
2023-07-05 16:44:08 roc_auc: 0.6286
2023-07-05 16:44:08 f1: 0.6089
2023-07-05 16:44:08 loss: 0.7305
2023-07-05 16:44:09 
2023-07-05 16:47:40 --- Train epoch-8, step-2187 ---
2023-07-05 16:47:40 loss: 0.5110
2023-07-05 16:47:48 --- Eval epoch-8, step-2187 ---
2023-07-05 16:47:48 accuracy: 0.5885
2023-07-05 16:47:48 pr_auc: 0.6612
2023-07-05 16:47:48 roc_auc: 0.6266
2023-07-05 16:47:48 f1: 0.6230
2023-07-05 16:47:48 loss: 0.7435
2023-07-05 16:47:48 
2023-07-05 16:51:20 --- Train epoch-9, step-2430 ---
2023-07-05 16:51:20 loss: 0.4849
2023-07-05 16:51:28 --- Eval epoch-9, step-2430 ---
2023-07-05 16:51:28 accuracy: 0.5586
2023-07-05 16:51:28 pr_auc: 0.6563
2023-07-05 16:51:28 roc_auc: 0.6221
2023-07-05 16:51:28 f1: 0.5118
2023-07-05 16:51:28 loss: 0.8284
2023-07-05 16:51:28 
2023-07-05 16:55:02 --- Train epoch-10, step-2673 ---
2023-07-05 16:55:02 loss: 0.4538
2023-07-05 16:55:11 --- Eval epoch-10, step-2673 ---
2023-07-05 16:55:11 accuracy: 0.6055
2023-07-05 16:55:11 pr_auc: 0.6669
2023-07-05 16:55:11 roc_auc: 0.6310
2023-07-05 16:55:11 f1: 0.6496
2023-07-05 16:55:11 loss: 0.7783
2023-07-05 16:55:11 
2023-07-05 16:58:40 --- Train epoch-11, step-2916 ---
2023-07-05 16:58:40 loss: 0.4291
2023-07-05 16:58:47 --- Eval epoch-11, step-2916 ---
2023-07-05 16:58:47 accuracy: 0.5736
2023-07-05 16:58:47 pr_auc: 0.6539
2023-07-05 16:58:47 roc_auc: 0.6212
2023-07-05 16:58:47 f1: 0.5816
2023-07-05 16:58:47 loss: 0.8937
2023-07-05 16:58:47 
2023-07-05 17:02:18 --- Train epoch-12, step-3159 ---
2023-07-05 17:02:18 loss: 0.3990
2023-07-05 17:02:25 --- Eval epoch-12, step-3159 ---
2023-07-05 17:02:25 accuracy: 0.5959
2023-07-05 17:02:25 pr_auc: 0.6619
2023-07-05 17:02:25 roc_auc: 0.6246
2023-07-05 17:02:25 f1: 0.6266
2023-07-05 17:02:25 loss: 0.8560
2023-07-05 17:02:25 
2023-07-05 17:05:59 --- Train epoch-13, step-3402 ---
2023-07-05 17:05:59 loss: 0.3629
2023-07-05 17:06:07 --- Eval epoch-13, step-3402 ---
2023-07-05 17:06:07 accuracy: 0.5896
2023-07-05 17:06:07 pr_auc: 0.6639
2023-07-05 17:06:07 roc_auc: 0.6223
2023-07-05 17:06:07 f1: 0.6358
2023-07-05 17:06:07 loss: 0.9227
2023-07-05 17:06:07 
2023-07-05 17:09:42 --- Train epoch-14, step-3645 ---
2023-07-05 17:09:42 loss: 0.3283
2023-07-05 17:09:50 --- Eval epoch-14, step-3645 ---
2023-07-05 17:09:50 accuracy: 0.5981
2023-07-05 17:09:50 pr_auc: 0.6619
2023-07-05 17:09:50 roc_auc: 0.6213
2023-07-05 17:09:50 f1: 0.6278
2023-07-05 17:09:50 loss: 0.9936
2023-07-05 17:09:50 
2023-07-05 17:13:15 --- Train epoch-15, step-3888 ---
2023-07-05 17:13:15 loss: 0.3094
2023-07-05 17:13:23 --- Eval epoch-15, step-3888 ---
2023-07-05 17:13:23 accuracy: 0.5959
2023-07-05 17:13:23 pr_auc: 0.6622
2023-07-05 17:13:23 roc_auc: 0.6234
2023-07-05 17:13:23 f1: 0.6031
2023-07-05 17:13:23 loss: 1.0232
2023-07-05 17:13:23 
2023-07-05 17:16:54 --- Train epoch-16, step-4131 ---
2023-07-05 17:16:54 loss: 0.2770
2023-07-05 17:17:02 --- Eval epoch-16, step-4131 ---
2023-07-05 17:17:02 accuracy: 0.5938
2023-07-05 17:17:02 pr_auc: 0.6543
2023-07-05 17:17:02 roc_auc: 0.6160
2023-07-05 17:17:02 f1: 0.6194
2023-07-05 17:17:02 loss: 1.0673
2023-07-05 17:17:02 
2023-07-05 17:20:31 --- Train epoch-17, step-4374 ---
2023-07-05 17:20:31 loss: 0.2463
2023-07-05 17:20:38 --- Eval epoch-17, step-4374 ---
2023-07-05 17:20:38 accuracy: 0.5970
2023-07-05 17:20:38 pr_auc: 0.6551
2023-07-05 17:20:38 roc_auc: 0.6167
2023-07-05 17:20:38 f1: 0.6159
2023-07-05 17:20:38 loss: 1.1373
2023-07-05 17:20:38 
2023-07-05 17:24:10 --- Train epoch-18, step-4617 ---
2023-07-05 17:24:10 loss: 0.2195
2023-07-05 17:24:17 --- Eval epoch-18, step-4617 ---
2023-07-05 17:24:17 accuracy: 0.5821
2023-07-05 17:24:17 pr_auc: 0.6516
2023-07-05 17:24:17 roc_auc: 0.6079
2023-07-05 17:24:17 f1: 0.6032
2023-07-05 17:24:17 loss: 1.2632
2023-07-05 17:24:17 
2023-07-05 17:27:46 --- Train epoch-19, step-4860 ---
2023-07-05 17:27:46 loss: 0.1989
2023-07-05 17:27:53 --- Eval epoch-19, step-4860 ---
2023-07-05 17:27:53 accuracy: 0.5896
2023-07-05 17:27:53 pr_auc: 0.6575
2023-07-05 17:27:53 roc_auc: 0.6175
2023-07-05 17:27:53 f1: 0.6309
2023-07-05 17:27:53 loss: 1.4277
2023-07-05 17:27:53 Loaded best model
2023-07-05 17:27:56 ConCare(
  (embeddings): ModuleDict(
    (conditions): Embedding(4031, 128, padding_idx=0)
    (procedures): Embedding(1276, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (concare): ModuleDict(
    (conditions): ConCareLayer(
      (PositionalEncoding): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (GRUs): ModuleList(
        (0): GRU(1, 128, batch_first=True)
        (1): GRU(1, 128, batch_first=True)
        (2): GRU(1, 128, batch_first=True)
        (3): GRU(1, 128, batch_first=True)
        (4): GRU(1, 128, batch_first=True)
        (5): GRU(1, 128, batch_first=True)
        (6): GRU(1, 128, batch_first=True)
        (7): GRU(1, 128, batch_first=True)
        (8): GRU(1, 128, batch_first=True)
        (9): GRU(1, 128, batch_first=True)
        (10): GRU(1, 128, batch_first=True)
        (11): GRU(1, 128, batch_first=True)
        (12): GRU(1, 128, batch_first=True)
        (13): GRU(1, 128, batch_first=True)
        (14): GRU(1, 128, batch_first=True)
        (15): GRU(1, 128, batch_first=True)
        (16): GRU(1, 128, batch_first=True)
        (17): GRU(1, 128, batch_first=True)
        (18): GRU(1, 128, batch_first=True)
        (19): GRU(1, 128, batch_first=True)
        (20): GRU(1, 128, batch_first=True)
        (21): GRU(1, 128, batch_first=True)
        (22): GRU(1, 128, batch_first=True)
        (23): GRU(1, 128, batch_first=True)
        (24): GRU(1, 128, batch_first=True)
        (25): GRU(1, 128, batch_first=True)
        (26): GRU(1, 128, batch_first=True)
        (27): GRU(1, 128, batch_first=True)
        (28): GRU(1, 128, batch_first=True)
        (29): GRU(1, 128, batch_first=True)
        (30): GRU(1, 128, batch_first=True)
        (31): GRU(1, 128, batch_first=True)
        (32): GRU(1, 128, batch_first=True)
        (33): GRU(1, 128, batch_first=True)
        (34): GRU(1, 128, batch_first=True)
        (35): GRU(1, 128, batch_first=True)
        (36): GRU(1, 128, batch_first=True)
        (37): GRU(1, 128, batch_first=True)
        (38): GRU(1, 128, batch_first=True)
        (39): GRU(1, 128, batch_first=True)
        (40): GRU(1, 128, batch_first=True)
        (41): GRU(1, 128, batch_first=True)
        (42): GRU(1, 128, batch_first=True)
        (43): GRU(1, 128, batch_first=True)
        (44): GRU(1, 128, batch_first=True)
        (45): GRU(1, 128, batch_first=True)
        (46): GRU(1, 128, batch_first=True)
        (47): GRU(1, 128, batch_first=True)
        (48): GRU(1, 128, batch_first=True)
        (49): GRU(1, 128, batch_first=True)
        (50): GRU(1, 128, batch_first=True)
        (51): GRU(1, 128, batch_first=True)
        (52): GRU(1, 128, batch_first=True)
        (53): GRU(1, 128, batch_first=True)
        (54): GRU(1, 128, batch_first=True)
        (55): GRU(1, 128, batch_first=True)
        (56): GRU(1, 128, batch_first=True)
        (57): GRU(1, 128, batch_first=True)
        (58): GRU(1, 128, batch_first=True)
        (59): GRU(1, 128, batch_first=True)
        (60): GRU(1, 128, batch_first=True)
        (61): GRU(1, 128, batch_first=True)
        (62): GRU(1, 128, batch_first=True)
        (63): GRU(1, 128, batch_first=True)
        (64): GRU(1, 128, batch_first=True)
        (65): GRU(1, 128, batch_first=True)
        (66): GRU(1, 128, batch_first=True)
        (67): GRU(1, 128, batch_first=True)
        (68): GRU(1, 128, batch_first=True)
        (69): GRU(1, 128, batch_first=True)
        (70): GRU(1, 128, batch_first=True)
        (71): GRU(1, 128, batch_first=True)
        (72): GRU(1, 128, batch_first=True)
        (73): GRU(1, 128, batch_first=True)
        (74): GRU(1, 128, batch_first=True)
        (75): GRU(1, 128, batch_first=True)
        (76): GRU(1, 128, batch_first=True)
        (77): GRU(1, 128, batch_first=True)
        (78): GRU(1, 128, batch_first=True)
        (79): GRU(1, 128, batch_first=True)
        (80): GRU(1, 128, batch_first=True)
        (81): GRU(1, 128, batch_first=True)
        (82): GRU(1, 128, batch_first=True)
        (83): GRU(1, 128, batch_first=True)
        (84): GRU(1, 128, batch_first=True)
        (85): GRU(1, 128, batch_first=True)
        (86): GRU(1, 128, batch_first=True)
        (87): GRU(1, 128, batch_first=True)
        (88): GRU(1, 128, batch_first=True)
        (89): GRU(1, 128, batch_first=True)
        (90): GRU(1, 128, batch_first=True)
        (91): GRU(1, 128, batch_first=True)
        (92): GRU(1, 128, batch_first=True)
        (93): GRU(1, 128, batch_first=True)
        (94): GRU(1, 128, batch_first=True)
        (95): GRU(1, 128, batch_first=True)
        (96): GRU(1, 128, batch_first=True)
        (97): GRU(1, 128, batch_first=True)
        (98): GRU(1, 128, batch_first=True)
        (99): GRU(1, 128, batch_first=True)
        (100): GRU(1, 128, batch_first=True)
        (101): GRU(1, 128, batch_first=True)
        (102): GRU(1, 128, batch_first=True)
        (103): GRU(1, 128, batch_first=True)
        (104): GRU(1, 128, batch_first=True)
        (105): GRU(1, 128, batch_first=True)
        (106): GRU(1, 128, batch_first=True)
        (107): GRU(1, 128, batch_first=True)
        (108): GRU(1, 128, batch_first=True)
        (109): GRU(1, 128, batch_first=True)
        (110): GRU(1, 128, batch_first=True)
        (111): GRU(1, 128, batch_first=True)
        (112): GRU(1, 128, batch_first=True)
        (113): GRU(1, 128, batch_first=True)
        (114): GRU(1, 128, batch_first=True)
        (115): GRU(1, 128, batch_first=True)
        (116): GRU(1, 128, batch_first=True)
        (117): GRU(1, 128, batch_first=True)
        (118): GRU(1, 128, batch_first=True)
        (119): GRU(1, 128, batch_first=True)
        (120): GRU(1, 128, batch_first=True)
        (121): GRU(1, 128, batch_first=True)
        (122): GRU(1, 128, batch_first=True)
        (123): GRU(1, 128, batch_first=True)
        (124): GRU(1, 128, batch_first=True)
        (125): GRU(1, 128, batch_first=True)
        (126): GRU(1, 128, batch_first=True)
        (127): GRU(1, 128, batch_first=True)
      )
      (LastStepAttentions): ModuleList(
        (0): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (1): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (2): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (3): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (4): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (5): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (6): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (7): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (8): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (9): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (10): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (11): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (12): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (13): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (14): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (15): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (16): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (17): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (18): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (19): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (20): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (21): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (22): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (23): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (24): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (25): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (26): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (27): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (28): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (29): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (30): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (31): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (32): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (33): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (34): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (35): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (36): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (37): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (38): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (39): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (40): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (41): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (42): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (43): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (44): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (45): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (46): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (47): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (48): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (49): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (50): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (51): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (52): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (53): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (54): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (55): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (56): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (57): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (58): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (59): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (60): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (61): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (62): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (63): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (64): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (65): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (66): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (67): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (68): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (69): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (70): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (71): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (72): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (73): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (74): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (75): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (76): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (77): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (78): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (79): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (80): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (81): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (82): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (83): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (84): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (85): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (86): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (87): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (88): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (89): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (90): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (91): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (92): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (93): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (94): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (95): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (96): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (97): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (98): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (99): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (100): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (101): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (102): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (103): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (104): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (105): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (106): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (107): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (108): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (109): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (110): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (111): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (112): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (113): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (114): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (115): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (116): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (117): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (118): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (119): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (120): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (121): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (122): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (123): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (124): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (125): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (126): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (127): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
      )
      (FinalAttentionQKV): FinalAttentionQKV(
        (W_q): Linear(in_features=128, out_features=128, bias=True)
        (W_k): Linear(in_features=128, out_features=128, bias=True)
        (W_v): Linear(in_features=128, out_features=128, bias=True)
        (W_out): Linear(in_features=128, out_features=1, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (tanh): Tanh()
        (softmax): Softmax(dim=1)
        (sigmoid): Sigmoid()
      )
      (MultiHeadedAttention): MultiHeadedAttention(
        (linears): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Linear(in_features=128, out_features=128, bias=True)
          (2): Linear(in_features=128, out_features=128, bias=True)
        )
        (final_linear): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (SublayerConnection): SublayerConnection(
        (norm): LayerNorm()
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (PositionwiseFeedForward): PositionwiseFeedForward(
        (w_1): Linear(in_features=128, out_features=64, bias=True)
        (w_2): Linear(in_features=64, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.5, inplace=False)
      (tanh): Tanh()
      (softmax): Softmax(dim=None)
      (sigmoid): Sigmoid()
      (relu): ReLU()
    )
    (procedures): ConCareLayer(
      (PositionalEncoding): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (GRUs): ModuleList(
        (0): GRU(1, 128, batch_first=True)
        (1): GRU(1, 128, batch_first=True)
        (2): GRU(1, 128, batch_first=True)
        (3): GRU(1, 128, batch_first=True)
        (4): GRU(1, 128, batch_first=True)
        (5): GRU(1, 128, batch_first=True)
        (6): GRU(1, 128, batch_first=True)
        (7): GRU(1, 128, batch_first=True)
        (8): GRU(1, 128, batch_first=True)
        (9): GRU(1, 128, batch_first=True)
        (10): GRU(1, 128, batch_first=True)
        (11): GRU(1, 128, batch_first=True)
        (12): GRU(1, 128, batch_first=True)
        (13): GRU(1, 128, batch_first=True)
        (14): GRU(1, 128, batch_first=True)
        (15): GRU(1, 128, batch_first=True)
        (16): GRU(1, 128, batch_first=True)
        (17): GRU(1, 128, batch_first=True)
        (18): GRU(1, 128, batch_first=True)
        (19): GRU(1, 128, batch_first=True)
        (20): GRU(1, 128, batch_first=True)
        (21): GRU(1, 128, batch_first=True)
        (22): GRU(1, 128, batch_first=True)
        (23): GRU(1, 128, batch_first=True)
        (24): GRU(1, 128, batch_first=True)
        (25): GRU(1, 128, batch_first=True)
        (26): GRU(1, 128, batch_first=True)
        (27): GRU(1, 128, batch_first=True)
        (28): GRU(1, 128, batch_first=True)
        (29): GRU(1, 128, batch_first=True)
        (30): GRU(1, 128, batch_first=True)
        (31): GRU(1, 128, batch_first=True)
        (32): GRU(1, 128, batch_first=True)
        (33): GRU(1, 128, batch_first=True)
        (34): GRU(1, 128, batch_first=True)
        (35): GRU(1, 128, batch_first=True)
        (36): GRU(1, 128, batch_first=True)
        (37): GRU(1, 128, batch_first=True)
        (38): GRU(1, 128, batch_first=True)
        (39): GRU(1, 128, batch_first=True)
        (40): GRU(1, 128, batch_first=True)
        (41): GRU(1, 128, batch_first=True)
        (42): GRU(1, 128, batch_first=True)
        (43): GRU(1, 128, batch_first=True)
        (44): GRU(1, 128, batch_first=True)
        (45): GRU(1, 128, batch_first=True)
        (46): GRU(1, 128, batch_first=True)
        (47): GRU(1, 128, batch_first=True)
        (48): GRU(1, 128, batch_first=True)
        (49): GRU(1, 128, batch_first=True)
        (50): GRU(1, 128, batch_first=True)
        (51): GRU(1, 128, batch_first=True)
        (52): GRU(1, 128, batch_first=True)
        (53): GRU(1, 128, batch_first=True)
        (54): GRU(1, 128, batch_first=True)
        (55): GRU(1, 128, batch_first=True)
        (56): GRU(1, 128, batch_first=True)
        (57): GRU(1, 128, batch_first=True)
        (58): GRU(1, 128, batch_first=True)
        (59): GRU(1, 128, batch_first=True)
        (60): GRU(1, 128, batch_first=True)
        (61): GRU(1, 128, batch_first=True)
        (62): GRU(1, 128, batch_first=True)
        (63): GRU(1, 128, batch_first=True)
        (64): GRU(1, 128, batch_first=True)
        (65): GRU(1, 128, batch_first=True)
        (66): GRU(1, 128, batch_first=True)
        (67): GRU(1, 128, batch_first=True)
        (68): GRU(1, 128, batch_first=True)
        (69): GRU(1, 128, batch_first=True)
        (70): GRU(1, 128, batch_first=True)
        (71): GRU(1, 128, batch_first=True)
        (72): GRU(1, 128, batch_first=True)
        (73): GRU(1, 128, batch_first=True)
        (74): GRU(1, 128, batch_first=True)
        (75): GRU(1, 128, batch_first=True)
        (76): GRU(1, 128, batch_first=True)
        (77): GRU(1, 128, batch_first=True)
        (78): GRU(1, 128, batch_first=True)
        (79): GRU(1, 128, batch_first=True)
        (80): GRU(1, 128, batch_first=True)
        (81): GRU(1, 128, batch_first=True)
        (82): GRU(1, 128, batch_first=True)
        (83): GRU(1, 128, batch_first=True)
        (84): GRU(1, 128, batch_first=True)
        (85): GRU(1, 128, batch_first=True)
        (86): GRU(1, 128, batch_first=True)
        (87): GRU(1, 128, batch_first=True)
        (88): GRU(1, 128, batch_first=True)
        (89): GRU(1, 128, batch_first=True)
        (90): GRU(1, 128, batch_first=True)
        (91): GRU(1, 128, batch_first=True)
        (92): GRU(1, 128, batch_first=True)
        (93): GRU(1, 128, batch_first=True)
        (94): GRU(1, 128, batch_first=True)
        (95): GRU(1, 128, batch_first=True)
        (96): GRU(1, 128, batch_first=True)
        (97): GRU(1, 128, batch_first=True)
        (98): GRU(1, 128, batch_first=True)
        (99): GRU(1, 128, batch_first=True)
        (100): GRU(1, 128, batch_first=True)
        (101): GRU(1, 128, batch_first=True)
        (102): GRU(1, 128, batch_first=True)
        (103): GRU(1, 128, batch_first=True)
        (104): GRU(1, 128, batch_first=True)
        (105): GRU(1, 128, batch_first=True)
        (106): GRU(1, 128, batch_first=True)
        (107): GRU(1, 128, batch_first=True)
        (108): GRU(1, 128, batch_first=True)
        (109): GRU(1, 128, batch_first=True)
        (110): GRU(1, 128, batch_first=True)
        (111): GRU(1, 128, batch_first=True)
        (112): GRU(1, 128, batch_first=True)
        (113): GRU(1, 128, batch_first=True)
        (114): GRU(1, 128, batch_first=True)
        (115): GRU(1, 128, batch_first=True)
        (116): GRU(1, 128, batch_first=True)
        (117): GRU(1, 128, batch_first=True)
        (118): GRU(1, 128, batch_first=True)
        (119): GRU(1, 128, batch_first=True)
        (120): GRU(1, 128, batch_first=True)
        (121): GRU(1, 128, batch_first=True)
        (122): GRU(1, 128, batch_first=True)
        (123): GRU(1, 128, batch_first=True)
        (124): GRU(1, 128, batch_first=True)
        (125): GRU(1, 128, batch_first=True)
        (126): GRU(1, 128, batch_first=True)
        (127): GRU(1, 128, batch_first=True)
      )
      (LastStepAttentions): ModuleList(
        (0): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (1): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (2): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (3): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (4): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (5): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (6): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (7): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (8): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (9): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (10): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (11): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (12): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (13): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (14): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (15): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (16): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (17): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (18): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (19): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (20): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (21): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (22): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (23): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (24): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (25): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (26): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (27): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (28): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (29): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (30): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (31): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (32): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (33): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (34): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (35): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (36): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (37): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (38): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (39): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (40): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (41): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (42): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (43): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (44): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (45): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (46): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (47): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (48): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (49): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (50): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (51): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (52): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (53): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (54): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (55): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (56): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (57): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (58): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (59): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (60): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (61): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (62): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (63): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (64): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (65): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (66): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (67): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (68): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (69): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (70): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (71): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (72): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (73): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (74): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (75): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (76): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (77): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (78): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (79): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (80): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (81): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (82): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (83): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (84): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (85): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (86): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (87): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (88): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (89): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (90): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (91): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (92): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (93): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (94): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (95): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (96): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (97): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (98): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (99): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (100): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (101): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (102): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (103): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (104): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (105): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (106): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (107): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (108): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (109): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (110): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (111): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (112): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (113): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (114): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (115): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (116): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (117): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (118): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (119): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (120): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (121): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (122): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (123): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (124): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (125): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (126): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (127): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
      )
      (FinalAttentionQKV): FinalAttentionQKV(
        (W_q): Linear(in_features=128, out_features=128, bias=True)
        (W_k): Linear(in_features=128, out_features=128, bias=True)
        (W_v): Linear(in_features=128, out_features=128, bias=True)
        (W_out): Linear(in_features=128, out_features=1, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (tanh): Tanh()
        (softmax): Softmax(dim=1)
        (sigmoid): Sigmoid()
      )
      (MultiHeadedAttention): MultiHeadedAttention(
        (linears): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Linear(in_features=128, out_features=128, bias=True)
          (2): Linear(in_features=128, out_features=128, bias=True)
        )
        (final_linear): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (SublayerConnection): SublayerConnection(
        (norm): LayerNorm()
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (PositionwiseFeedForward): PositionwiseFeedForward(
        (w_1): Linear(in_features=128, out_features=64, bias=True)
        (w_2): Linear(in_features=64, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.5, inplace=False)
      (tanh): Tanh()
      (softmax): Softmax(dim=None)
      (sigmoid): Sigmoid()
      (relu): ReLU()
    )
  )
  (fc): Linear(in_features=256, out_features=1, bias=True)
)
2023-07-05 17:27:56 Metrics: ['accuracy', 'pr_auc', 'roc_auc', 'f1']
2023-07-05 17:27:56 Device: cuda
2023-07-05 17:27:56 
2023-07-05 17:27:56 Training:
2023-07-05 17:27:56 Batch size: 32
2023-07-05 17:27:56 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-05 17:27:56 Optimizer params: {'lr': 0.001}
2023-07-05 17:27:56 Weight decay: 0.0
2023-07-05 17:27:56 Max grad norm: None
2023-07-05 17:27:56 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd7958ef150>
2023-07-05 17:27:56 Monitor: roc_auc
2023-07-05 17:27:56 Monitor criterion: max
2023-07-05 17:27:56 Epochs: 20
2023-07-05 17:27:56 
2023-07-05 17:31:33 --- Train epoch-0, step-241 ---
2023-07-05 17:31:33 loss: 0.2664
2023-07-05 17:31:40 --- Eval epoch-0, step-241 ---
2023-07-05 17:31:40 accuracy: 0.9379
2023-07-05 17:31:40 pr_auc: 0.0762
2023-07-05 17:31:40 roc_auc: 0.5420
2023-07-05 17:31:40 f1: 0.0000
2023-07-05 17:31:40 loss: 0.2352
2023-07-05 17:31:40 New best roc_auc score (0.5420) at epoch-0, step-241
2023-07-05 17:31:41 
2023-07-05 17:35:15 --- Train epoch-1, step-482 ---
2023-07-05 17:35:15 loss: 0.2547
2023-07-05 17:35:23 --- Eval epoch-1, step-482 ---
2023-07-05 17:35:23 accuracy: 0.9379
2023-07-05 17:35:23 pr_auc: 0.0798
2023-07-05 17:35:23 roc_auc: 0.5680
2023-07-05 17:35:23 f1: 0.0000
2023-07-05 17:35:23 loss: 0.2337
2023-07-05 17:35:23 New best roc_auc score (0.5680) at epoch-1, step-482
2023-07-05 17:35:24 
2023-07-05 17:39:00 --- Train epoch-2, step-723 ---
2023-07-05 17:39:00 loss: 0.2560
2023-07-05 17:39:08 --- Eval epoch-2, step-723 ---
2023-07-05 17:39:08 accuracy: 0.9379
2023-07-05 17:39:08 pr_auc: 0.0975
2023-07-05 17:39:08 roc_auc: 0.6146
2023-07-05 17:39:08 f1: 0.0000
2023-07-05 17:39:08 loss: 0.2329
2023-07-05 17:39:08 New best roc_auc score (0.6146) at epoch-2, step-723
2023-07-05 17:39:08 
2023-07-05 17:42:42 --- Train epoch-3, step-964 ---
2023-07-05 17:42:42 loss: 0.2469
2023-07-05 17:42:51 --- Eval epoch-3, step-964 ---
2023-07-05 17:42:51 accuracy: 0.9379
2023-07-05 17:42:51 pr_auc: 0.0811
2023-07-05 17:42:51 roc_auc: 0.5706
2023-07-05 17:42:51 f1: 0.0000
2023-07-05 17:42:51 loss: 0.2398
2023-07-05 17:42:51 
2023-07-05 17:46:26 --- Train epoch-4, step-1205 ---
2023-07-05 17:46:26 loss: 0.2279
2023-07-05 17:46:35 --- Eval epoch-4, step-1205 ---
2023-07-05 17:46:35 accuracy: 0.9379
2023-07-05 17:46:35 pr_auc: 0.0835
2023-07-05 17:46:35 roc_auc: 0.5783
2023-07-05 17:46:35 f1: 0.0000
2023-07-05 17:46:35 loss: 0.2492
2023-07-05 17:46:35 
2023-07-05 17:50:35 --- Train epoch-5, step-1446 ---
2023-07-05 17:50:35 loss: 0.1989
2023-07-05 17:50:43 --- Eval epoch-5, step-1446 ---
2023-07-05 17:50:43 accuracy: 0.9349
2023-07-05 17:50:43 pr_auc: 0.0833
2023-07-05 17:50:43 roc_auc: 0.5625
2023-07-05 17:50:43 f1: 0.0571
2023-07-05 17:50:43 loss: 0.2653
2023-07-05 17:50:43 
2023-07-05 17:54:17 --- Train epoch-6, step-1687 ---
2023-07-05 17:54:17 loss: 0.1802
2023-07-05 17:54:25 --- Eval epoch-6, step-1687 ---
2023-07-05 17:54:25 accuracy: 0.9241
2023-07-05 17:54:25 pr_auc: 0.0827
2023-07-05 17:54:25 roc_auc: 0.5524
2023-07-05 17:54:25 f1: 0.0494
2023-07-05 17:54:25 loss: 0.2961
2023-07-05 17:54:25 
2023-07-05 17:57:58 --- Train epoch-7, step-1928 ---
2023-07-05 17:57:58 loss: 0.1662
2023-07-05 17:58:07 --- Eval epoch-7, step-1928 ---
2023-07-05 17:58:07 accuracy: 0.9241
2023-07-05 17:58:07 pr_auc: 0.0813
2023-07-05 17:58:07 roc_auc: 0.5407
2023-07-05 17:58:07 f1: 0.0494
2023-07-05 17:58:07 loss: 0.3134
2023-07-05 17:58:07 
2023-07-05 18:01:38 --- Train epoch-8, step-2169 ---
2023-07-05 18:01:38 loss: 0.1519
2023-07-05 18:01:47 --- Eval epoch-8, step-2169 ---
2023-07-05 18:01:47 accuracy: 0.9103
2023-07-05 18:01:47 pr_auc: 0.0786
2023-07-05 18:01:47 roc_auc: 0.5354
2023-07-05 18:01:47 f1: 0.0619
2023-07-05 18:01:47 loss: 0.3327
2023-07-05 18:01:47 
2023-07-05 18:05:14 --- Train epoch-9, step-2410 ---
2023-07-05 18:05:14 loss: 0.1368
2023-07-05 18:05:23 --- Eval epoch-9, step-2410 ---
2023-07-05 18:05:23 accuracy: 0.9310
2023-07-05 18:05:23 pr_auc: 0.0846
2023-07-05 18:05:23 roc_auc: 0.5296
2023-07-05 18:05:23 f1: 0.0278
2023-07-05 18:05:23 loss: 0.3821
2023-07-05 18:05:23 
2023-07-05 18:08:55 --- Train epoch-10, step-2651 ---
2023-07-05 18:08:55 loss: 0.1124
2023-07-05 18:09:03 --- Eval epoch-10, step-2651 ---
2023-07-05 18:09:03 accuracy: 0.9191
2023-07-05 18:09:03 pr_auc: 0.0738
2023-07-05 18:09:03 roc_auc: 0.5221
2023-07-05 18:09:03 f1: 0.0682
2023-07-05 18:09:03 loss: 0.4403
2023-07-05 18:09:03 
2023-07-05 18:12:38 --- Train epoch-11, step-2892 ---
2023-07-05 18:12:38 loss: 0.0989
2023-07-05 18:12:46 --- Eval epoch-11, step-2892 ---
2023-07-05 18:12:46 accuracy: 0.9142
2023-07-05 18:12:46 pr_auc: 0.0734
2023-07-05 18:12:46 roc_auc: 0.5254
2023-07-05 18:12:46 f1: 0.0645
2023-07-05 18:12:46 loss: 0.4379
2023-07-05 18:12:46 
2023-07-05 18:16:20 --- Train epoch-12, step-3133 ---
2023-07-05 18:16:20 loss: 0.0854
2023-07-05 18:16:28 --- Eval epoch-12, step-3133 ---
2023-07-05 18:16:28 accuracy: 0.9132
2023-07-05 18:16:28 pr_auc: 0.0723
2023-07-05 18:16:28 roc_auc: 0.5152
2023-07-05 18:16:28 f1: 0.0638
2023-07-05 18:16:28 loss: 0.5031
2023-07-05 18:16:28 
2023-07-05 18:19:59 --- Train epoch-13, step-3374 ---
2023-07-05 18:19:59 loss: 0.0809
2023-07-05 18:20:07 --- Eval epoch-13, step-3374 ---
2023-07-05 18:20:07 accuracy: 0.9300
2023-07-05 18:20:07 pr_auc: 0.0712
2023-07-05 18:20:07 roc_auc: 0.5120
2023-07-05 18:20:07 f1: 0.0000
2023-07-05 18:20:07 loss: 0.5666
2023-07-05 18:20:07 
2023-07-05 18:23:41 --- Train epoch-14, step-3615 ---
2023-07-05 18:23:41 loss: 0.0678
2023-07-05 18:23:49 --- Eval epoch-14, step-3615 ---
2023-07-05 18:23:49 accuracy: 0.9162
2023-07-05 18:23:49 pr_auc: 0.0801
2023-07-05 18:23:49 roc_auc: 0.5177
2023-07-05 18:23:49 f1: 0.0860
2023-07-05 18:23:49 loss: 0.5068
2023-07-05 18:23:49 
2023-07-05 18:27:18 --- Train epoch-15, step-3856 ---
2023-07-05 18:27:18 loss: 0.0615
2023-07-05 18:27:26 --- Eval epoch-15, step-3856 ---
2023-07-05 18:27:26 accuracy: 0.9270
2023-07-05 18:27:26 pr_auc: 0.0775
2023-07-05 18:27:26 roc_auc: 0.5080
2023-07-05 18:27:26 f1: 0.0513
2023-07-05 18:27:26 loss: 0.6719
2023-07-05 18:27:26 
2023-07-05 18:30:54 --- Train epoch-16, step-4097 ---
2023-07-05 18:30:54 loss: 0.0522
2023-07-05 18:31:02 --- Eval epoch-16, step-4097 ---
2023-07-05 18:31:02 accuracy: 0.9211
2023-07-05 18:31:02 pr_auc: 0.0656
2023-07-05 18:31:02 roc_auc: 0.4939
2023-07-05 18:31:02 f1: 0.0476
2023-07-05 18:31:02 loss: 0.6962
2023-07-05 18:31:02 
2023-07-05 18:34:38 --- Train epoch-17, step-4338 ---
2023-07-05 18:34:38 loss: 0.0439
2023-07-05 18:34:46 --- Eval epoch-17, step-4338 ---
2023-07-05 18:34:46 accuracy: 0.8619
2023-07-05 18:34:46 pr_auc: 0.0638
2023-07-05 18:34:46 roc_auc: 0.4851
2023-07-05 18:34:46 f1: 0.0541
2023-07-05 18:34:46 loss: 0.7215
2023-07-05 18:34:46 
2023-07-05 18:38:17 --- Train epoch-18, step-4579 ---
2023-07-05 18:38:17 loss: 0.0366
2023-07-05 18:38:26 --- Eval epoch-18, step-4579 ---
2023-07-05 18:38:26 accuracy: 0.9043
2023-07-05 18:38:26 pr_auc: 0.0669
2023-07-05 18:38:26 roc_auc: 0.4956
2023-07-05 18:38:26 f1: 0.0583
2023-07-05 18:38:26 loss: 0.8325
2023-07-05 18:38:26 
2023-07-05 18:41:59 --- Train epoch-19, step-4820 ---
2023-07-05 18:41:59 loss: 0.0341
2023-07-05 18:42:07 --- Eval epoch-19, step-4820 ---
2023-07-05 18:42:07 accuracy: 0.8955
2023-07-05 18:42:07 pr_auc: 0.0698
2023-07-05 18:42:07 roc_auc: 0.4914
2023-07-05 18:42:07 f1: 0.0536
2023-07-05 18:42:07 loss: 0.8752
2023-07-05 18:42:07 Loaded best model
2023-07-05 18:42:15 ConCare(
  (embeddings): ModuleDict(
    (conditions): Embedding(6664, 128, padding_idx=0)
    (procedures): Embedding(1980, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (concare): ModuleDict(
    (conditions): ConCareLayer(
      (PositionalEncoding): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (GRUs): ModuleList(
        (0): GRU(1, 128, batch_first=True)
        (1): GRU(1, 128, batch_first=True)
        (2): GRU(1, 128, batch_first=True)
        (3): GRU(1, 128, batch_first=True)
        (4): GRU(1, 128, batch_first=True)
        (5): GRU(1, 128, batch_first=True)
        (6): GRU(1, 128, batch_first=True)
        (7): GRU(1, 128, batch_first=True)
        (8): GRU(1, 128, batch_first=True)
        (9): GRU(1, 128, batch_first=True)
        (10): GRU(1, 128, batch_first=True)
        (11): GRU(1, 128, batch_first=True)
        (12): GRU(1, 128, batch_first=True)
        (13): GRU(1, 128, batch_first=True)
        (14): GRU(1, 128, batch_first=True)
        (15): GRU(1, 128, batch_first=True)
        (16): GRU(1, 128, batch_first=True)
        (17): GRU(1, 128, batch_first=True)
        (18): GRU(1, 128, batch_first=True)
        (19): GRU(1, 128, batch_first=True)
        (20): GRU(1, 128, batch_first=True)
        (21): GRU(1, 128, batch_first=True)
        (22): GRU(1, 128, batch_first=True)
        (23): GRU(1, 128, batch_first=True)
        (24): GRU(1, 128, batch_first=True)
        (25): GRU(1, 128, batch_first=True)
        (26): GRU(1, 128, batch_first=True)
        (27): GRU(1, 128, batch_first=True)
        (28): GRU(1, 128, batch_first=True)
        (29): GRU(1, 128, batch_first=True)
        (30): GRU(1, 128, batch_first=True)
        (31): GRU(1, 128, batch_first=True)
        (32): GRU(1, 128, batch_first=True)
        (33): GRU(1, 128, batch_first=True)
        (34): GRU(1, 128, batch_first=True)
        (35): GRU(1, 128, batch_first=True)
        (36): GRU(1, 128, batch_first=True)
        (37): GRU(1, 128, batch_first=True)
        (38): GRU(1, 128, batch_first=True)
        (39): GRU(1, 128, batch_first=True)
        (40): GRU(1, 128, batch_first=True)
        (41): GRU(1, 128, batch_first=True)
        (42): GRU(1, 128, batch_first=True)
        (43): GRU(1, 128, batch_first=True)
        (44): GRU(1, 128, batch_first=True)
        (45): GRU(1, 128, batch_first=True)
        (46): GRU(1, 128, batch_first=True)
        (47): GRU(1, 128, batch_first=True)
        (48): GRU(1, 128, batch_first=True)
        (49): GRU(1, 128, batch_first=True)
        (50): GRU(1, 128, batch_first=True)
        (51): GRU(1, 128, batch_first=True)
        (52): GRU(1, 128, batch_first=True)
        (53): GRU(1, 128, batch_first=True)
        (54): GRU(1, 128, batch_first=True)
        (55): GRU(1, 128, batch_first=True)
        (56): GRU(1, 128, batch_first=True)
        (57): GRU(1, 128, batch_first=True)
        (58): GRU(1, 128, batch_first=True)
        (59): GRU(1, 128, batch_first=True)
        (60): GRU(1, 128, batch_first=True)
        (61): GRU(1, 128, batch_first=True)
        (62): GRU(1, 128, batch_first=True)
        (63): GRU(1, 128, batch_first=True)
        (64): GRU(1, 128, batch_first=True)
        (65): GRU(1, 128, batch_first=True)
        (66): GRU(1, 128, batch_first=True)
        (67): GRU(1, 128, batch_first=True)
        (68): GRU(1, 128, batch_first=True)
        (69): GRU(1, 128, batch_first=True)
        (70): GRU(1, 128, batch_first=True)
        (71): GRU(1, 128, batch_first=True)
        (72): GRU(1, 128, batch_first=True)
        (73): GRU(1, 128, batch_first=True)
        (74): GRU(1, 128, batch_first=True)
        (75): GRU(1, 128, batch_first=True)
        (76): GRU(1, 128, batch_first=True)
        (77): GRU(1, 128, batch_first=True)
        (78): GRU(1, 128, batch_first=True)
        (79): GRU(1, 128, batch_first=True)
        (80): GRU(1, 128, batch_first=True)
        (81): GRU(1, 128, batch_first=True)
        (82): GRU(1, 128, batch_first=True)
        (83): GRU(1, 128, batch_first=True)
        (84): GRU(1, 128, batch_first=True)
        (85): GRU(1, 128, batch_first=True)
        (86): GRU(1, 128, batch_first=True)
        (87): GRU(1, 128, batch_first=True)
        (88): GRU(1, 128, batch_first=True)
        (89): GRU(1, 128, batch_first=True)
        (90): GRU(1, 128, batch_first=True)
        (91): GRU(1, 128, batch_first=True)
        (92): GRU(1, 128, batch_first=True)
        (93): GRU(1, 128, batch_first=True)
        (94): GRU(1, 128, batch_first=True)
        (95): GRU(1, 128, batch_first=True)
        (96): GRU(1, 128, batch_first=True)
        (97): GRU(1, 128, batch_first=True)
        (98): GRU(1, 128, batch_first=True)
        (99): GRU(1, 128, batch_first=True)
        (100): GRU(1, 128, batch_first=True)
        (101): GRU(1, 128, batch_first=True)
        (102): GRU(1, 128, batch_first=True)
        (103): GRU(1, 128, batch_first=True)
        (104): GRU(1, 128, batch_first=True)
        (105): GRU(1, 128, batch_first=True)
        (106): GRU(1, 128, batch_first=True)
        (107): GRU(1, 128, batch_first=True)
        (108): GRU(1, 128, batch_first=True)
        (109): GRU(1, 128, batch_first=True)
        (110): GRU(1, 128, batch_first=True)
        (111): GRU(1, 128, batch_first=True)
        (112): GRU(1, 128, batch_first=True)
        (113): GRU(1, 128, batch_first=True)
        (114): GRU(1, 128, batch_first=True)
        (115): GRU(1, 128, batch_first=True)
        (116): GRU(1, 128, batch_first=True)
        (117): GRU(1, 128, batch_first=True)
        (118): GRU(1, 128, batch_first=True)
        (119): GRU(1, 128, batch_first=True)
        (120): GRU(1, 128, batch_first=True)
        (121): GRU(1, 128, batch_first=True)
        (122): GRU(1, 128, batch_first=True)
        (123): GRU(1, 128, batch_first=True)
        (124): GRU(1, 128, batch_first=True)
        (125): GRU(1, 128, batch_first=True)
        (126): GRU(1, 128, batch_first=True)
        (127): GRU(1, 128, batch_first=True)
      )
      (LastStepAttentions): ModuleList(
        (0): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (1): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (2): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (3): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (4): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (5): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (6): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (7): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (8): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (9): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (10): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (11): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (12): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (13): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (14): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (15): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (16): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (17): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (18): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (19): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (20): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (21): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (22): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (23): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (24): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (25): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (26): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (27): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (28): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (29): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (30): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (31): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (32): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (33): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (34): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (35): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (36): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (37): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (38): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (39): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (40): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (41): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (42): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (43): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (44): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (45): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (46): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (47): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (48): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (49): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (50): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (51): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (52): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (53): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (54): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (55): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (56): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (57): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (58): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (59): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (60): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (61): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (62): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (63): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (64): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (65): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (66): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (67): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (68): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (69): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (70): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (71): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (72): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (73): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (74): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (75): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (76): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (77): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (78): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (79): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (80): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (81): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (82): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (83): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (84): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (85): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (86): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (87): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (88): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (89): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (90): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (91): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (92): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (93): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (94): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (95): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (96): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (97): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (98): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (99): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (100): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (101): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (102): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (103): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (104): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (105): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (106): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (107): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (108): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (109): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (110): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (111): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (112): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (113): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (114): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (115): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (116): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (117): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (118): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (119): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (120): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (121): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (122): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (123): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (124): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (125): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (126): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (127): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
      )
      (FinalAttentionQKV): FinalAttentionQKV(
        (W_q): Linear(in_features=128, out_features=128, bias=True)
        (W_k): Linear(in_features=128, out_features=128, bias=True)
        (W_v): Linear(in_features=128, out_features=128, bias=True)
        (W_out): Linear(in_features=128, out_features=1, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (tanh): Tanh()
        (softmax): Softmax(dim=1)
        (sigmoid): Sigmoid()
      )
      (MultiHeadedAttention): MultiHeadedAttention(
        (linears): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Linear(in_features=128, out_features=128, bias=True)
          (2): Linear(in_features=128, out_features=128, bias=True)
        )
        (final_linear): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (SublayerConnection): SublayerConnection(
        (norm): LayerNorm()
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (PositionwiseFeedForward): PositionwiseFeedForward(
        (w_1): Linear(in_features=128, out_features=64, bias=True)
        (w_2): Linear(in_features=64, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.5, inplace=False)
      (tanh): Tanh()
      (softmax): Softmax(dim=None)
      (sigmoid): Sigmoid()
      (relu): ReLU()
    )
    (procedures): ConCareLayer(
      (PositionalEncoding): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (GRUs): ModuleList(
        (0): GRU(1, 128, batch_first=True)
        (1): GRU(1, 128, batch_first=True)
        (2): GRU(1, 128, batch_first=True)
        (3): GRU(1, 128, batch_first=True)
        (4): GRU(1, 128, batch_first=True)
        (5): GRU(1, 128, batch_first=True)
        (6): GRU(1, 128, batch_first=True)
        (7): GRU(1, 128, batch_first=True)
        (8): GRU(1, 128, batch_first=True)
        (9): GRU(1, 128, batch_first=True)
        (10): GRU(1, 128, batch_first=True)
        (11): GRU(1, 128, batch_first=True)
        (12): GRU(1, 128, batch_first=True)
        (13): GRU(1, 128, batch_first=True)
        (14): GRU(1, 128, batch_first=True)
        (15): GRU(1, 128, batch_first=True)
        (16): GRU(1, 128, batch_first=True)
        (17): GRU(1, 128, batch_first=True)
        (18): GRU(1, 128, batch_first=True)
        (19): GRU(1, 128, batch_first=True)
        (20): GRU(1, 128, batch_first=True)
        (21): GRU(1, 128, batch_first=True)
        (22): GRU(1, 128, batch_first=True)
        (23): GRU(1, 128, batch_first=True)
        (24): GRU(1, 128, batch_first=True)
        (25): GRU(1, 128, batch_first=True)
        (26): GRU(1, 128, batch_first=True)
        (27): GRU(1, 128, batch_first=True)
        (28): GRU(1, 128, batch_first=True)
        (29): GRU(1, 128, batch_first=True)
        (30): GRU(1, 128, batch_first=True)
        (31): GRU(1, 128, batch_first=True)
        (32): GRU(1, 128, batch_first=True)
        (33): GRU(1, 128, batch_first=True)
        (34): GRU(1, 128, batch_first=True)
        (35): GRU(1, 128, batch_first=True)
        (36): GRU(1, 128, batch_first=True)
        (37): GRU(1, 128, batch_first=True)
        (38): GRU(1, 128, batch_first=True)
        (39): GRU(1, 128, batch_first=True)
        (40): GRU(1, 128, batch_first=True)
        (41): GRU(1, 128, batch_first=True)
        (42): GRU(1, 128, batch_first=True)
        (43): GRU(1, 128, batch_first=True)
        (44): GRU(1, 128, batch_first=True)
        (45): GRU(1, 128, batch_first=True)
        (46): GRU(1, 128, batch_first=True)
        (47): GRU(1, 128, batch_first=True)
        (48): GRU(1, 128, batch_first=True)
        (49): GRU(1, 128, batch_first=True)
        (50): GRU(1, 128, batch_first=True)
        (51): GRU(1, 128, batch_first=True)
        (52): GRU(1, 128, batch_first=True)
        (53): GRU(1, 128, batch_first=True)
        (54): GRU(1, 128, batch_first=True)
        (55): GRU(1, 128, batch_first=True)
        (56): GRU(1, 128, batch_first=True)
        (57): GRU(1, 128, batch_first=True)
        (58): GRU(1, 128, batch_first=True)
        (59): GRU(1, 128, batch_first=True)
        (60): GRU(1, 128, batch_first=True)
        (61): GRU(1, 128, batch_first=True)
        (62): GRU(1, 128, batch_first=True)
        (63): GRU(1, 128, batch_first=True)
        (64): GRU(1, 128, batch_first=True)
        (65): GRU(1, 128, batch_first=True)
        (66): GRU(1, 128, batch_first=True)
        (67): GRU(1, 128, batch_first=True)
        (68): GRU(1, 128, batch_first=True)
        (69): GRU(1, 128, batch_first=True)
        (70): GRU(1, 128, batch_first=True)
        (71): GRU(1, 128, batch_first=True)
        (72): GRU(1, 128, batch_first=True)
        (73): GRU(1, 128, batch_first=True)
        (74): GRU(1, 128, batch_first=True)
        (75): GRU(1, 128, batch_first=True)
        (76): GRU(1, 128, batch_first=True)
        (77): GRU(1, 128, batch_first=True)
        (78): GRU(1, 128, batch_first=True)
        (79): GRU(1, 128, batch_first=True)
        (80): GRU(1, 128, batch_first=True)
        (81): GRU(1, 128, batch_first=True)
        (82): GRU(1, 128, batch_first=True)
        (83): GRU(1, 128, batch_first=True)
        (84): GRU(1, 128, batch_first=True)
        (85): GRU(1, 128, batch_first=True)
        (86): GRU(1, 128, batch_first=True)
        (87): GRU(1, 128, batch_first=True)
        (88): GRU(1, 128, batch_first=True)
        (89): GRU(1, 128, batch_first=True)
        (90): GRU(1, 128, batch_first=True)
        (91): GRU(1, 128, batch_first=True)
        (92): GRU(1, 128, batch_first=True)
        (93): GRU(1, 128, batch_first=True)
        (94): GRU(1, 128, batch_first=True)
        (95): GRU(1, 128, batch_first=True)
        (96): GRU(1, 128, batch_first=True)
        (97): GRU(1, 128, batch_first=True)
        (98): GRU(1, 128, batch_first=True)
        (99): GRU(1, 128, batch_first=True)
        (100): GRU(1, 128, batch_first=True)
        (101): GRU(1, 128, batch_first=True)
        (102): GRU(1, 128, batch_first=True)
        (103): GRU(1, 128, batch_first=True)
        (104): GRU(1, 128, batch_first=True)
        (105): GRU(1, 128, batch_first=True)
        (106): GRU(1, 128, batch_first=True)
        (107): GRU(1, 128, batch_first=True)
        (108): GRU(1, 128, batch_first=True)
        (109): GRU(1, 128, batch_first=True)
        (110): GRU(1, 128, batch_first=True)
        (111): GRU(1, 128, batch_first=True)
        (112): GRU(1, 128, batch_first=True)
        (113): GRU(1, 128, batch_first=True)
        (114): GRU(1, 128, batch_first=True)
        (115): GRU(1, 128, batch_first=True)
        (116): GRU(1, 128, batch_first=True)
        (117): GRU(1, 128, batch_first=True)
        (118): GRU(1, 128, batch_first=True)
        (119): GRU(1, 128, batch_first=True)
        (120): GRU(1, 128, batch_first=True)
        (121): GRU(1, 128, batch_first=True)
        (122): GRU(1, 128, batch_first=True)
        (123): GRU(1, 128, batch_first=True)
        (124): GRU(1, 128, batch_first=True)
        (125): GRU(1, 128, batch_first=True)
        (126): GRU(1, 128, batch_first=True)
        (127): GRU(1, 128, batch_first=True)
      )
      (LastStepAttentions): ModuleList(
        (0): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (1): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (2): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (3): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (4): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (5): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (6): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (7): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (8): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (9): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (10): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (11): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (12): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (13): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (14): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (15): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (16): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (17): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (18): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (19): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (20): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (21): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (22): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (23): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (24): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (25): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (26): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (27): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (28): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (29): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (30): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (31): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (32): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (33): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (34): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (35): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (36): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (37): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (38): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (39): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (40): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (41): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (42): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (43): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (44): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (45): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (46): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (47): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (48): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (49): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (50): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (51): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (52): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (53): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (54): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (55): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (56): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (57): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (58): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (59): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (60): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (61): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (62): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (63): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (64): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (65): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (66): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (67): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (68): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (69): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (70): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (71): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (72): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (73): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (74): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (75): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (76): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (77): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (78): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (79): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (80): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (81): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (82): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (83): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (84): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (85): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (86): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (87): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (88): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (89): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (90): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (91): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (92): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (93): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (94): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (95): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (96): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (97): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (98): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (99): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (100): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (101): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (102): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (103): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (104): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (105): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (106): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (107): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (108): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (109): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (110): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (111): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (112): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (113): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (114): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (115): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (116): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (117): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (118): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (119): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (120): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (121): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (122): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (123): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (124): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (125): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (126): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
        (127): SingleAttention(
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
          (relu): ReLU()
        )
      )
      (FinalAttentionQKV): FinalAttentionQKV(
        (W_q): Linear(in_features=128, out_features=128, bias=True)
        (W_k): Linear(in_features=128, out_features=128, bias=True)
        (W_v): Linear(in_features=128, out_features=128, bias=True)
        (W_out): Linear(in_features=128, out_features=1, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (tanh): Tanh()
        (softmax): Softmax(dim=1)
        (sigmoid): Sigmoid()
      )
      (MultiHeadedAttention): MultiHeadedAttention(
        (linears): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Linear(in_features=128, out_features=128, bias=True)
          (2): Linear(in_features=128, out_features=128, bias=True)
        )
        (final_linear): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (SublayerConnection): SublayerConnection(
        (norm): LayerNorm()
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (PositionwiseFeedForward): PositionwiseFeedForward(
        (w_1): Linear(in_features=128, out_features=64, bias=True)
        (w_2): Linear(in_features=64, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.5, inplace=False)
      (tanh): Tanh()
      (softmax): Softmax(dim=None)
      (sigmoid): Sigmoid()
      (relu): ReLU()
    )
  )
  (fc): Linear(in_features=256, out_features=10, bias=True)
)
2023-07-05 18:42:15 Metrics: ['accuracy', 'f1_macro', 'roc_auc_weighted_ovo']
2023-07-05 18:42:15 Device: cuda
2023-07-05 18:42:15 
2023-07-05 18:42:15 Training:
2023-07-05 18:42:15 Batch size: 32
2023-07-05 18:42:15 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-05 18:42:15 Optimizer params: {'lr': 0.001}
2023-07-05 18:42:15 Weight decay: 0.0
2023-07-05 18:42:15 Max grad norm: None
2023-07-05 18:42:15 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd720665210>
2023-07-05 18:42:15 Monitor: accuracy
2023-07-05 18:42:15 Monitor criterion: max
2023-07-05 18:42:15 Epochs: 20
2023-07-05 18:42:15 
2023-07-05 18:58:19 --- Train epoch-0, step-1111 ---
2023-07-05 18:58:19 loss: 1.7962
2023-07-05 18:58:52 --- Eval epoch-0, step-1111 ---
2023-07-05 18:58:52 accuracy: 0.3936
2023-07-05 18:58:52 f1_macro: 0.1952
2023-07-05 18:58:52 roc_auc_weighted_ovo: 0.7703
2023-07-05 18:58:52 loss: 1.6814
2023-07-05 18:58:52 New best accuracy score (0.3936) at epoch-0, step-1111
2023-07-05 18:58:53 
2023-07-05 19:14:56 --- Train epoch-1, step-2222 ---
2023-07-05 19:14:56 loss: 1.6576
2023-07-05 19:15:31 --- Eval epoch-1, step-2222 ---
2023-07-05 19:15:31 accuracy: 0.3943
2023-07-05 19:15:31 f1_macro: 0.1998
2023-07-05 19:15:31 roc_auc_weighted_ovo: 0.7812
2023-07-05 19:15:31 loss: 1.6704
2023-07-05 19:15:31 New best accuracy score (0.3943) at epoch-1, step-2222
2023-07-05 19:15:32 
2023-07-05 19:32:01 --- Train epoch-2, step-3333 ---
2023-07-05 19:32:01 loss: 1.6108
2023-07-05 19:32:35 --- Eval epoch-2, step-3333 ---
2023-07-05 19:32:35 accuracy: 0.3831
2023-07-05 19:32:35 f1_macro: 0.2326
2023-07-05 19:32:35 roc_auc_weighted_ovo: 0.7848
2023-07-05 19:32:35 loss: 1.6882
2023-07-05 19:32:35 
2023-07-05 19:48:34 --- Train epoch-3, step-4444 ---
2023-07-05 19:48:34 loss: 1.5758
2023-07-05 19:49:11 --- Eval epoch-3, step-4444 ---
2023-07-05 19:49:11 accuracy: 0.3758
2023-07-05 19:49:11 f1_macro: 0.2207
2023-07-05 19:49:11 roc_auc_weighted_ovo: 0.7823
2023-07-05 19:49:11 loss: 1.7054
2023-07-05 19:49:11 
2023-07-05 20:05:19 --- Train epoch-4, step-5555 ---
2023-07-05 20:05:19 loss: 1.5500
2023-07-05 20:05:54 --- Eval epoch-4, step-5555 ---
2023-07-05 20:05:54 accuracy: 0.4121
2023-07-05 20:05:54 f1_macro: 0.2523
2023-07-05 20:05:54 roc_auc_weighted_ovo: 0.7935
2023-07-05 20:05:54 loss: 1.6113
2023-07-05 20:05:54 New best accuracy score (0.4121) at epoch-4, step-5555
2023-07-05 20:05:55 
2023-07-05 20:22:00 --- Train epoch-5, step-6666 ---
2023-07-05 20:22:00 loss: 1.5195
2023-07-05 20:22:37 --- Eval epoch-5, step-6666 ---
2023-07-05 20:22:37 accuracy: 0.3998
2023-07-05 20:22:37 f1_macro: 0.2425
2023-07-05 20:22:37 roc_auc_weighted_ovo: 0.7913
2023-07-05 20:22:37 loss: 1.6419
2023-07-05 20:22:37 
2023-07-05 20:38:43 --- Train epoch-6, step-7777 ---
2023-07-05 20:38:43 loss: 1.4921
2023-07-05 20:39:21 --- Eval epoch-6, step-7777 ---
2023-07-05 20:39:21 accuracy: 0.4081
2023-07-05 20:39:21 f1_macro: 0.2376
2023-07-05 20:39:21 roc_auc_weighted_ovo: 0.7886
2023-07-05 20:39:21 loss: 1.6466
2023-07-05 20:39:21 
2023-07-05 20:55:25 --- Train epoch-7, step-8888 ---
2023-07-05 20:55:25 loss: 1.4671
2023-07-05 20:55:58 --- Eval epoch-7, step-8888 ---
2023-07-05 20:55:58 accuracy: 0.4014
2023-07-05 20:55:58 f1_macro: 0.2405
2023-07-05 20:55:58 roc_auc_weighted_ovo: 0.7831
2023-07-05 20:55:58 loss: 1.6855
2023-07-05 20:55:58 
2023-07-05 21:13:26 --- Train epoch-8, step-9999 ---
2023-07-05 21:13:26 loss: 1.4404
2023-07-05 21:14:04 --- Eval epoch-8, step-9999 ---
2023-07-05 21:14:04 accuracy: 0.4043
2023-07-05 21:14:04 f1_macro: 0.2457
2023-07-05 21:14:04 roc_auc_weighted_ovo: 0.7910
2023-07-05 21:14:04 loss: 1.6569
2023-07-05 21:14:04 
2023-07-05 21:31:04 --- Train epoch-9, step-11110 ---
2023-07-05 21:31:04 loss: 1.4084
2023-07-05 21:31:46 --- Eval epoch-9, step-11110 ---
2023-07-05 21:31:46 accuracy: 0.4054
2023-07-05 21:31:46 f1_macro: 0.2606
2023-07-05 21:31:46 roc_auc_weighted_ovo: 0.7906
2023-07-05 21:31:46 loss: 1.6915
2023-07-05 21:31:46 
2023-07-05 21:49:07 --- Train epoch-10, step-12221 ---
2023-07-05 21:49:07 loss: 1.3716
2023-07-05 21:49:48 --- Eval epoch-10, step-12221 ---
2023-07-05 21:49:48 accuracy: 0.3858
2023-07-05 21:49:48 f1_macro: 0.2530
2023-07-05 21:49:48 roc_auc_weighted_ovo: 0.7888
2023-07-05 21:49:48 loss: 1.7548
2023-07-05 21:49:48 
2023-07-05 22:06:44 --- Train epoch-11, step-13332 ---
2023-07-05 22:06:44 loss: 1.3443
2023-07-05 22:07:22 --- Eval epoch-11, step-13332 ---
2023-07-05 22:07:22 accuracy: 0.3780
2023-07-05 22:07:22 f1_macro: 0.2613
2023-07-05 22:07:22 roc_auc_weighted_ovo: 0.7879
2023-07-05 22:07:22 loss: 1.7884
2023-07-05 22:07:22 
2023-07-05 22:24:37 --- Train epoch-12, step-14443 ---
2023-07-05 22:24:37 loss: 1.3079
2023-07-05 22:25:18 --- Eval epoch-12, step-14443 ---
2023-07-05 22:25:18 accuracy: 0.3820
2023-07-05 22:25:18 f1_macro: 0.2520
2023-07-05 22:25:18 roc_auc_weighted_ovo: 0.7848
2023-07-05 22:25:18 loss: 1.8025
2023-07-05 22:25:18 
2023-07-05 22:42:34 --- Train epoch-13, step-15554 ---
2023-07-05 22:42:34 loss: 1.2761
2023-07-05 22:43:14 --- Eval epoch-13, step-15554 ---
2023-07-05 22:43:14 accuracy: 0.4072
2023-07-05 22:43:14 f1_macro: 0.2625
2023-07-05 22:43:14 roc_auc_weighted_ovo: 0.7786
2023-07-05 22:43:14 loss: 1.8291
2023-07-05 22:43:14 
2023-07-05 23:00:58 --- Train epoch-14, step-16665 ---
2023-07-05 23:00:58 loss: 1.2311
2023-07-05 23:01:36 --- Eval epoch-14, step-16665 ---
2023-07-05 23:01:36 accuracy: 0.3836
2023-07-05 23:01:36 f1_macro: 0.2617
2023-07-05 23:01:36 roc_auc_weighted_ovo: 0.7772
2023-07-05 23:01:36 loss: 1.9367
2023-07-05 23:01:36 
2023-07-05 23:18:44 --- Train epoch-15, step-17776 ---
2023-07-05 23:18:44 loss: 1.1969
2023-07-05 23:19:24 --- Eval epoch-15, step-17776 ---
2023-07-05 23:19:24 accuracy: 0.3849
2023-07-05 23:19:24 f1_macro: 0.2540
2023-07-05 23:19:24 roc_auc_weighted_ovo: 0.7748
2023-07-05 23:19:24 loss: 1.9431
2023-07-05 23:19:24 
2023-07-05 23:36:29 --- Train epoch-16, step-18887 ---
2023-07-05 23:36:29 loss: 1.1612
2023-07-05 23:37:06 --- Eval epoch-16, step-18887 ---
2023-07-05 23:37:06 accuracy: 0.3780
2023-07-05 23:37:06 f1_macro: 0.2670
2023-07-05 23:37:06 roc_auc_weighted_ovo: 0.7736
2023-07-05 23:37:06 loss: 2.0380
2023-07-05 23:37:06 
2023-07-05 23:54:12 --- Train epoch-17, step-19998 ---
2023-07-05 23:54:12 loss: 1.1228
2023-07-05 23:54:47 --- Eval epoch-17, step-19998 ---
2023-07-05 23:54:47 accuracy: 0.3575
2023-07-05 23:54:47 f1_macro: 0.2647
2023-07-05 23:54:47 roc_auc_weighted_ovo: 0.7702
2023-07-05 23:54:47 loss: 2.2593
2023-07-05 23:54:47 
2023-07-06 00:11:52 --- Train epoch-18, step-21109 ---
2023-07-06 00:11:52 loss: 1.0882
2023-07-06 00:12:30 --- Eval epoch-18, step-21109 ---
2023-07-06 00:12:30 accuracy: 0.3769
2023-07-06 00:12:30 f1_macro: 0.2566
2023-07-06 00:12:30 roc_auc_weighted_ovo: 0.7691
2023-07-06 00:12:30 loss: 2.2713
2023-07-06 00:12:30 
2023-07-06 00:29:20 --- Train epoch-19, step-22220 ---
2023-07-06 00:29:20 loss: 1.0447
2023-07-06 00:29:55 --- Eval epoch-19, step-22220 ---
2023-07-06 00:29:55 accuracy: 0.3626
2023-07-06 00:29:55 f1_macro: 0.2588
2023-07-06 00:29:55 roc_auc_weighted_ovo: 0.7628
2023-07-06 00:29:55 loss: 2.5095
2023-07-06 00:29:55 Loaded best model
2023-07-06 00:29:58 GRASP(
  (embeddings): ModuleDict(
    (conditions): Embedding(4031, 128, padding_idx=0)
    (procedures): Embedding(1276, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (grasp): ModuleDict(
    (conditions): GRASPLayer(
      (backbone): ConCareLayer(
        (PositionalEncoding): PositionalEncoding(
          (dropout): Dropout(p=0, inplace=False)
        )
        (GRUs): ModuleList(
          (0): GRU(1, 128, batch_first=True)
          (1): GRU(1, 128, batch_first=True)
          (2): GRU(1, 128, batch_first=True)
          (3): GRU(1, 128, batch_first=True)
          (4): GRU(1, 128, batch_first=True)
          (5): GRU(1, 128, batch_first=True)
          (6): GRU(1, 128, batch_first=True)
          (7): GRU(1, 128, batch_first=True)
          (8): GRU(1, 128, batch_first=True)
          (9): GRU(1, 128, batch_first=True)
          (10): GRU(1, 128, batch_first=True)
          (11): GRU(1, 128, batch_first=True)
          (12): GRU(1, 128, batch_first=True)
          (13): GRU(1, 128, batch_first=True)
          (14): GRU(1, 128, batch_first=True)
          (15): GRU(1, 128, batch_first=True)
          (16): GRU(1, 128, batch_first=True)
          (17): GRU(1, 128, batch_first=True)
          (18): GRU(1, 128, batch_first=True)
          (19): GRU(1, 128, batch_first=True)
          (20): GRU(1, 128, batch_first=True)
          (21): GRU(1, 128, batch_first=True)
          (22): GRU(1, 128, batch_first=True)
          (23): GRU(1, 128, batch_first=True)
          (24): GRU(1, 128, batch_first=True)
          (25): GRU(1, 128, batch_first=True)
          (26): GRU(1, 128, batch_first=True)
          (27): GRU(1, 128, batch_first=True)
          (28): GRU(1, 128, batch_first=True)
          (29): GRU(1, 128, batch_first=True)
          (30): GRU(1, 128, batch_first=True)
          (31): GRU(1, 128, batch_first=True)
          (32): GRU(1, 128, batch_first=True)
          (33): GRU(1, 128, batch_first=True)
          (34): GRU(1, 128, batch_first=True)
          (35): GRU(1, 128, batch_first=True)
          (36): GRU(1, 128, batch_first=True)
          (37): GRU(1, 128, batch_first=True)
          (38): GRU(1, 128, batch_first=True)
          (39): GRU(1, 128, batch_first=True)
          (40): GRU(1, 128, batch_first=True)
          (41): GRU(1, 128, batch_first=True)
          (42): GRU(1, 128, batch_first=True)
          (43): GRU(1, 128, batch_first=True)
          (44): GRU(1, 128, batch_first=True)
          (45): GRU(1, 128, batch_first=True)
          (46): GRU(1, 128, batch_first=True)
          (47): GRU(1, 128, batch_first=True)
          (48): GRU(1, 128, batch_first=True)
          (49): GRU(1, 128, batch_first=True)
          (50): GRU(1, 128, batch_first=True)
          (51): GRU(1, 128, batch_first=True)
          (52): GRU(1, 128, batch_first=True)
          (53): GRU(1, 128, batch_first=True)
          (54): GRU(1, 128, batch_first=True)
          (55): GRU(1, 128, batch_first=True)
          (56): GRU(1, 128, batch_first=True)
          (57): GRU(1, 128, batch_first=True)
          (58): GRU(1, 128, batch_first=True)
          (59): GRU(1, 128, batch_first=True)
          (60): GRU(1, 128, batch_first=True)
          (61): GRU(1, 128, batch_first=True)
          (62): GRU(1, 128, batch_first=True)
          (63): GRU(1, 128, batch_first=True)
          (64): GRU(1, 128, batch_first=True)
          (65): GRU(1, 128, batch_first=True)
          (66): GRU(1, 128, batch_first=True)
          (67): GRU(1, 128, batch_first=True)
          (68): GRU(1, 128, batch_first=True)
          (69): GRU(1, 128, batch_first=True)
          (70): GRU(1, 128, batch_first=True)
          (71): GRU(1, 128, batch_first=True)
          (72): GRU(1, 128, batch_first=True)
          (73): GRU(1, 128, batch_first=True)
          (74): GRU(1, 128, batch_first=True)
          (75): GRU(1, 128, batch_first=True)
          (76): GRU(1, 128, batch_first=True)
          (77): GRU(1, 128, batch_first=True)
          (78): GRU(1, 128, batch_first=True)
          (79): GRU(1, 128, batch_first=True)
          (80): GRU(1, 128, batch_first=True)
          (81): GRU(1, 128, batch_first=True)
          (82): GRU(1, 128, batch_first=True)
          (83): GRU(1, 128, batch_first=True)
          (84): GRU(1, 128, batch_first=True)
          (85): GRU(1, 128, batch_first=True)
          (86): GRU(1, 128, batch_first=True)
          (87): GRU(1, 128, batch_first=True)
          (88): GRU(1, 128, batch_first=True)
          (89): GRU(1, 128, batch_first=True)
          (90): GRU(1, 128, batch_first=True)
          (91): GRU(1, 128, batch_first=True)
          (92): GRU(1, 128, batch_first=True)
          (93): GRU(1, 128, batch_first=True)
          (94): GRU(1, 128, batch_first=True)
          (95): GRU(1, 128, batch_first=True)
          (96): GRU(1, 128, batch_first=True)
          (97): GRU(1, 128, batch_first=True)
          (98): GRU(1, 128, batch_first=True)
          (99): GRU(1, 128, batch_first=True)
          (100): GRU(1, 128, batch_first=True)
          (101): GRU(1, 128, batch_first=True)
          (102): GRU(1, 128, batch_first=True)
          (103): GRU(1, 128, batch_first=True)
          (104): GRU(1, 128, batch_first=True)
          (105): GRU(1, 128, batch_first=True)
          (106): GRU(1, 128, batch_first=True)
          (107): GRU(1, 128, batch_first=True)
          (108): GRU(1, 128, batch_first=True)
          (109): GRU(1, 128, batch_first=True)
          (110): GRU(1, 128, batch_first=True)
          (111): GRU(1, 128, batch_first=True)
          (112): GRU(1, 128, batch_first=True)
          (113): GRU(1, 128, batch_first=True)
          (114): GRU(1, 128, batch_first=True)
          (115): GRU(1, 128, batch_first=True)
          (116): GRU(1, 128, batch_first=True)
          (117): GRU(1, 128, batch_first=True)
          (118): GRU(1, 128, batch_first=True)
          (119): GRU(1, 128, batch_first=True)
          (120): GRU(1, 128, batch_first=True)
          (121): GRU(1, 128, batch_first=True)
          (122): GRU(1, 128, batch_first=True)
          (123): GRU(1, 128, batch_first=True)
          (124): GRU(1, 128, batch_first=True)
          (125): GRU(1, 128, batch_first=True)
          (126): GRU(1, 128, batch_first=True)
          (127): GRU(1, 128, batch_first=True)
        )
        (LastStepAttentions): ModuleList(
          (0): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (1): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (2): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (3): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (4): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (5): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (6): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (7): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (8): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (9): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (10): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (11): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (12): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (13): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (14): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (15): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (16): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (17): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (18): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (19): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (20): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (21): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (22): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (23): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (24): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (25): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (26): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (27): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (28): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (29): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (30): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (31): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (32): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (33): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (34): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (35): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (36): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (37): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (38): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (39): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (40): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (41): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (42): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (43): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (44): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (45): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (46): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (47): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (48): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (49): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (50): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (51): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (52): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (53): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (54): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (55): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (56): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (57): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (58): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (59): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (60): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (61): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (62): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (63): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (64): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (65): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (66): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (67): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (68): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (69): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (70): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (71): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (72): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (73): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (74): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (75): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (76): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (77): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (78): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (79): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (80): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (81): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (82): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (83): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (84): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (85): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (86): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (87): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (88): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (89): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (90): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (91): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (92): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (93): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (94): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (95): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (96): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (97): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (98): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (99): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (100): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (101): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (102): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (103): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (104): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (105): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (106): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (107): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (108): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (109): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (110): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (111): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (112): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (113): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (114): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (115): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (116): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (117): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (118): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (119): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (120): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (121): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (122): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (123): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (124): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (125): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (126): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (127): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
        )
        (FinalAttentionQKV): FinalAttentionQKV(
          (W_q): Linear(in_features=128, out_features=128, bias=True)
          (W_k): Linear(in_features=128, out_features=128, bias=True)
          (W_v): Linear(in_features=128, out_features=128, bias=True)
          (W_out): Linear(in_features=128, out_features=1, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
        )
        (MultiHeadedAttention): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
            (2): Linear(in_features=128, out_features=128, bias=True)
          )
          (final_linear): Linear(in_features=128, out_features=128, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (SublayerConnection): SublayerConnection(
          (norm): LayerNorm()
          (dropout): Dropout(p=0, inplace=False)
        )
        (PositionwiseFeedForward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0, inplace=False)
        (tanh): Tanh()
        (softmax): Softmax(dim=None)
        (sigmoid): Sigmoid()
        (relu): ReLU()
      )
      (relu): ReLU()
      (tanh): Tanh()
      (sigmoid): Sigmoid()
      (dropout): Dropout(p=0.5, inplace=False)
      (weight1): Linear(in_features=128, out_features=1, bias=True)
      (weight2): Linear(in_features=128, out_features=1, bias=True)
      (GCN): GraphConvolution()
      (GCN_2): GraphConvolution()
      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (procedures): GRASPLayer(
      (backbone): ConCareLayer(
        (PositionalEncoding): PositionalEncoding(
          (dropout): Dropout(p=0, inplace=False)
        )
        (GRUs): ModuleList(
          (0): GRU(1, 128, batch_first=True)
          (1): GRU(1, 128, batch_first=True)
          (2): GRU(1, 128, batch_first=True)
          (3): GRU(1, 128, batch_first=True)
          (4): GRU(1, 128, batch_first=True)
          (5): GRU(1, 128, batch_first=True)
          (6): GRU(1, 128, batch_first=True)
          (7): GRU(1, 128, batch_first=True)
          (8): GRU(1, 128, batch_first=True)
          (9): GRU(1, 128, batch_first=True)
          (10): GRU(1, 128, batch_first=True)
          (11): GRU(1, 128, batch_first=True)
          (12): GRU(1, 128, batch_first=True)
          (13): GRU(1, 128, batch_first=True)
          (14): GRU(1, 128, batch_first=True)
          (15): GRU(1, 128, batch_first=True)
          (16): GRU(1, 128, batch_first=True)
          (17): GRU(1, 128, batch_first=True)
          (18): GRU(1, 128, batch_first=True)
          (19): GRU(1, 128, batch_first=True)
          (20): GRU(1, 128, batch_first=True)
          (21): GRU(1, 128, batch_first=True)
          (22): GRU(1, 128, batch_first=True)
          (23): GRU(1, 128, batch_first=True)
          (24): GRU(1, 128, batch_first=True)
          (25): GRU(1, 128, batch_first=True)
          (26): GRU(1, 128, batch_first=True)
          (27): GRU(1, 128, batch_first=True)
          (28): GRU(1, 128, batch_first=True)
          (29): GRU(1, 128, batch_first=True)
          (30): GRU(1, 128, batch_first=True)
          (31): GRU(1, 128, batch_first=True)
          (32): GRU(1, 128, batch_first=True)
          (33): GRU(1, 128, batch_first=True)
          (34): GRU(1, 128, batch_first=True)
          (35): GRU(1, 128, batch_first=True)
          (36): GRU(1, 128, batch_first=True)
          (37): GRU(1, 128, batch_first=True)
          (38): GRU(1, 128, batch_first=True)
          (39): GRU(1, 128, batch_first=True)
          (40): GRU(1, 128, batch_first=True)
          (41): GRU(1, 128, batch_first=True)
          (42): GRU(1, 128, batch_first=True)
          (43): GRU(1, 128, batch_first=True)
          (44): GRU(1, 128, batch_first=True)
          (45): GRU(1, 128, batch_first=True)
          (46): GRU(1, 128, batch_first=True)
          (47): GRU(1, 128, batch_first=True)
          (48): GRU(1, 128, batch_first=True)
          (49): GRU(1, 128, batch_first=True)
          (50): GRU(1, 128, batch_first=True)
          (51): GRU(1, 128, batch_first=True)
          (52): GRU(1, 128, batch_first=True)
          (53): GRU(1, 128, batch_first=True)
          (54): GRU(1, 128, batch_first=True)
          (55): GRU(1, 128, batch_first=True)
          (56): GRU(1, 128, batch_first=True)
          (57): GRU(1, 128, batch_first=True)
          (58): GRU(1, 128, batch_first=True)
          (59): GRU(1, 128, batch_first=True)
          (60): GRU(1, 128, batch_first=True)
          (61): GRU(1, 128, batch_first=True)
          (62): GRU(1, 128, batch_first=True)
          (63): GRU(1, 128, batch_first=True)
          (64): GRU(1, 128, batch_first=True)
          (65): GRU(1, 128, batch_first=True)
          (66): GRU(1, 128, batch_first=True)
          (67): GRU(1, 128, batch_first=True)
          (68): GRU(1, 128, batch_first=True)
          (69): GRU(1, 128, batch_first=True)
          (70): GRU(1, 128, batch_first=True)
          (71): GRU(1, 128, batch_first=True)
          (72): GRU(1, 128, batch_first=True)
          (73): GRU(1, 128, batch_first=True)
          (74): GRU(1, 128, batch_first=True)
          (75): GRU(1, 128, batch_first=True)
          (76): GRU(1, 128, batch_first=True)
          (77): GRU(1, 128, batch_first=True)
          (78): GRU(1, 128, batch_first=True)
          (79): GRU(1, 128, batch_first=True)
          (80): GRU(1, 128, batch_first=True)
          (81): GRU(1, 128, batch_first=True)
          (82): GRU(1, 128, batch_first=True)
          (83): GRU(1, 128, batch_first=True)
          (84): GRU(1, 128, batch_first=True)
          (85): GRU(1, 128, batch_first=True)
          (86): GRU(1, 128, batch_first=True)
          (87): GRU(1, 128, batch_first=True)
          (88): GRU(1, 128, batch_first=True)
          (89): GRU(1, 128, batch_first=True)
          (90): GRU(1, 128, batch_first=True)
          (91): GRU(1, 128, batch_first=True)
          (92): GRU(1, 128, batch_first=True)
          (93): GRU(1, 128, batch_first=True)
          (94): GRU(1, 128, batch_first=True)
          (95): GRU(1, 128, batch_first=True)
          (96): GRU(1, 128, batch_first=True)
          (97): GRU(1, 128, batch_first=True)
          (98): GRU(1, 128, batch_first=True)
          (99): GRU(1, 128, batch_first=True)
          (100): GRU(1, 128, batch_first=True)
          (101): GRU(1, 128, batch_first=True)
          (102): GRU(1, 128, batch_first=True)
          (103): GRU(1, 128, batch_first=True)
          (104): GRU(1, 128, batch_first=True)
          (105): GRU(1, 128, batch_first=True)
          (106): GRU(1, 128, batch_first=True)
          (107): GRU(1, 128, batch_first=True)
          (108): GRU(1, 128, batch_first=True)
          (109): GRU(1, 128, batch_first=True)
          (110): GRU(1, 128, batch_first=True)
          (111): GRU(1, 128, batch_first=True)
          (112): GRU(1, 128, batch_first=True)
          (113): GRU(1, 128, batch_first=True)
          (114): GRU(1, 128, batch_first=True)
          (115): GRU(1, 128, batch_first=True)
          (116): GRU(1, 128, batch_first=True)
          (117): GRU(1, 128, batch_first=True)
          (118): GRU(1, 128, batch_first=True)
          (119): GRU(1, 128, batch_first=True)
          (120): GRU(1, 128, batch_first=True)
          (121): GRU(1, 128, batch_first=True)
          (122): GRU(1, 128, batch_first=True)
          (123): GRU(1, 128, batch_first=True)
          (124): GRU(1, 128, batch_first=True)
          (125): GRU(1, 128, batch_first=True)
          (126): GRU(1, 128, batch_first=True)
          (127): GRU(1, 128, batch_first=True)
        )
        (LastStepAttentions): ModuleList(
          (0): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (1): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (2): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (3): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (4): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (5): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (6): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (7): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (8): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (9): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (10): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (11): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (12): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (13): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (14): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (15): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (16): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (17): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (18): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (19): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (20): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (21): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (22): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (23): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (24): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (25): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (26): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (27): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (28): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (29): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (30): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (31): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (32): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (33): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (34): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (35): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (36): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (37): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (38): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (39): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (40): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (41): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (42): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (43): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (44): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (45): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (46): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (47): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (48): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (49): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (50): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (51): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (52): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (53): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (54): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (55): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (56): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (57): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (58): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (59): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (60): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (61): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (62): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (63): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (64): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (65): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (66): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (67): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (68): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (69): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (70): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (71): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (72): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (73): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (74): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (75): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (76): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (77): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (78): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (79): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (80): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (81): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (82): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (83): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (84): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (85): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (86): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (87): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (88): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (89): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (90): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (91): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (92): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (93): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (94): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (95): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (96): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (97): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (98): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (99): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (100): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (101): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (102): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (103): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (104): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (105): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (106): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (107): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (108): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (109): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (110): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (111): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (112): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (113): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (114): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (115): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (116): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (117): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (118): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (119): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (120): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (121): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (122): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (123): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (124): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (125): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (126): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (127): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
        )
        (FinalAttentionQKV): FinalAttentionQKV(
          (W_q): Linear(in_features=128, out_features=128, bias=True)
          (W_k): Linear(in_features=128, out_features=128, bias=True)
          (W_v): Linear(in_features=128, out_features=128, bias=True)
          (W_out): Linear(in_features=128, out_features=1, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
        )
        (MultiHeadedAttention): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
            (2): Linear(in_features=128, out_features=128, bias=True)
          )
          (final_linear): Linear(in_features=128, out_features=128, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (SublayerConnection): SublayerConnection(
          (norm): LayerNorm()
          (dropout): Dropout(p=0, inplace=False)
        )
        (PositionwiseFeedForward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0, inplace=False)
        (tanh): Tanh()
        (softmax): Softmax(dim=None)
        (sigmoid): Sigmoid()
        (relu): ReLU()
      )
      (relu): ReLU()
      (tanh): Tanh()
      (sigmoid): Sigmoid()
      (dropout): Dropout(p=0.5, inplace=False)
      (weight1): Linear(in_features=128, out_features=1, bias=True)
      (weight2): Linear(in_features=128, out_features=1, bias=True)
      (GCN): GraphConvolution()
      (GCN_2): GraphConvolution()
      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (fc): Linear(in_features=256, out_features=1, bias=True)
)
2023-07-06 00:29:59 Metrics: ['accuracy', 'pr_auc', 'roc_auc', 'f1']
2023-07-06 00:29:59 Device: cuda
2023-07-06 00:29:59 
2023-07-06 00:29:59 Training:
2023-07-06 00:29:59 Batch size: 32
2023-07-06 00:29:59 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-06 00:29:59 Optimizer params: {'lr': 0.001}
2023-07-06 00:29:59 Weight decay: 0.0
2023-07-06 00:29:59 Max grad norm: None
2023-07-06 00:29:59 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd711845110>
2023-07-06 00:29:59 Monitor: roc_auc
2023-07-06 00:29:59 Monitor criterion: max
2023-07-06 00:29:59 Epochs: 20
2023-07-06 00:29:59 
2023-07-06 00:33:11 --- Train epoch-0, step-243 ---
2023-07-06 00:33:11 loss: 0.6839
2023-07-06 00:33:20 --- Eval epoch-0, step-243 ---
2023-07-06 00:33:20 accuracy: 0.5845
2023-07-06 00:33:20 pr_auc: 0.6542
2023-07-06 00:33:20 roc_auc: 0.6170
2023-07-06 00:33:20 f1: 0.6650
2023-07-06 00:33:20 loss: 0.6708
2023-07-06 00:33:20 New best roc_auc score (0.6170) at epoch-0, step-243
2023-07-06 00:33:20 
2023-07-06 00:36:30 --- Train epoch-1, step-486 ---
2023-07-06 00:36:30 loss: 0.6595
2023-07-06 00:36:37 --- Eval epoch-1, step-486 ---
2023-07-06 00:36:37 accuracy: 0.5897
2023-07-06 00:36:37 pr_auc: 0.6647
2023-07-06 00:36:37 roc_auc: 0.6231
2023-07-06 00:36:37 f1: 0.6592
2023-07-06 00:36:37 loss: 0.6609
2023-07-06 00:36:37 New best roc_auc score (0.6231) at epoch-1, step-486
2023-07-06 00:36:38 
2023-07-06 00:39:50 --- Train epoch-2, step-729 ---
2023-07-06 00:39:50 loss: 0.6277
2023-07-06 00:39:58 --- Eval epoch-2, step-729 ---
2023-07-06 00:39:58 accuracy: 0.5959
2023-07-06 00:39:58 pr_auc: 0.6717
2023-07-06 00:39:58 roc_auc: 0.6339
2023-07-06 00:39:58 f1: 0.6942
2023-07-06 00:39:58 loss: 0.6650
2023-07-06 00:39:58 New best roc_auc score (0.6339) at epoch-2, step-729
2023-07-06 00:39:59 
2023-07-06 00:43:09 --- Train epoch-3, step-972 ---
2023-07-06 00:43:09 loss: 0.6018
2023-07-06 00:43:17 --- Eval epoch-3, step-972 ---
2023-07-06 00:43:17 accuracy: 0.6134
2023-07-06 00:43:17 pr_auc: 0.6715
2023-07-06 00:43:17 roc_auc: 0.6371
2023-07-06 00:43:17 f1: 0.6040
2023-07-06 00:43:17 loss: 0.6932
2023-07-06 00:43:17 New best roc_auc score (0.6371) at epoch-3, step-972
2023-07-06 00:43:17 
2023-07-06 00:46:35 --- Train epoch-4, step-1215 ---
2023-07-06 00:46:35 loss: 0.5730
2023-07-06 00:46:43 --- Eval epoch-4, step-1215 ---
2023-07-06 00:46:43 accuracy: 0.6165
2023-07-06 00:46:43 pr_auc: 0.6835
2023-07-06 00:46:43 roc_auc: 0.6484
2023-07-06 00:46:43 f1: 0.6444
2023-07-06 00:46:43 loss: 0.6688
2023-07-06 00:46:43 New best roc_auc score (0.6484) at epoch-4, step-1215
2023-07-06 00:46:43 
2023-07-06 00:50:33 --- Train epoch-5, step-1458 ---
2023-07-06 00:50:33 loss: 0.5551
2023-07-06 00:50:41 --- Eval epoch-5, step-1458 ---
2023-07-06 00:50:41 accuracy: 0.6021
2023-07-06 00:50:41 pr_auc: 0.6744
2023-07-06 00:50:41 roc_auc: 0.6442
2023-07-06 00:50:41 f1: 0.6132
2023-07-06 00:50:41 loss: 0.6833
2023-07-06 00:50:41 
2023-07-06 00:54:00 --- Train epoch-6, step-1701 ---
2023-07-06 00:54:00 loss: 0.5215
2023-07-06 00:54:08 --- Eval epoch-6, step-1701 ---
2023-07-06 00:54:08 accuracy: 0.6186
2023-07-06 00:54:08 pr_auc: 0.6706
2023-07-06 00:54:08 roc_auc: 0.6351
2023-07-06 00:54:08 f1: 0.6574
2023-07-06 00:54:08 loss: 0.6999
2023-07-06 00:54:09 
2023-07-06 00:57:21 --- Train epoch-7, step-1944 ---
2023-07-06 00:57:21 loss: 0.5104
2023-07-06 00:57:29 --- Eval epoch-7, step-1944 ---
2023-07-06 00:57:29 accuracy: 0.6165
2023-07-06 00:57:29 pr_auc: 0.6646
2023-07-06 00:57:29 roc_auc: 0.6311
2023-07-06 00:57:29 f1: 0.6821
2023-07-06 00:57:29 loss: 0.7791
2023-07-06 00:57:29 
2023-07-06 01:00:38 --- Train epoch-8, step-2187 ---
2023-07-06 01:00:38 loss: 0.4884
2023-07-06 01:00:46 --- Eval epoch-8, step-2187 ---
2023-07-06 01:00:46 accuracy: 0.5959
2023-07-06 01:00:46 pr_auc: 0.6733
2023-07-06 01:00:46 roc_auc: 0.6311
2023-07-06 01:00:46 f1: 0.6032
2023-07-06 01:00:46 loss: 0.7655
2023-07-06 01:00:46 
2023-07-06 01:04:03 --- Train epoch-9, step-2430 ---
2023-07-06 01:04:03 loss: 0.4547
2023-07-06 01:04:11 --- Eval epoch-9, step-2430 ---
2023-07-06 01:04:11 accuracy: 0.5990
2023-07-06 01:04:11 pr_auc: 0.6635
2023-07-06 01:04:11 roc_auc: 0.6260
2023-07-06 01:04:11 f1: 0.6814
2023-07-06 01:04:11 loss: 0.8642
2023-07-06 01:04:11 
2023-07-06 01:07:26 --- Train epoch-10, step-2673 ---
2023-07-06 01:07:26 loss: 0.4338
2023-07-06 01:07:36 --- Eval epoch-10, step-2673 ---
2023-07-06 01:07:36 accuracy: 0.5948
2023-07-06 01:07:36 pr_auc: 0.6681
2023-07-06 01:07:36 roc_auc: 0.6313
2023-07-06 01:07:36 f1: 0.5902
2023-07-06 01:07:36 loss: 0.9455
2023-07-06 01:07:36 
2023-07-06 01:10:48 --- Train epoch-11, step-2916 ---
2023-07-06 01:10:48 loss: 0.4050
2023-07-06 01:10:56 --- Eval epoch-11, step-2916 ---
2023-07-06 01:10:56 accuracy: 0.6134
2023-07-06 01:10:56 pr_auc: 0.6629
2023-07-06 01:10:56 roc_auc: 0.6235
2023-07-06 01:10:56 f1: 0.6550
2023-07-06 01:10:56 loss: 0.8421
2023-07-06 01:10:56 
2023-07-06 01:14:08 --- Train epoch-12, step-3159 ---
2023-07-06 01:14:08 loss: 0.3976
2023-07-06 01:14:16 --- Eval epoch-12, step-3159 ---
2023-07-06 01:14:16 accuracy: 0.6186
2023-07-06 01:14:16 pr_auc: 0.6604
2023-07-06 01:14:16 roc_auc: 0.6208
2023-07-06 01:14:16 f1: 0.6630
2023-07-06 01:14:16 loss: 0.8821
2023-07-06 01:14:16 
2023-07-06 01:17:27 --- Train epoch-13, step-3402 ---
2023-07-06 01:17:27 loss: 0.3661
2023-07-06 01:17:35 --- Eval epoch-13, step-3402 ---
2023-07-06 01:17:35 accuracy: 0.5918
2023-07-06 01:17:35 pr_auc: 0.6588
2023-07-06 01:17:35 roc_auc: 0.6155
2023-07-06 01:17:35 f1: 0.6095
2023-07-06 01:17:35 loss: 1.0354
2023-07-06 01:17:36 
2023-07-06 01:20:47 --- Train epoch-14, step-3645 ---
2023-07-06 01:20:47 loss: 0.3314
2023-07-06 01:20:55 --- Eval epoch-14, step-3645 ---
2023-07-06 01:20:55 accuracy: 0.6000
2023-07-06 01:20:55 pr_auc: 0.6585
2023-07-06 01:20:55 roc_auc: 0.6144
2023-07-06 01:20:55 f1: 0.6135
2023-07-06 01:20:55 loss: 1.1996
2023-07-06 01:20:55 
2023-07-06 01:24:09 --- Train epoch-15, step-3888 ---
2023-07-06 01:24:09 loss: 0.3171
2023-07-06 01:24:17 --- Eval epoch-15, step-3888 ---
2023-07-06 01:24:17 accuracy: 0.5969
2023-07-06 01:24:17 pr_auc: 0.6537
2023-07-06 01:24:17 roc_auc: 0.6104
2023-07-06 01:24:17 f1: 0.6591
2023-07-06 01:24:17 loss: 1.0902
2023-07-06 01:24:17 
2023-07-06 01:27:32 --- Train epoch-16, step-4131 ---
2023-07-06 01:27:32 loss: 0.2812
2023-07-06 01:27:40 --- Eval epoch-16, step-4131 ---
2023-07-06 01:27:40 accuracy: 0.6072
2023-07-06 01:27:40 pr_auc: 0.6679
2023-07-06 01:27:40 roc_auc: 0.6230
2023-07-06 01:27:40 f1: 0.6443
2023-07-06 01:27:40 loss: 1.2256
2023-07-06 01:27:40 
2023-07-06 01:30:50 --- Train epoch-17, step-4374 ---
2023-07-06 01:30:50 loss: 0.2723
2023-07-06 01:30:58 --- Eval epoch-17, step-4374 ---
2023-07-06 01:30:58 accuracy: 0.5938
2023-07-06 01:30:58 pr_auc: 0.6585
2023-07-06 01:30:58 roc_auc: 0.6108
2023-07-06 01:30:58 f1: 0.6060
2023-07-06 01:30:58 loss: 1.4429
2023-07-06 01:30:58 
2023-07-06 01:34:13 --- Train epoch-18, step-4617 ---
2023-07-06 01:34:13 loss: 0.2487
2023-07-06 01:34:20 --- Eval epoch-18, step-4617 ---
2023-07-06 01:34:20 accuracy: 0.5979
2023-07-06 01:34:20 pr_auc: 0.6519
2023-07-06 01:34:20 roc_auc: 0.6074
2023-07-06 01:34:20 f1: 0.6415
2023-07-06 01:34:20 loss: 1.3750
2023-07-06 01:34:20 
2023-07-06 01:37:27 --- Train epoch-19, step-4860 ---
2023-07-06 01:37:27 loss: 0.2393
2023-07-06 01:37:35 --- Eval epoch-19, step-4860 ---
2023-07-06 01:37:35 accuracy: 0.6093
2023-07-06 01:37:35 pr_auc: 0.6657
2023-07-06 01:37:35 roc_auc: 0.6145
2023-07-06 01:37:35 f1: 0.6455
2023-07-06 01:37:35 loss: 1.5120
2023-07-06 01:37:35 Loaded best model
2023-07-06 01:37:38 GRASP(
  (embeddings): ModuleDict(
    (conditions): Embedding(4031, 128, padding_idx=0)
    (procedures): Embedding(1276, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (grasp): ModuleDict(
    (conditions): GRASPLayer(
      (backbone): ConCareLayer(
        (PositionalEncoding): PositionalEncoding(
          (dropout): Dropout(p=0, inplace=False)
        )
        (GRUs): ModuleList(
          (0): GRU(1, 128, batch_first=True)
          (1): GRU(1, 128, batch_first=True)
          (2): GRU(1, 128, batch_first=True)
          (3): GRU(1, 128, batch_first=True)
          (4): GRU(1, 128, batch_first=True)
          (5): GRU(1, 128, batch_first=True)
          (6): GRU(1, 128, batch_first=True)
          (7): GRU(1, 128, batch_first=True)
          (8): GRU(1, 128, batch_first=True)
          (9): GRU(1, 128, batch_first=True)
          (10): GRU(1, 128, batch_first=True)
          (11): GRU(1, 128, batch_first=True)
          (12): GRU(1, 128, batch_first=True)
          (13): GRU(1, 128, batch_first=True)
          (14): GRU(1, 128, batch_first=True)
          (15): GRU(1, 128, batch_first=True)
          (16): GRU(1, 128, batch_first=True)
          (17): GRU(1, 128, batch_first=True)
          (18): GRU(1, 128, batch_first=True)
          (19): GRU(1, 128, batch_first=True)
          (20): GRU(1, 128, batch_first=True)
          (21): GRU(1, 128, batch_first=True)
          (22): GRU(1, 128, batch_first=True)
          (23): GRU(1, 128, batch_first=True)
          (24): GRU(1, 128, batch_first=True)
          (25): GRU(1, 128, batch_first=True)
          (26): GRU(1, 128, batch_first=True)
          (27): GRU(1, 128, batch_first=True)
          (28): GRU(1, 128, batch_first=True)
          (29): GRU(1, 128, batch_first=True)
          (30): GRU(1, 128, batch_first=True)
          (31): GRU(1, 128, batch_first=True)
          (32): GRU(1, 128, batch_first=True)
          (33): GRU(1, 128, batch_first=True)
          (34): GRU(1, 128, batch_first=True)
          (35): GRU(1, 128, batch_first=True)
          (36): GRU(1, 128, batch_first=True)
          (37): GRU(1, 128, batch_first=True)
          (38): GRU(1, 128, batch_first=True)
          (39): GRU(1, 128, batch_first=True)
          (40): GRU(1, 128, batch_first=True)
          (41): GRU(1, 128, batch_first=True)
          (42): GRU(1, 128, batch_first=True)
          (43): GRU(1, 128, batch_first=True)
          (44): GRU(1, 128, batch_first=True)
          (45): GRU(1, 128, batch_first=True)
          (46): GRU(1, 128, batch_first=True)
          (47): GRU(1, 128, batch_first=True)
          (48): GRU(1, 128, batch_first=True)
          (49): GRU(1, 128, batch_first=True)
          (50): GRU(1, 128, batch_first=True)
          (51): GRU(1, 128, batch_first=True)
          (52): GRU(1, 128, batch_first=True)
          (53): GRU(1, 128, batch_first=True)
          (54): GRU(1, 128, batch_first=True)
          (55): GRU(1, 128, batch_first=True)
          (56): GRU(1, 128, batch_first=True)
          (57): GRU(1, 128, batch_first=True)
          (58): GRU(1, 128, batch_first=True)
          (59): GRU(1, 128, batch_first=True)
          (60): GRU(1, 128, batch_first=True)
          (61): GRU(1, 128, batch_first=True)
          (62): GRU(1, 128, batch_first=True)
          (63): GRU(1, 128, batch_first=True)
          (64): GRU(1, 128, batch_first=True)
          (65): GRU(1, 128, batch_first=True)
          (66): GRU(1, 128, batch_first=True)
          (67): GRU(1, 128, batch_first=True)
          (68): GRU(1, 128, batch_first=True)
          (69): GRU(1, 128, batch_first=True)
          (70): GRU(1, 128, batch_first=True)
          (71): GRU(1, 128, batch_first=True)
          (72): GRU(1, 128, batch_first=True)
          (73): GRU(1, 128, batch_first=True)
          (74): GRU(1, 128, batch_first=True)
          (75): GRU(1, 128, batch_first=True)
          (76): GRU(1, 128, batch_first=True)
          (77): GRU(1, 128, batch_first=True)
          (78): GRU(1, 128, batch_first=True)
          (79): GRU(1, 128, batch_first=True)
          (80): GRU(1, 128, batch_first=True)
          (81): GRU(1, 128, batch_first=True)
          (82): GRU(1, 128, batch_first=True)
          (83): GRU(1, 128, batch_first=True)
          (84): GRU(1, 128, batch_first=True)
          (85): GRU(1, 128, batch_first=True)
          (86): GRU(1, 128, batch_first=True)
          (87): GRU(1, 128, batch_first=True)
          (88): GRU(1, 128, batch_first=True)
          (89): GRU(1, 128, batch_first=True)
          (90): GRU(1, 128, batch_first=True)
          (91): GRU(1, 128, batch_first=True)
          (92): GRU(1, 128, batch_first=True)
          (93): GRU(1, 128, batch_first=True)
          (94): GRU(1, 128, batch_first=True)
          (95): GRU(1, 128, batch_first=True)
          (96): GRU(1, 128, batch_first=True)
          (97): GRU(1, 128, batch_first=True)
          (98): GRU(1, 128, batch_first=True)
          (99): GRU(1, 128, batch_first=True)
          (100): GRU(1, 128, batch_first=True)
          (101): GRU(1, 128, batch_first=True)
          (102): GRU(1, 128, batch_first=True)
          (103): GRU(1, 128, batch_first=True)
          (104): GRU(1, 128, batch_first=True)
          (105): GRU(1, 128, batch_first=True)
          (106): GRU(1, 128, batch_first=True)
          (107): GRU(1, 128, batch_first=True)
          (108): GRU(1, 128, batch_first=True)
          (109): GRU(1, 128, batch_first=True)
          (110): GRU(1, 128, batch_first=True)
          (111): GRU(1, 128, batch_first=True)
          (112): GRU(1, 128, batch_first=True)
          (113): GRU(1, 128, batch_first=True)
          (114): GRU(1, 128, batch_first=True)
          (115): GRU(1, 128, batch_first=True)
          (116): GRU(1, 128, batch_first=True)
          (117): GRU(1, 128, batch_first=True)
          (118): GRU(1, 128, batch_first=True)
          (119): GRU(1, 128, batch_first=True)
          (120): GRU(1, 128, batch_first=True)
          (121): GRU(1, 128, batch_first=True)
          (122): GRU(1, 128, batch_first=True)
          (123): GRU(1, 128, batch_first=True)
          (124): GRU(1, 128, batch_first=True)
          (125): GRU(1, 128, batch_first=True)
          (126): GRU(1, 128, batch_first=True)
          (127): GRU(1, 128, batch_first=True)
        )
        (LastStepAttentions): ModuleList(
          (0): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (1): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (2): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (3): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (4): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (5): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (6): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (7): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (8): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (9): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (10): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (11): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (12): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (13): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (14): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (15): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (16): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (17): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (18): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (19): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (20): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (21): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (22): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (23): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (24): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (25): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (26): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (27): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (28): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (29): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (30): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (31): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (32): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (33): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (34): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (35): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (36): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (37): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (38): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (39): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (40): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (41): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (42): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (43): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (44): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (45): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (46): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (47): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (48): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (49): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (50): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (51): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (52): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (53): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (54): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (55): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (56): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (57): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (58): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (59): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (60): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (61): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (62): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (63): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (64): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (65): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (66): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (67): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (68): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (69): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (70): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (71): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (72): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (73): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (74): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (75): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (76): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (77): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (78): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (79): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (80): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (81): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (82): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (83): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (84): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (85): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (86): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (87): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (88): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (89): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (90): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (91): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (92): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (93): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (94): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (95): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (96): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (97): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (98): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (99): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (100): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (101): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (102): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (103): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (104): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (105): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (106): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (107): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (108): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (109): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (110): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (111): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (112): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (113): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (114): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (115): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (116): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (117): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (118): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (119): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (120): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (121): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (122): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (123): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (124): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (125): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (126): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (127): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
        )
        (FinalAttentionQKV): FinalAttentionQKV(
          (W_q): Linear(in_features=128, out_features=128, bias=True)
          (W_k): Linear(in_features=128, out_features=128, bias=True)
          (W_v): Linear(in_features=128, out_features=128, bias=True)
          (W_out): Linear(in_features=128, out_features=1, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
        )
        (MultiHeadedAttention): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
            (2): Linear(in_features=128, out_features=128, bias=True)
          )
          (final_linear): Linear(in_features=128, out_features=128, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (SublayerConnection): SublayerConnection(
          (norm): LayerNorm()
          (dropout): Dropout(p=0, inplace=False)
        )
        (PositionwiseFeedForward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0, inplace=False)
        (tanh): Tanh()
        (softmax): Softmax(dim=None)
        (sigmoid): Sigmoid()
        (relu): ReLU()
      )
      (relu): ReLU()
      (tanh): Tanh()
      (sigmoid): Sigmoid()
      (dropout): Dropout(p=0.5, inplace=False)
      (weight1): Linear(in_features=128, out_features=1, bias=True)
      (weight2): Linear(in_features=128, out_features=1, bias=True)
      (GCN): GraphConvolution()
      (GCN_2): GraphConvolution()
      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (procedures): GRASPLayer(
      (backbone): ConCareLayer(
        (PositionalEncoding): PositionalEncoding(
          (dropout): Dropout(p=0, inplace=False)
        )
        (GRUs): ModuleList(
          (0): GRU(1, 128, batch_first=True)
          (1): GRU(1, 128, batch_first=True)
          (2): GRU(1, 128, batch_first=True)
          (3): GRU(1, 128, batch_first=True)
          (4): GRU(1, 128, batch_first=True)
          (5): GRU(1, 128, batch_first=True)
          (6): GRU(1, 128, batch_first=True)
          (7): GRU(1, 128, batch_first=True)
          (8): GRU(1, 128, batch_first=True)
          (9): GRU(1, 128, batch_first=True)
          (10): GRU(1, 128, batch_first=True)
          (11): GRU(1, 128, batch_first=True)
          (12): GRU(1, 128, batch_first=True)
          (13): GRU(1, 128, batch_first=True)
          (14): GRU(1, 128, batch_first=True)
          (15): GRU(1, 128, batch_first=True)
          (16): GRU(1, 128, batch_first=True)
          (17): GRU(1, 128, batch_first=True)
          (18): GRU(1, 128, batch_first=True)
          (19): GRU(1, 128, batch_first=True)
          (20): GRU(1, 128, batch_first=True)
          (21): GRU(1, 128, batch_first=True)
          (22): GRU(1, 128, batch_first=True)
          (23): GRU(1, 128, batch_first=True)
          (24): GRU(1, 128, batch_first=True)
          (25): GRU(1, 128, batch_first=True)
          (26): GRU(1, 128, batch_first=True)
          (27): GRU(1, 128, batch_first=True)
          (28): GRU(1, 128, batch_first=True)
          (29): GRU(1, 128, batch_first=True)
          (30): GRU(1, 128, batch_first=True)
          (31): GRU(1, 128, batch_first=True)
          (32): GRU(1, 128, batch_first=True)
          (33): GRU(1, 128, batch_first=True)
          (34): GRU(1, 128, batch_first=True)
          (35): GRU(1, 128, batch_first=True)
          (36): GRU(1, 128, batch_first=True)
          (37): GRU(1, 128, batch_first=True)
          (38): GRU(1, 128, batch_first=True)
          (39): GRU(1, 128, batch_first=True)
          (40): GRU(1, 128, batch_first=True)
          (41): GRU(1, 128, batch_first=True)
          (42): GRU(1, 128, batch_first=True)
          (43): GRU(1, 128, batch_first=True)
          (44): GRU(1, 128, batch_first=True)
          (45): GRU(1, 128, batch_first=True)
          (46): GRU(1, 128, batch_first=True)
          (47): GRU(1, 128, batch_first=True)
          (48): GRU(1, 128, batch_first=True)
          (49): GRU(1, 128, batch_first=True)
          (50): GRU(1, 128, batch_first=True)
          (51): GRU(1, 128, batch_first=True)
          (52): GRU(1, 128, batch_first=True)
          (53): GRU(1, 128, batch_first=True)
          (54): GRU(1, 128, batch_first=True)
          (55): GRU(1, 128, batch_first=True)
          (56): GRU(1, 128, batch_first=True)
          (57): GRU(1, 128, batch_first=True)
          (58): GRU(1, 128, batch_first=True)
          (59): GRU(1, 128, batch_first=True)
          (60): GRU(1, 128, batch_first=True)
          (61): GRU(1, 128, batch_first=True)
          (62): GRU(1, 128, batch_first=True)
          (63): GRU(1, 128, batch_first=True)
          (64): GRU(1, 128, batch_first=True)
          (65): GRU(1, 128, batch_first=True)
          (66): GRU(1, 128, batch_first=True)
          (67): GRU(1, 128, batch_first=True)
          (68): GRU(1, 128, batch_first=True)
          (69): GRU(1, 128, batch_first=True)
          (70): GRU(1, 128, batch_first=True)
          (71): GRU(1, 128, batch_first=True)
          (72): GRU(1, 128, batch_first=True)
          (73): GRU(1, 128, batch_first=True)
          (74): GRU(1, 128, batch_first=True)
          (75): GRU(1, 128, batch_first=True)
          (76): GRU(1, 128, batch_first=True)
          (77): GRU(1, 128, batch_first=True)
          (78): GRU(1, 128, batch_first=True)
          (79): GRU(1, 128, batch_first=True)
          (80): GRU(1, 128, batch_first=True)
          (81): GRU(1, 128, batch_first=True)
          (82): GRU(1, 128, batch_first=True)
          (83): GRU(1, 128, batch_first=True)
          (84): GRU(1, 128, batch_first=True)
          (85): GRU(1, 128, batch_first=True)
          (86): GRU(1, 128, batch_first=True)
          (87): GRU(1, 128, batch_first=True)
          (88): GRU(1, 128, batch_first=True)
          (89): GRU(1, 128, batch_first=True)
          (90): GRU(1, 128, batch_first=True)
          (91): GRU(1, 128, batch_first=True)
          (92): GRU(1, 128, batch_first=True)
          (93): GRU(1, 128, batch_first=True)
          (94): GRU(1, 128, batch_first=True)
          (95): GRU(1, 128, batch_first=True)
          (96): GRU(1, 128, batch_first=True)
          (97): GRU(1, 128, batch_first=True)
          (98): GRU(1, 128, batch_first=True)
          (99): GRU(1, 128, batch_first=True)
          (100): GRU(1, 128, batch_first=True)
          (101): GRU(1, 128, batch_first=True)
          (102): GRU(1, 128, batch_first=True)
          (103): GRU(1, 128, batch_first=True)
          (104): GRU(1, 128, batch_first=True)
          (105): GRU(1, 128, batch_first=True)
          (106): GRU(1, 128, batch_first=True)
          (107): GRU(1, 128, batch_first=True)
          (108): GRU(1, 128, batch_first=True)
          (109): GRU(1, 128, batch_first=True)
          (110): GRU(1, 128, batch_first=True)
          (111): GRU(1, 128, batch_first=True)
          (112): GRU(1, 128, batch_first=True)
          (113): GRU(1, 128, batch_first=True)
          (114): GRU(1, 128, batch_first=True)
          (115): GRU(1, 128, batch_first=True)
          (116): GRU(1, 128, batch_first=True)
          (117): GRU(1, 128, batch_first=True)
          (118): GRU(1, 128, batch_first=True)
          (119): GRU(1, 128, batch_first=True)
          (120): GRU(1, 128, batch_first=True)
          (121): GRU(1, 128, batch_first=True)
          (122): GRU(1, 128, batch_first=True)
          (123): GRU(1, 128, batch_first=True)
          (124): GRU(1, 128, batch_first=True)
          (125): GRU(1, 128, batch_first=True)
          (126): GRU(1, 128, batch_first=True)
          (127): GRU(1, 128, batch_first=True)
        )
        (LastStepAttentions): ModuleList(
          (0): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (1): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (2): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (3): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (4): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (5): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (6): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (7): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (8): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (9): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (10): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (11): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (12): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (13): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (14): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (15): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (16): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (17): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (18): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (19): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (20): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (21): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (22): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (23): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (24): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (25): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (26): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (27): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (28): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (29): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (30): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (31): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (32): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (33): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (34): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (35): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (36): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (37): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (38): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (39): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (40): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (41): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (42): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (43): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (44): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (45): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (46): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (47): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (48): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (49): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (50): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (51): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (52): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (53): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (54): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (55): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (56): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (57): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (58): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (59): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (60): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (61): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (62): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (63): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (64): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (65): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (66): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (67): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (68): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (69): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (70): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (71): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (72): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (73): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (74): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (75): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (76): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (77): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (78): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (79): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (80): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (81): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (82): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (83): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (84): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (85): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (86): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (87): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (88): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (89): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (90): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (91): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (92): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (93): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (94): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (95): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (96): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (97): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (98): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (99): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (100): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (101): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (102): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (103): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (104): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (105): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (106): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (107): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (108): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (109): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (110): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (111): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (112): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (113): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (114): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (115): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (116): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (117): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (118): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (119): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (120): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (121): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (122): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (123): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (124): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (125): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (126): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (127): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
        )
        (FinalAttentionQKV): FinalAttentionQKV(
          (W_q): Linear(in_features=128, out_features=128, bias=True)
          (W_k): Linear(in_features=128, out_features=128, bias=True)
          (W_v): Linear(in_features=128, out_features=128, bias=True)
          (W_out): Linear(in_features=128, out_features=1, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
        )
        (MultiHeadedAttention): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
            (2): Linear(in_features=128, out_features=128, bias=True)
          )
          (final_linear): Linear(in_features=128, out_features=128, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (SublayerConnection): SublayerConnection(
          (norm): LayerNorm()
          (dropout): Dropout(p=0, inplace=False)
        )
        (PositionwiseFeedForward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0, inplace=False)
        (tanh): Tanh()
        (softmax): Softmax(dim=None)
        (sigmoid): Sigmoid()
        (relu): ReLU()
      )
      (relu): ReLU()
      (tanh): Tanh()
      (sigmoid): Sigmoid()
      (dropout): Dropout(p=0.5, inplace=False)
      (weight1): Linear(in_features=128, out_features=1, bias=True)
      (weight2): Linear(in_features=128, out_features=1, bias=True)
      (GCN): GraphConvolution()
      (GCN_2): GraphConvolution()
      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (fc): Linear(in_features=256, out_features=1, bias=True)
)
2023-07-06 01:37:38 Metrics: ['accuracy', 'pr_auc', 'roc_auc', 'f1']
2023-07-06 01:37:38 Device: cuda
2023-07-06 01:37:38 
2023-07-06 01:37:38 Training:
2023-07-06 01:37:38 Batch size: 32
2023-07-06 01:37:38 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-06 01:37:38 Optimizer params: {'lr': 0.001}
2023-07-06 01:37:38 Weight decay: 0.0
2023-07-06 01:37:38 Max grad norm: None
2023-07-06 01:37:38 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd79523cc50>
2023-07-06 01:37:38 Monitor: roc_auc
2023-07-06 01:37:38 Monitor criterion: max
2023-07-06 01:37:38 Epochs: 20
2023-07-06 01:37:38 
2023-07-06 01:40:50 --- Train epoch-0, step-243 ---
2023-07-06 01:40:50 loss: 0.2719
2023-07-06 01:41:00 --- Eval epoch-0, step-243 ---
2023-07-06 01:41:00 accuracy: 0.9294
2023-07-06 01:41:00 pr_auc: 0.0747
2023-07-06 01:41:00 roc_auc: 0.5021
2023-07-06 01:41:00 f1: 0.0000
2023-07-06 01:41:00 loss: 0.2568
2023-07-06 01:41:00 New best roc_auc score (0.5021) at epoch-0, step-243
2023-07-06 01:41:00 
2023-07-06 01:44:15 --- Train epoch-1, step-486 ---
2023-07-06 01:44:15 loss: 0.2540
2023-07-06 01:44:24 --- Eval epoch-1, step-486 ---
2023-07-06 01:44:24 accuracy: 0.9294
2023-07-06 01:44:24 pr_auc: 0.0945
2023-07-06 01:44:24 roc_auc: 0.5827
2023-07-06 01:44:24 f1: 0.0000
2023-07-06 01:44:24 loss: 0.2528
2023-07-06 01:44:24 New best roc_auc score (0.5827) at epoch-1, step-486
2023-07-06 01:44:24 
2023-07-06 01:47:34 --- Train epoch-2, step-729 ---
2023-07-06 01:47:34 loss: 0.2387
2023-07-06 01:47:43 --- Eval epoch-2, step-729 ---
2023-07-06 01:47:43 accuracy: 0.9294
2023-07-06 01:47:43 pr_auc: 0.0984
2023-07-06 01:47:43 roc_auc: 0.6132
2023-07-06 01:47:43 f1: 0.0000
2023-07-06 01:47:43 loss: 0.2921
2023-07-06 01:47:43 New best roc_auc score (0.6132) at epoch-2, step-729
2023-07-06 01:47:43 
2023-07-06 01:50:54 --- Train epoch-3, step-972 ---
2023-07-06 01:50:54 loss: 0.2217
2023-07-06 01:51:03 --- Eval epoch-3, step-972 ---
2023-07-06 01:51:03 accuracy: 0.9273
2023-07-06 01:51:03 pr_auc: 0.0971
2023-07-06 01:51:03 roc_auc: 0.6084
2023-07-06 01:51:03 f1: 0.0270
2023-07-06 01:51:03 loss: 0.2660
2023-07-06 01:51:03 
2023-07-06 01:54:14 --- Train epoch-4, step-1215 ---
2023-07-06 01:54:14 loss: 0.1976
2023-07-06 01:54:23 --- Eval epoch-4, step-1215 ---
2023-07-06 01:54:23 accuracy: 0.9273
2023-07-06 01:54:23 pr_auc: 0.1066
2023-07-06 01:54:23 roc_auc: 0.6284
2023-07-06 01:54:23 f1: 0.0000
2023-07-06 01:54:23 loss: 0.2777
2023-07-06 01:54:23 New best roc_auc score (0.6284) at epoch-4, step-1215
2023-07-06 01:54:23 
2023-07-06 01:57:36 --- Train epoch-5, step-1458 ---
2023-07-06 01:57:36 loss: 0.1815
2023-07-06 01:57:44 --- Eval epoch-5, step-1458 ---
2023-07-06 01:57:44 accuracy: 0.9263
2023-07-06 01:57:44 pr_auc: 0.1125
2023-07-06 01:57:44 roc_auc: 0.6190
2023-07-06 01:57:44 f1: 0.0267
2023-07-06 01:57:44 loss: 0.3129
2023-07-06 01:57:44 
2023-07-06 02:00:55 --- Train epoch-6, step-1701 ---
2023-07-06 02:00:55 loss: 0.1663
2023-07-06 02:01:02 --- Eval epoch-6, step-1701 ---
2023-07-06 02:01:02 accuracy: 0.9062
2023-07-06 02:01:02 pr_auc: 0.1011
2023-07-06 02:01:02 roc_auc: 0.6107
2023-07-06 02:01:02 f1: 0.0412
2023-07-06 02:01:02 loss: 0.3476
2023-07-06 02:01:02 
2023-07-06 02:04:15 --- Train epoch-7, step-1944 ---
2023-07-06 02:04:15 loss: 0.1496
2023-07-06 02:04:23 --- Eval epoch-7, step-1944 ---
2023-07-06 02:04:23 accuracy: 0.9132
2023-07-06 02:04:23 pr_auc: 0.0976
2023-07-06 02:04:23 roc_auc: 0.6079
2023-07-06 02:04:23 f1: 0.0444
2023-07-06 02:04:23 loss: 0.3545
2023-07-06 02:04:23 
2023-07-06 02:07:39 --- Train epoch-8, step-2187 ---
2023-07-06 02:07:39 loss: 0.1389
2023-07-06 02:07:47 --- Eval epoch-8, step-2187 ---
2023-07-06 02:07:47 accuracy: 0.9263
2023-07-06 02:07:47 pr_auc: 0.0923
2023-07-06 02:07:47 roc_auc: 0.5845
2023-07-06 02:07:47 f1: 0.0519
2023-07-06 02:07:47 loss: 0.4454
2023-07-06 02:07:47 
2023-07-06 02:10:59 --- Train epoch-9, step-2430 ---
2023-07-06 02:10:59 loss: 0.1207
2023-07-06 02:11:08 --- Eval epoch-9, step-2430 ---
2023-07-06 02:11:08 accuracy: 0.9092
2023-07-06 02:11:08 pr_auc: 0.0810
2023-07-06 02:11:08 roc_auc: 0.5769
2023-07-06 02:11:08 f1: 0.0426
2023-07-06 02:11:08 loss: 0.4456
2023-07-06 02:11:08 
2023-07-06 02:14:24 --- Train epoch-10, step-2673 ---
2023-07-06 02:14:24 loss: 0.1170
2023-07-06 02:14:37 --- Eval epoch-10, step-2673 ---
2023-07-06 02:14:37 accuracy: 0.9213
2023-07-06 02:14:37 pr_auc: 0.0824
2023-07-06 02:14:37 roc_auc: 0.5614
2023-07-06 02:14:37 f1: 0.0250
2023-07-06 02:14:37 loss: 0.4961
2023-07-06 02:14:37 
2023-07-06 02:17:48 --- Train epoch-11, step-2916 ---
2023-07-06 02:17:48 loss: 0.0996
2023-07-06 02:17:57 --- Eval epoch-11, step-2916 ---
2023-07-06 02:17:57 accuracy: 0.9132
2023-07-06 02:17:57 pr_auc: 0.0786
2023-07-06 02:17:57 roc_auc: 0.5515
2023-07-06 02:17:57 f1: 0.0000
2023-07-06 02:17:57 loss: 0.4796
2023-07-06 02:17:57 
2023-07-06 02:21:08 --- Train epoch-12, step-3159 ---
2023-07-06 02:21:08 loss: 0.0974
2023-07-06 02:21:18 --- Eval epoch-12, step-3159 ---
2023-07-06 02:21:18 accuracy: 0.8940
2023-07-06 02:21:18 pr_auc: 0.0892
2023-07-06 02:21:18 roc_auc: 0.5557
2023-07-06 02:21:18 f1: 0.0708
2023-07-06 02:21:18 loss: 0.5470
2023-07-06 02:21:18 
2023-07-06 02:24:34 --- Train epoch-13, step-3402 ---
2023-07-06 02:24:34 loss: 0.0767
2023-07-06 02:24:42 --- Eval epoch-13, step-3402 ---
2023-07-06 02:24:42 accuracy: 0.8920
2023-07-06 02:24:42 pr_auc: 0.0858
2023-07-06 02:24:42 roc_auc: 0.5604
2023-07-06 02:24:42 f1: 0.0531
2023-07-06 02:24:42 loss: 0.6437
2023-07-06 02:24:42 
2023-07-06 02:27:57 --- Train epoch-14, step-3645 ---
2023-07-06 02:27:57 loss: 0.0627
2023-07-06 02:28:05 --- Eval epoch-14, step-3645 ---
2023-07-06 02:28:05 accuracy: 0.8819
2023-07-06 02:28:05 pr_auc: 0.0808
2023-07-06 02:28:05 roc_auc: 0.5369
2023-07-06 02:28:05 f1: 0.0640
2023-07-06 02:28:05 loss: 0.7020
2023-07-06 02:28:05 
2023-07-06 02:31:19 --- Train epoch-15, step-3888 ---
2023-07-06 02:31:19 loss: 0.0542
2023-07-06 02:31:27 --- Eval epoch-15, step-3888 ---
2023-07-06 02:31:27 accuracy: 0.9102
2023-07-06 02:31:27 pr_auc: 0.0757
2023-07-06 02:31:27 roc_auc: 0.5233
2023-07-06 02:31:27 f1: 0.0220
2023-07-06 02:31:27 loss: 0.8656
2023-07-06 02:31:27 
2023-07-06 02:34:44 --- Train epoch-16, step-4131 ---
2023-07-06 02:34:44 loss: 0.0468
2023-07-06 02:34:53 --- Eval epoch-16, step-4131 ---
2023-07-06 02:34:53 accuracy: 0.8930
2023-07-06 02:34:53 pr_auc: 0.0777
2023-07-06 02:34:53 roc_auc: 0.5498
2023-07-06 02:34:53 f1: 0.0185
2023-07-06 02:34:53 loss: 0.9110
2023-07-06 02:34:53 
2023-07-06 02:38:02 --- Train epoch-17, step-4374 ---
2023-07-06 02:38:02 loss: 0.0445
2023-07-06 02:38:10 --- Eval epoch-17, step-4374 ---
2023-07-06 02:38:10 accuracy: 0.9233
2023-07-06 02:38:10 pr_auc: 0.0762
2023-07-06 02:38:10 roc_auc: 0.5415
2023-07-06 02:38:10 f1: 0.0000
2023-07-06 02:38:10 loss: 0.7191
2023-07-06 02:38:10 
2023-07-06 02:41:22 --- Train epoch-18, step-4617 ---
2023-07-06 02:41:22 loss: 0.0348
2023-07-06 02:41:30 --- Eval epoch-18, step-4617 ---
2023-07-06 02:41:30 accuracy: 0.9132
2023-07-06 02:41:30 pr_auc: 0.0726
2023-07-06 02:41:30 roc_auc: 0.5204
2023-07-06 02:41:30 f1: 0.0000
2023-07-06 02:41:30 loss: 1.1371
2023-07-06 02:41:30 
2023-07-06 02:44:48 --- Train epoch-19, step-4860 ---
2023-07-06 02:44:48 loss: 0.0277
2023-07-06 02:44:56 --- Eval epoch-19, step-4860 ---
2023-07-06 02:44:56 accuracy: 0.9001
2023-07-06 02:44:56 pr_auc: 0.0716
2023-07-06 02:44:56 roc_auc: 0.5074
2023-07-06 02:44:56 f1: 0.0198
2023-07-06 02:44:56 loss: 1.1101
2023-07-06 02:44:56 Loaded best model
2023-07-06 02:45:03 GRASP(
  (embeddings): ModuleDict(
    (conditions): Embedding(6664, 128, padding_idx=0)
    (procedures): Embedding(1980, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (grasp): ModuleDict(
    (conditions): GRASPLayer(
      (backbone): ConCareLayer(
        (PositionalEncoding): PositionalEncoding(
          (dropout): Dropout(p=0, inplace=False)
        )
        (GRUs): ModuleList(
          (0): GRU(1, 128, batch_first=True)
          (1): GRU(1, 128, batch_first=True)
          (2): GRU(1, 128, batch_first=True)
          (3): GRU(1, 128, batch_first=True)
          (4): GRU(1, 128, batch_first=True)
          (5): GRU(1, 128, batch_first=True)
          (6): GRU(1, 128, batch_first=True)
          (7): GRU(1, 128, batch_first=True)
          (8): GRU(1, 128, batch_first=True)
          (9): GRU(1, 128, batch_first=True)
          (10): GRU(1, 128, batch_first=True)
          (11): GRU(1, 128, batch_first=True)
          (12): GRU(1, 128, batch_first=True)
          (13): GRU(1, 128, batch_first=True)
          (14): GRU(1, 128, batch_first=True)
          (15): GRU(1, 128, batch_first=True)
          (16): GRU(1, 128, batch_first=True)
          (17): GRU(1, 128, batch_first=True)
          (18): GRU(1, 128, batch_first=True)
          (19): GRU(1, 128, batch_first=True)
          (20): GRU(1, 128, batch_first=True)
          (21): GRU(1, 128, batch_first=True)
          (22): GRU(1, 128, batch_first=True)
          (23): GRU(1, 128, batch_first=True)
          (24): GRU(1, 128, batch_first=True)
          (25): GRU(1, 128, batch_first=True)
          (26): GRU(1, 128, batch_first=True)
          (27): GRU(1, 128, batch_first=True)
          (28): GRU(1, 128, batch_first=True)
          (29): GRU(1, 128, batch_first=True)
          (30): GRU(1, 128, batch_first=True)
          (31): GRU(1, 128, batch_first=True)
          (32): GRU(1, 128, batch_first=True)
          (33): GRU(1, 128, batch_first=True)
          (34): GRU(1, 128, batch_first=True)
          (35): GRU(1, 128, batch_first=True)
          (36): GRU(1, 128, batch_first=True)
          (37): GRU(1, 128, batch_first=True)
          (38): GRU(1, 128, batch_first=True)
          (39): GRU(1, 128, batch_first=True)
          (40): GRU(1, 128, batch_first=True)
          (41): GRU(1, 128, batch_first=True)
          (42): GRU(1, 128, batch_first=True)
          (43): GRU(1, 128, batch_first=True)
          (44): GRU(1, 128, batch_first=True)
          (45): GRU(1, 128, batch_first=True)
          (46): GRU(1, 128, batch_first=True)
          (47): GRU(1, 128, batch_first=True)
          (48): GRU(1, 128, batch_first=True)
          (49): GRU(1, 128, batch_first=True)
          (50): GRU(1, 128, batch_first=True)
          (51): GRU(1, 128, batch_first=True)
          (52): GRU(1, 128, batch_first=True)
          (53): GRU(1, 128, batch_first=True)
          (54): GRU(1, 128, batch_first=True)
          (55): GRU(1, 128, batch_first=True)
          (56): GRU(1, 128, batch_first=True)
          (57): GRU(1, 128, batch_first=True)
          (58): GRU(1, 128, batch_first=True)
          (59): GRU(1, 128, batch_first=True)
          (60): GRU(1, 128, batch_first=True)
          (61): GRU(1, 128, batch_first=True)
          (62): GRU(1, 128, batch_first=True)
          (63): GRU(1, 128, batch_first=True)
          (64): GRU(1, 128, batch_first=True)
          (65): GRU(1, 128, batch_first=True)
          (66): GRU(1, 128, batch_first=True)
          (67): GRU(1, 128, batch_first=True)
          (68): GRU(1, 128, batch_first=True)
          (69): GRU(1, 128, batch_first=True)
          (70): GRU(1, 128, batch_first=True)
          (71): GRU(1, 128, batch_first=True)
          (72): GRU(1, 128, batch_first=True)
          (73): GRU(1, 128, batch_first=True)
          (74): GRU(1, 128, batch_first=True)
          (75): GRU(1, 128, batch_first=True)
          (76): GRU(1, 128, batch_first=True)
          (77): GRU(1, 128, batch_first=True)
          (78): GRU(1, 128, batch_first=True)
          (79): GRU(1, 128, batch_first=True)
          (80): GRU(1, 128, batch_first=True)
          (81): GRU(1, 128, batch_first=True)
          (82): GRU(1, 128, batch_first=True)
          (83): GRU(1, 128, batch_first=True)
          (84): GRU(1, 128, batch_first=True)
          (85): GRU(1, 128, batch_first=True)
          (86): GRU(1, 128, batch_first=True)
          (87): GRU(1, 128, batch_first=True)
          (88): GRU(1, 128, batch_first=True)
          (89): GRU(1, 128, batch_first=True)
          (90): GRU(1, 128, batch_first=True)
          (91): GRU(1, 128, batch_first=True)
          (92): GRU(1, 128, batch_first=True)
          (93): GRU(1, 128, batch_first=True)
          (94): GRU(1, 128, batch_first=True)
          (95): GRU(1, 128, batch_first=True)
          (96): GRU(1, 128, batch_first=True)
          (97): GRU(1, 128, batch_first=True)
          (98): GRU(1, 128, batch_first=True)
          (99): GRU(1, 128, batch_first=True)
          (100): GRU(1, 128, batch_first=True)
          (101): GRU(1, 128, batch_first=True)
          (102): GRU(1, 128, batch_first=True)
          (103): GRU(1, 128, batch_first=True)
          (104): GRU(1, 128, batch_first=True)
          (105): GRU(1, 128, batch_first=True)
          (106): GRU(1, 128, batch_first=True)
          (107): GRU(1, 128, batch_first=True)
          (108): GRU(1, 128, batch_first=True)
          (109): GRU(1, 128, batch_first=True)
          (110): GRU(1, 128, batch_first=True)
          (111): GRU(1, 128, batch_first=True)
          (112): GRU(1, 128, batch_first=True)
          (113): GRU(1, 128, batch_first=True)
          (114): GRU(1, 128, batch_first=True)
          (115): GRU(1, 128, batch_first=True)
          (116): GRU(1, 128, batch_first=True)
          (117): GRU(1, 128, batch_first=True)
          (118): GRU(1, 128, batch_first=True)
          (119): GRU(1, 128, batch_first=True)
          (120): GRU(1, 128, batch_first=True)
          (121): GRU(1, 128, batch_first=True)
          (122): GRU(1, 128, batch_first=True)
          (123): GRU(1, 128, batch_first=True)
          (124): GRU(1, 128, batch_first=True)
          (125): GRU(1, 128, batch_first=True)
          (126): GRU(1, 128, batch_first=True)
          (127): GRU(1, 128, batch_first=True)
        )
        (LastStepAttentions): ModuleList(
          (0): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (1): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (2): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (3): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (4): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (5): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (6): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (7): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (8): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (9): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (10): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (11): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (12): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (13): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (14): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (15): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (16): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (17): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (18): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (19): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (20): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (21): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (22): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (23): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (24): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (25): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (26): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (27): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (28): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (29): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (30): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (31): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (32): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (33): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (34): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (35): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (36): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (37): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (38): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (39): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (40): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (41): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (42): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (43): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (44): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (45): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (46): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (47): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (48): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (49): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (50): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (51): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (52): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (53): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (54): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (55): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (56): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (57): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (58): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (59): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (60): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (61): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (62): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (63): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (64): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (65): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (66): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (67): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (68): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (69): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (70): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (71): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (72): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (73): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (74): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (75): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (76): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (77): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (78): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (79): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (80): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (81): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (82): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (83): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (84): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (85): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (86): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (87): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (88): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (89): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (90): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (91): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (92): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (93): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (94): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (95): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (96): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (97): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (98): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (99): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (100): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (101): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (102): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (103): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (104): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (105): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (106): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (107): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (108): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (109): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (110): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (111): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (112): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (113): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (114): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (115): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (116): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (117): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (118): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (119): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (120): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (121): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (122): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (123): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (124): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (125): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (126): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (127): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
        )
        (FinalAttentionQKV): FinalAttentionQKV(
          (W_q): Linear(in_features=128, out_features=128, bias=True)
          (W_k): Linear(in_features=128, out_features=128, bias=True)
          (W_v): Linear(in_features=128, out_features=128, bias=True)
          (W_out): Linear(in_features=128, out_features=1, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
        )
        (MultiHeadedAttention): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
            (2): Linear(in_features=128, out_features=128, bias=True)
          )
          (final_linear): Linear(in_features=128, out_features=128, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (SublayerConnection): SublayerConnection(
          (norm): LayerNorm()
          (dropout): Dropout(p=0, inplace=False)
        )
        (PositionwiseFeedForward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0, inplace=False)
        (tanh): Tanh()
        (softmax): Softmax(dim=None)
        (sigmoid): Sigmoid()
        (relu): ReLU()
      )
      (relu): ReLU()
      (tanh): Tanh()
      (sigmoid): Sigmoid()
      (dropout): Dropout(p=0.5, inplace=False)
      (weight1): Linear(in_features=128, out_features=1, bias=True)
      (weight2): Linear(in_features=128, out_features=1, bias=True)
      (GCN): GraphConvolution()
      (GCN_2): GraphConvolution()
      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (procedures): GRASPLayer(
      (backbone): ConCareLayer(
        (PositionalEncoding): PositionalEncoding(
          (dropout): Dropout(p=0, inplace=False)
        )
        (GRUs): ModuleList(
          (0): GRU(1, 128, batch_first=True)
          (1): GRU(1, 128, batch_first=True)
          (2): GRU(1, 128, batch_first=True)
          (3): GRU(1, 128, batch_first=True)
          (4): GRU(1, 128, batch_first=True)
          (5): GRU(1, 128, batch_first=True)
          (6): GRU(1, 128, batch_first=True)
          (7): GRU(1, 128, batch_first=True)
          (8): GRU(1, 128, batch_first=True)
          (9): GRU(1, 128, batch_first=True)
          (10): GRU(1, 128, batch_first=True)
          (11): GRU(1, 128, batch_first=True)
          (12): GRU(1, 128, batch_first=True)
          (13): GRU(1, 128, batch_first=True)
          (14): GRU(1, 128, batch_first=True)
          (15): GRU(1, 128, batch_first=True)
          (16): GRU(1, 128, batch_first=True)
          (17): GRU(1, 128, batch_first=True)
          (18): GRU(1, 128, batch_first=True)
          (19): GRU(1, 128, batch_first=True)
          (20): GRU(1, 128, batch_first=True)
          (21): GRU(1, 128, batch_first=True)
          (22): GRU(1, 128, batch_first=True)
          (23): GRU(1, 128, batch_first=True)
          (24): GRU(1, 128, batch_first=True)
          (25): GRU(1, 128, batch_first=True)
          (26): GRU(1, 128, batch_first=True)
          (27): GRU(1, 128, batch_first=True)
          (28): GRU(1, 128, batch_first=True)
          (29): GRU(1, 128, batch_first=True)
          (30): GRU(1, 128, batch_first=True)
          (31): GRU(1, 128, batch_first=True)
          (32): GRU(1, 128, batch_first=True)
          (33): GRU(1, 128, batch_first=True)
          (34): GRU(1, 128, batch_first=True)
          (35): GRU(1, 128, batch_first=True)
          (36): GRU(1, 128, batch_first=True)
          (37): GRU(1, 128, batch_first=True)
          (38): GRU(1, 128, batch_first=True)
          (39): GRU(1, 128, batch_first=True)
          (40): GRU(1, 128, batch_first=True)
          (41): GRU(1, 128, batch_first=True)
          (42): GRU(1, 128, batch_first=True)
          (43): GRU(1, 128, batch_first=True)
          (44): GRU(1, 128, batch_first=True)
          (45): GRU(1, 128, batch_first=True)
          (46): GRU(1, 128, batch_first=True)
          (47): GRU(1, 128, batch_first=True)
          (48): GRU(1, 128, batch_first=True)
          (49): GRU(1, 128, batch_first=True)
          (50): GRU(1, 128, batch_first=True)
          (51): GRU(1, 128, batch_first=True)
          (52): GRU(1, 128, batch_first=True)
          (53): GRU(1, 128, batch_first=True)
          (54): GRU(1, 128, batch_first=True)
          (55): GRU(1, 128, batch_first=True)
          (56): GRU(1, 128, batch_first=True)
          (57): GRU(1, 128, batch_first=True)
          (58): GRU(1, 128, batch_first=True)
          (59): GRU(1, 128, batch_first=True)
          (60): GRU(1, 128, batch_first=True)
          (61): GRU(1, 128, batch_first=True)
          (62): GRU(1, 128, batch_first=True)
          (63): GRU(1, 128, batch_first=True)
          (64): GRU(1, 128, batch_first=True)
          (65): GRU(1, 128, batch_first=True)
          (66): GRU(1, 128, batch_first=True)
          (67): GRU(1, 128, batch_first=True)
          (68): GRU(1, 128, batch_first=True)
          (69): GRU(1, 128, batch_first=True)
          (70): GRU(1, 128, batch_first=True)
          (71): GRU(1, 128, batch_first=True)
          (72): GRU(1, 128, batch_first=True)
          (73): GRU(1, 128, batch_first=True)
          (74): GRU(1, 128, batch_first=True)
          (75): GRU(1, 128, batch_first=True)
          (76): GRU(1, 128, batch_first=True)
          (77): GRU(1, 128, batch_first=True)
          (78): GRU(1, 128, batch_first=True)
          (79): GRU(1, 128, batch_first=True)
          (80): GRU(1, 128, batch_first=True)
          (81): GRU(1, 128, batch_first=True)
          (82): GRU(1, 128, batch_first=True)
          (83): GRU(1, 128, batch_first=True)
          (84): GRU(1, 128, batch_first=True)
          (85): GRU(1, 128, batch_first=True)
          (86): GRU(1, 128, batch_first=True)
          (87): GRU(1, 128, batch_first=True)
          (88): GRU(1, 128, batch_first=True)
          (89): GRU(1, 128, batch_first=True)
          (90): GRU(1, 128, batch_first=True)
          (91): GRU(1, 128, batch_first=True)
          (92): GRU(1, 128, batch_first=True)
          (93): GRU(1, 128, batch_first=True)
          (94): GRU(1, 128, batch_first=True)
          (95): GRU(1, 128, batch_first=True)
          (96): GRU(1, 128, batch_first=True)
          (97): GRU(1, 128, batch_first=True)
          (98): GRU(1, 128, batch_first=True)
          (99): GRU(1, 128, batch_first=True)
          (100): GRU(1, 128, batch_first=True)
          (101): GRU(1, 128, batch_first=True)
          (102): GRU(1, 128, batch_first=True)
          (103): GRU(1, 128, batch_first=True)
          (104): GRU(1, 128, batch_first=True)
          (105): GRU(1, 128, batch_first=True)
          (106): GRU(1, 128, batch_first=True)
          (107): GRU(1, 128, batch_first=True)
          (108): GRU(1, 128, batch_first=True)
          (109): GRU(1, 128, batch_first=True)
          (110): GRU(1, 128, batch_first=True)
          (111): GRU(1, 128, batch_first=True)
          (112): GRU(1, 128, batch_first=True)
          (113): GRU(1, 128, batch_first=True)
          (114): GRU(1, 128, batch_first=True)
          (115): GRU(1, 128, batch_first=True)
          (116): GRU(1, 128, batch_first=True)
          (117): GRU(1, 128, batch_first=True)
          (118): GRU(1, 128, batch_first=True)
          (119): GRU(1, 128, batch_first=True)
          (120): GRU(1, 128, batch_first=True)
          (121): GRU(1, 128, batch_first=True)
          (122): GRU(1, 128, batch_first=True)
          (123): GRU(1, 128, batch_first=True)
          (124): GRU(1, 128, batch_first=True)
          (125): GRU(1, 128, batch_first=True)
          (126): GRU(1, 128, batch_first=True)
          (127): GRU(1, 128, batch_first=True)
        )
        (LastStepAttentions): ModuleList(
          (0): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (1): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (2): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (3): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (4): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (5): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (6): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (7): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (8): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (9): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (10): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (11): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (12): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (13): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (14): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (15): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (16): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (17): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (18): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (19): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (20): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (21): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (22): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (23): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (24): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (25): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (26): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (27): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (28): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (29): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (30): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (31): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (32): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (33): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (34): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (35): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (36): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (37): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (38): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (39): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (40): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (41): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (42): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (43): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (44): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (45): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (46): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (47): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (48): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (49): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (50): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (51): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (52): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (53): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (54): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (55): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (56): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (57): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (58): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (59): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (60): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (61): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (62): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (63): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (64): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (65): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (66): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (67): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (68): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (69): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (70): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (71): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (72): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (73): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (74): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (75): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (76): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (77): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (78): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (79): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (80): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (81): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (82): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (83): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (84): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (85): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (86): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (87): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (88): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (89): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (90): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (91): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (92): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (93): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (94): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (95): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (96): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (97): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (98): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (99): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (100): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (101): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (102): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (103): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (104): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (105): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (106): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (107): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (108): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (109): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (110): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (111): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (112): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (113): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (114): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (115): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (116): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (117): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (118): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (119): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (120): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (121): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (122): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (123): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (124): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (125): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (126): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
          (127): SingleAttention(
            (tanh): Tanh()
            (softmax): Softmax(dim=1)
            (sigmoid): Sigmoid()
            (relu): ReLU()
          )
        )
        (FinalAttentionQKV): FinalAttentionQKV(
          (W_q): Linear(in_features=128, out_features=128, bias=True)
          (W_k): Linear(in_features=128, out_features=128, bias=True)
          (W_v): Linear(in_features=128, out_features=128, bias=True)
          (W_out): Linear(in_features=128, out_features=1, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (tanh): Tanh()
          (softmax): Softmax(dim=1)
          (sigmoid): Sigmoid()
        )
        (MultiHeadedAttention): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
            (2): Linear(in_features=128, out_features=128, bias=True)
          )
          (final_linear): Linear(in_features=128, out_features=128, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (SublayerConnection): SublayerConnection(
          (norm): LayerNorm()
          (dropout): Dropout(p=0, inplace=False)
        )
        (PositionwiseFeedForward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=64, bias=True)
          (w_2): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0, inplace=False)
        (tanh): Tanh()
        (softmax): Softmax(dim=None)
        (sigmoid): Sigmoid()
        (relu): ReLU()
      )
      (relu): ReLU()
      (tanh): Tanh()
      (sigmoid): Sigmoid()
      (dropout): Dropout(p=0.5, inplace=False)
      (weight1): Linear(in_features=128, out_features=1, bias=True)
      (weight2): Linear(in_features=128, out_features=1, bias=True)
      (GCN): GraphConvolution()
      (GCN_2): GraphConvolution()
      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (fc): Linear(in_features=256, out_features=10, bias=True)
)
2023-07-06 02:45:03 Metrics: ['accuracy', 'f1_macro', 'roc_auc_weighted_ovo']
2023-07-06 02:45:03 Device: cuda
2023-07-06 02:45:03 
2023-07-06 02:45:03 Training:
2023-07-06 02:45:03 Batch size: 32
2023-07-06 02:45:03 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-06 02:45:03 Optimizer params: {'lr': 0.001}
2023-07-06 02:45:03 Weight decay: 0.0
2023-07-06 02:45:03 Max grad norm: None
2023-07-06 02:45:03 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd721b717d0>
2023-07-06 02:45:03 Monitor: accuracy
2023-07-06 02:45:03 Monitor criterion: max
2023-07-06 02:45:03 Epochs: 20
2023-07-06 02:45:03 
2023-07-06 03:00:03 --- Train epoch-0, step-1111 ---
2023-07-06 03:00:03 loss: 1.7538
2023-07-06 03:00:41 --- Eval epoch-0, step-1111 ---
2023-07-06 03:00:41 accuracy: 0.3913
2023-07-06 03:00:41 f1_macro: 0.1702
2023-07-06 03:00:41 roc_auc_weighted_ovo: 0.7780
2023-07-06 03:00:41 loss: 1.6500
2023-07-06 03:00:41 New best accuracy score (0.3913) at epoch-0, step-1111
2023-07-06 03:00:41 
2023-07-06 03:15:25 --- Train epoch-1, step-2222 ---
2023-07-06 03:15:25 loss: 1.6256
2023-07-06 03:16:01 --- Eval epoch-1, step-2222 ---
2023-07-06 03:16:01 accuracy: 0.3945
2023-07-06 03:16:01 f1_macro: 0.2066
2023-07-06 03:16:01 roc_auc_weighted_ovo: 0.7760
2023-07-06 03:16:01 loss: 1.6359
2023-07-06 03:16:01 New best accuracy score (0.3945) at epoch-1, step-2222
2023-07-06 03:16:01 
2023-07-06 03:30:40 --- Train epoch-2, step-3333 ---
2023-07-06 03:30:40 loss: 1.5879
2023-07-06 03:31:13 --- Eval epoch-2, step-3333 ---
2023-07-06 03:31:13 accuracy: 0.4044
2023-07-06 03:31:13 f1_macro: 0.2244
2023-07-06 03:31:13 roc_auc_weighted_ovo: 0.7936
2023-07-06 03:31:13 loss: 1.6095
2023-07-06 03:31:13 New best accuracy score (0.4044) at epoch-2, step-3333
2023-07-06 03:31:13 
2023-07-06 03:45:56 --- Train epoch-3, step-4444 ---
2023-07-06 03:45:56 loss: 1.5519
2023-07-06 03:46:34 --- Eval epoch-3, step-4444 ---
2023-07-06 03:46:34 accuracy: 0.4033
2023-07-06 03:46:34 f1_macro: 0.2279
2023-07-06 03:46:34 roc_auc_weighted_ovo: 0.7927
2023-07-06 03:46:34 loss: 1.6149
2023-07-06 03:46:34 
2023-07-06 04:01:05 --- Train epoch-4, step-5555 ---
2023-07-06 04:01:05 loss: 1.5506
2023-07-06 04:01:45 --- Eval epoch-4, step-5555 ---
2023-07-06 04:01:45 accuracy: 0.3807
2023-07-06 04:01:45 f1_macro: 0.1969
2023-07-06 04:01:45 roc_auc_weighted_ovo: 0.7588
2023-07-06 04:01:45 loss: 1.7334
2023-07-06 04:01:46 
2023-07-06 04:16:28 --- Train epoch-5, step-6666 ---
2023-07-06 04:16:28 loss: 1.5842
2023-07-06 04:17:02 --- Eval epoch-5, step-6666 ---
2023-07-06 04:17:02 accuracy: 0.3431
2023-07-06 04:17:02 f1_macro: 0.1926
2023-07-06 04:17:02 roc_auc_weighted_ovo: 0.7571
2023-07-06 04:17:02 loss: 1.8145
2023-07-06 04:17:02 
2023-07-06 04:31:41 --- Train epoch-6, step-7777 ---
2023-07-06 04:31:41 loss: 1.6424
2023-07-06 04:32:18 --- Eval epoch-6, step-7777 ---
2023-07-06 04:32:18 accuracy: 0.4051
2023-07-06 04:32:18 f1_macro: 0.2416
2023-07-06 04:32:18 roc_auc_weighted_ovo: 0.7907
2023-07-06 04:32:18 loss: 1.6202
2023-07-06 04:32:18 New best accuracy score (0.4051) at epoch-6, step-7777
2023-07-06 04:32:18 
2023-07-06 04:46:50 --- Train epoch-7, step-8888 ---
2023-07-06 04:46:50 loss: 1.5620
2023-07-06 04:47:28 --- Eval epoch-7, step-8888 ---
2023-07-06 04:47:28 accuracy: 0.3888
2023-07-06 04:47:28 f1_macro: 0.2325
2023-07-06 04:47:28 roc_auc_weighted_ovo: 0.7720
2023-07-06 04:47:28 loss: 1.7068
2023-07-06 04:47:28 
2023-07-06 05:02:03 --- Train epoch-8, step-9999 ---
2023-07-06 05:02:03 loss: 1.6036
2023-07-06 05:02:38 --- Eval epoch-8, step-9999 ---
2023-07-06 05:02:38 accuracy: 0.3526
2023-07-06 05:02:38 f1_macro: 0.1651
2023-07-06 05:02:38 roc_auc_weighted_ovo: 0.7104
2023-07-06 05:02:38 loss: 1.8437
2023-07-06 05:02:38 
2023-07-06 05:17:10 --- Train epoch-9, step-11110 ---
2023-07-06 05:17:10 loss: 1.6664
2023-07-06 05:17:46 --- Eval epoch-9, step-11110 ---
2023-07-06 05:17:46 accuracy: 0.3705
2023-07-06 05:17:46 f1_macro: 0.1859
2023-07-06 05:17:46 roc_auc_weighted_ovo: 0.7396
2023-07-06 05:17:46 loss: 1.7646
2023-07-06 05:17:46 
2023-07-06 05:33:05 --- Train epoch-10, step-12221 ---
2023-07-06 05:33:05 loss: 1.6209
2023-07-06 05:33:41 --- Eval epoch-10, step-12221 ---
2023-07-06 05:33:41 accuracy: 0.3528
2023-07-06 05:33:41 f1_macro: 0.1701
2023-07-06 05:33:41 roc_auc_weighted_ovo: 0.7131
2023-07-06 05:33:41 loss: 1.9135
2023-07-06 05:33:41 
2023-07-06 05:48:18 --- Train epoch-11, step-13332 ---
2023-07-06 05:48:18 loss: 1.5471
2023-07-06 05:49:01 --- Eval epoch-11, step-13332 ---
2023-07-06 05:49:01 accuracy: 0.4008
2023-07-06 05:49:01 f1_macro: 0.2328
2023-07-06 05:49:01 roc_auc_weighted_ovo: 0.7752
2023-07-06 05:49:01 loss: 1.6856
2023-07-06 05:49:01 
2023-07-06 06:03:39 --- Train epoch-12, step-14443 ---
2023-07-06 06:03:39 loss: 1.4753
2023-07-06 06:04:14 --- Eval epoch-12, step-14443 ---
2023-07-06 06:04:14 accuracy: 0.4019
2023-07-06 06:04:14 f1_macro: 0.2225
2023-07-06 06:04:14 roc_auc_weighted_ovo: 0.7748
2023-07-06 06:04:14 loss: 1.6617
2023-07-06 06:04:14 
2023-07-06 06:18:42 --- Train epoch-13, step-15554 ---
2023-07-06 06:18:42 loss: 1.4764
2023-07-06 06:19:21 --- Eval epoch-13, step-15554 ---
2023-07-06 06:19:21 accuracy: 0.3750
2023-07-06 06:19:21 f1_macro: 0.2051
2023-07-06 06:19:21 roc_auc_weighted_ovo: 0.7778
2023-07-06 06:19:21 loss: 1.7883
2023-07-06 06:19:21 
2023-07-06 06:33:53 --- Train epoch-14, step-16665 ---
2023-07-06 06:33:53 loss: 1.7235
2023-07-06 06:34:32 --- Eval epoch-14, step-16665 ---
2023-07-06 06:34:32 accuracy: 0.3211
2023-07-06 06:34:32 f1_macro: 0.0951
2023-07-06 06:34:32 roc_auc_weighted_ovo: 0.6934
2023-07-06 06:34:32 loss: 1.9038
2023-07-06 06:34:32 
2023-07-06 06:49:16 --- Train epoch-15, step-17776 ---
2023-07-06 06:49:16 loss: 1.9776
2023-07-06 06:49:57 --- Eval epoch-15, step-17776 ---
2023-07-06 06:49:57 accuracy: 0.2711
2023-07-06 06:49:57 f1_macro: 0.0861
2023-07-06 06:49:57 roc_auc_weighted_ovo: 0.5532
2023-07-06 06:49:57 loss: 2.0531
2023-07-06 06:49:57 
2023-07-06 07:04:33 --- Train epoch-16, step-18887 ---
2023-07-06 07:04:33 loss: 2.0203
2023-07-06 07:05:08 --- Eval epoch-16, step-18887 ---
2023-07-06 07:05:08 accuracy: 0.2745
2023-07-06 07:05:08 f1_macro: 0.0930
2023-07-06 07:05:08 roc_auc_weighted_ovo: 0.5490
2023-07-06 07:05:08 loss: 2.0417
2023-07-06 07:05:08 
2023-07-06 07:19:58 --- Train epoch-17, step-19998 ---
2023-07-06 07:19:58 loss: 2.0377
2023-07-06 07:20:33 --- Eval epoch-17, step-19998 ---
2023-07-06 07:20:33 accuracy: 0.2749
2023-07-06 07:20:33 f1_macro: 0.1207
2023-07-06 07:20:33 roc_auc_weighted_ovo: 0.5980
2023-07-06 07:20:33 loss: 2.0349
2023-07-06 07:20:33 
2023-07-06 07:35:19 --- Train epoch-18, step-21109 ---
2023-07-06 07:35:19 loss: 1.9862
2023-07-06 07:35:56 --- Eval epoch-18, step-21109 ---
2023-07-06 07:35:56 accuracy: 0.2767
2023-07-06 07:35:56 f1_macro: 0.1213
2023-07-06 07:35:56 roc_auc_weighted_ovo: 0.6564
2023-07-06 07:35:56 loss: 1.9826
2023-07-06 07:35:56 
2023-07-06 07:50:45 --- Train epoch-19, step-22220 ---
2023-07-06 07:50:45 loss: 1.7579
2023-07-06 07:51:21 --- Eval epoch-19, step-22220 ---
2023-07-06 07:51:21 accuracy: 0.3816
2023-07-06 07:51:21 f1_macro: 0.2078
2023-07-06 07:51:21 roc_auc_weighted_ovo: 0.7657
2023-07-06 07:51:21 loss: 1.7120
2023-07-06 07:51:21 Loaded best model
2023-07-06 07:51:23 Deepr(
  (embeddings): ModuleDict(
    (conditions): Embedding(4032, 128, padding_idx=0)
    (procedures): Embedding(1277, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (cnn): ModuleDict(
    (conditions): DeeprLayer(
      (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,))
    )
    (procedures): DeeprLayer(
      (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,))
    )
  )
  (fc): Linear(in_features=256, out_features=1, bias=True)
)
2023-07-06 07:51:23 Metrics: ['accuracy', 'pr_auc', 'roc_auc', 'f1']
2023-07-06 07:51:23 Device: cuda
2023-07-06 07:51:23 
2023-07-06 07:51:23 Training:
2023-07-06 07:51:23 Batch size: 32
2023-07-06 07:51:23 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-06 07:51:23 Optimizer params: {'lr': 0.001}
2023-07-06 07:51:23 Weight decay: 0.0
2023-07-06 07:51:23 Max grad norm: None
2023-07-06 07:51:23 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd711cf6150>
2023-07-06 07:51:23 Monitor: roc_auc
2023-07-06 07:51:23 Monitor criterion: max
2023-07-06 07:51:23 Epochs: 20
2023-07-06 07:51:23 
2023-07-06 07:51:24 --- Train epoch-0, step-242 ---
2023-07-06 07:51:24 loss: 0.6773
2023-07-06 07:51:24 --- Eval epoch-0, step-242 ---
2023-07-06 07:51:24 accuracy: 0.6138
2023-07-06 07:51:24 pr_auc: 0.6786
2023-07-06 07:51:24 roc_auc: 0.6479
2023-07-06 07:51:24 f1: 0.6934
2023-07-06 07:51:24 loss: 0.6536
2023-07-06 07:51:24 New best roc_auc score (0.6479) at epoch-0, step-242
2023-07-06 07:51:24 
2023-07-06 07:51:25 --- Train epoch-1, step-484 ---
2023-07-06 07:51:25 loss: 0.5690
2023-07-06 07:51:26 --- Eval epoch-1, step-484 ---
2023-07-06 07:51:26 accuracy: 0.6057
2023-07-06 07:51:26 pr_auc: 0.6829
2023-07-06 07:51:26 roc_auc: 0.6439
2023-07-06 07:51:26 f1: 0.6667
2023-07-06 07:51:26 loss: 0.6620
2023-07-06 07:51:26 
2023-07-06 07:51:27 --- Train epoch-2, step-726 ---
2023-07-06 07:51:27 loss: 0.4399
2023-07-06 07:51:27 --- Eval epoch-2, step-726 ---
2023-07-06 07:51:27 accuracy: 0.5642
2023-07-06 07:51:27 pr_auc: 0.6552
2023-07-06 07:51:27 roc_auc: 0.6155
2023-07-06 07:51:27 f1: 0.5976
2023-07-06 07:51:27 loss: 0.7202
2023-07-06 07:51:27 
2023-07-06 07:51:28 --- Train epoch-3, step-968 ---
2023-07-06 07:51:28 loss: 0.2818
2023-07-06 07:51:28 --- Eval epoch-3, step-968 ---
2023-07-06 07:51:28 accuracy: 0.5844
2023-07-06 07:51:28 pr_auc: 0.6477
2023-07-06 07:51:28 roc_auc: 0.6123
2023-07-06 07:51:28 f1: 0.6327
2023-07-06 07:51:28 loss: 0.7750
2023-07-06 07:51:28 
2023-07-06 07:51:29 --- Train epoch-4, step-1210 ---
2023-07-06 07:51:29 loss: 0.1473
2023-07-06 07:51:29 --- Eval epoch-4, step-1210 ---
2023-07-06 07:51:29 accuracy: 0.5895
2023-07-06 07:51:29 pr_auc: 0.6519
2023-07-06 07:51:29 roc_auc: 0.6103
2023-07-06 07:51:29 f1: 0.6721
2023-07-06 07:51:29 loss: 0.8825
2023-07-06 07:51:29 
2023-07-06 07:51:30 --- Train epoch-5, step-1452 ---
2023-07-06 07:51:30 loss: 0.0714
2023-07-06 07:51:30 --- Eval epoch-5, step-1452 ---
2023-07-06 07:51:30 accuracy: 0.5885
2023-07-06 07:51:30 pr_auc: 0.6510
2023-07-06 07:51:30 roc_auc: 0.6120
2023-07-06 07:51:30 f1: 0.6470
2023-07-06 07:51:30 loss: 0.9330
2023-07-06 07:51:30 
2023-07-06 07:51:31 --- Train epoch-6, step-1694 ---
2023-07-06 07:51:31 loss: 0.0350
2023-07-06 07:51:31 --- Eval epoch-6, step-1694 ---
2023-07-06 07:51:31 accuracy: 0.5814
2023-07-06 07:51:31 pr_auc: 0.6533
2023-07-06 07:51:31 roc_auc: 0.6117
2023-07-06 07:51:31 f1: 0.6368
2023-07-06 07:51:31 loss: 0.9924
2023-07-06 07:51:31 
2023-07-06 07:51:32 --- Train epoch-7, step-1936 ---
2023-07-06 07:51:32 loss: 0.0201
2023-07-06 07:51:32 --- Eval epoch-7, step-1936 ---
2023-07-06 07:51:32 accuracy: 0.5774
2023-07-06 07:51:32 pr_auc: 0.6479
2023-07-06 07:51:32 roc_auc: 0.6056
2023-07-06 07:51:32 f1: 0.6409
2023-07-06 07:51:32 loss: 1.0571
2023-07-06 07:51:32 
2023-07-06 07:51:33 --- Train epoch-8, step-2178 ---
2023-07-06 07:51:33 loss: 0.0130
2023-07-06 07:51:33 --- Eval epoch-8, step-2178 ---
2023-07-06 07:51:33 accuracy: 0.5794
2023-07-06 07:51:33 pr_auc: 0.6491
2023-07-06 07:51:33 roc_auc: 0.6071
2023-07-06 07:51:33 f1: 0.6432
2023-07-06 07:51:33 loss: 1.1149
2023-07-06 07:51:33 
2023-07-06 07:51:34 --- Train epoch-9, step-2420 ---
2023-07-06 07:51:34 loss: 0.0092
2023-07-06 07:51:34 --- Eval epoch-9, step-2420 ---
2023-07-06 07:51:34 accuracy: 0.5743
2023-07-06 07:51:34 pr_auc: 0.6509
2023-07-06 07:51:34 roc_auc: 0.6080
2023-07-06 07:51:34 f1: 0.6405
2023-07-06 07:51:34 loss: 1.1648
2023-07-06 07:51:34 
2023-07-06 07:51:35 --- Train epoch-10, step-2662 ---
2023-07-06 07:51:35 loss: 0.0067
2023-07-06 07:51:35 --- Eval epoch-10, step-2662 ---
2023-07-06 07:51:35 accuracy: 0.5784
2023-07-06 07:51:35 pr_auc: 0.6483
2023-07-06 07:51:35 roc_auc: 0.6051
2023-07-06 07:51:35 f1: 0.6326
2023-07-06 07:51:35 loss: 1.1900
2023-07-06 07:51:35 
2023-07-06 07:51:37 --- Train epoch-11, step-2904 ---
2023-07-06 07:51:37 loss: 0.0054
2023-07-06 07:51:37 --- Eval epoch-11, step-2904 ---
2023-07-06 07:51:37 accuracy: 0.5794
2023-07-06 07:51:37 pr_auc: 0.6492
2023-07-06 07:51:37 roc_auc: 0.6065
2023-07-06 07:51:37 f1: 0.6426
2023-07-06 07:51:37 loss: 1.2441
2023-07-06 07:51:37 
2023-07-06 07:51:38 --- Train epoch-12, step-3146 ---
2023-07-06 07:51:38 loss: 0.0043
2023-07-06 07:51:38 --- Eval epoch-12, step-3146 ---
2023-07-06 07:51:38 accuracy: 0.5814
2023-07-06 07:51:38 pr_auc: 0.6508
2023-07-06 07:51:38 roc_auc: 0.6089
2023-07-06 07:51:38 f1: 0.6455
2023-07-06 07:51:38 loss: 1.2705
2023-07-06 07:51:38 
2023-07-06 07:51:39 --- Train epoch-13, step-3388 ---
2023-07-06 07:51:39 loss: 0.0036
2023-07-06 07:51:39 --- Eval epoch-13, step-3388 ---
2023-07-06 07:51:39 accuracy: 0.5854
2023-07-06 07:51:39 pr_auc: 0.6483
2023-07-06 07:51:39 roc_auc: 0.6065
2023-07-06 07:51:39 f1: 0.6435
2023-07-06 07:51:39 loss: 1.3012
2023-07-06 07:51:39 
2023-07-06 07:51:40 --- Train epoch-14, step-3630 ---
2023-07-06 07:51:40 loss: 0.0031
2023-07-06 07:51:40 --- Eval epoch-14, step-3630 ---
2023-07-06 07:51:40 accuracy: 0.5824
2023-07-06 07:51:40 pr_auc: 0.6469
2023-07-06 07:51:40 roc_auc: 0.6045
2023-07-06 07:51:40 f1: 0.6418
2023-07-06 07:51:40 loss: 1.3444
2023-07-06 07:51:40 
2023-07-06 07:51:41 --- Train epoch-15, step-3872 ---
2023-07-06 07:51:41 loss: 0.0030
2023-07-06 07:51:41 --- Eval epoch-15, step-3872 ---
2023-07-06 07:51:41 accuracy: 0.5935
2023-07-06 07:51:41 pr_auc: 0.6531
2023-07-06 07:51:41 roc_auc: 0.6093
2023-07-06 07:51:41 f1: 0.6582
2023-07-06 07:51:41 loss: 1.4268
2023-07-06 07:51:41 
2023-07-06 07:51:42 --- Train epoch-16, step-4114 ---
2023-07-06 07:51:42 loss: 0.0259
2023-07-06 07:51:42 --- Eval epoch-16, step-4114 ---
2023-07-06 07:51:42 accuracy: 0.5865
2023-07-06 07:51:42 pr_auc: 0.6286
2023-07-06 07:51:42 roc_auc: 0.5904
2023-07-06 07:51:42 f1: 0.6725
2023-07-06 07:51:42 loss: 1.8691
2023-07-06 07:51:42 
2023-07-06 07:51:43 --- Train epoch-17, step-4356 ---
2023-07-06 07:51:43 loss: 0.1096
2023-07-06 07:51:43 --- Eval epoch-17, step-4356 ---
2023-07-06 07:51:43 accuracy: 0.5723
2023-07-06 07:51:43 pr_auc: 0.6446
2023-07-06 07:51:43 roc_auc: 0.5937
2023-07-06 07:51:43 f1: 0.6306
2023-07-06 07:51:43 loss: 1.6336
2023-07-06 07:51:44 
2023-07-06 07:51:45 --- Train epoch-18, step-4598 ---
2023-07-06 07:51:45 loss: 0.0202
2023-07-06 07:51:45 --- Eval epoch-18, step-4598 ---
2023-07-06 07:51:45 accuracy: 0.5784
2023-07-06 07:51:45 pr_auc: 0.6428
2023-07-06 07:51:45 roc_auc: 0.6023
2023-07-06 07:51:45 f1: 0.6287
2023-07-06 07:51:45 loss: 1.6458
2023-07-06 07:51:45 
2023-07-06 07:51:46 --- Train epoch-19, step-4840 ---
2023-07-06 07:51:46 loss: 0.0043
2023-07-06 07:51:46 --- Eval epoch-19, step-4840 ---
2023-07-06 07:51:46 accuracy: 0.5834
2023-07-06 07:51:46 pr_auc: 0.6439
2023-07-06 07:51:46 roc_auc: 0.6009
2023-07-06 07:51:46 f1: 0.6373
2023-07-06 07:51:46 loss: 1.6441
2023-07-06 07:51:46 Loaded best model
2023-07-06 07:51:47 Deepr(
  (embeddings): ModuleDict(
    (conditions): Embedding(4032, 128, padding_idx=0)
    (procedures): Embedding(1277, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (cnn): ModuleDict(
    (conditions): DeeprLayer(
      (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,))
    )
    (procedures): DeeprLayer(
      (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,))
    )
  )
  (fc): Linear(in_features=256, out_features=1, bias=True)
)
2023-07-06 07:51:47 Metrics: ['accuracy', 'pr_auc', 'roc_auc', 'f1']
2023-07-06 07:51:47 Device: cuda
2023-07-06 07:51:47 
2023-07-06 07:51:47 Training:
2023-07-06 07:51:47 Batch size: 32
2023-07-06 07:51:47 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-06 07:51:47 Optimizer params: {'lr': 0.001}
2023-07-06 07:51:47 Weight decay: 0.0
2023-07-06 07:51:47 Max grad norm: None
2023-07-06 07:51:47 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd711cc1390>
2023-07-06 07:51:47 Monitor: roc_auc
2023-07-06 07:51:47 Monitor criterion: max
2023-07-06 07:51:47 Epochs: 20
2023-07-06 07:51:47 
2023-07-06 07:51:48 --- Train epoch-0, step-244 ---
2023-07-06 07:51:48 loss: 0.2578
2023-07-06 07:51:48 --- Eval epoch-0, step-244 ---
2023-07-06 07:51:48 accuracy: 0.9249
2023-07-06 07:51:48 pr_auc: 0.0906
2023-07-06 07:51:48 roc_auc: 0.5389
2023-07-06 07:51:48 f1: 0.0000
2023-07-06 07:51:48 loss: 0.2850
2023-07-06 07:51:48 New best roc_auc score (0.5389) at epoch-0, step-244
2023-07-06 07:51:48 
2023-07-06 07:51:49 --- Train epoch-1, step-488 ---
2023-07-06 07:51:49 loss: 0.1920
2023-07-06 07:51:49 --- Eval epoch-1, step-488 ---
2023-07-06 07:51:49 accuracy: 0.9249
2023-07-06 07:51:49 pr_auc: 0.1090
2023-07-06 07:51:49 roc_auc: 0.5327
2023-07-06 07:51:49 f1: 0.0282
2023-07-06 07:51:49 loss: 0.2929
2023-07-06 07:51:49 
2023-07-06 07:51:50 --- Train epoch-2, step-732 ---
2023-07-06 07:51:50 loss: 0.1120
2023-07-06 07:51:50 --- Eval epoch-2, step-732 ---
2023-07-06 07:51:50 accuracy: 0.9217
2023-07-06 07:51:50 pr_auc: 0.1071
2023-07-06 07:51:50 roc_auc: 0.5356
2023-07-06 07:51:50 f1: 0.0526
2023-07-06 07:51:50 loss: 0.3295
2023-07-06 07:51:50 
2023-07-06 07:51:51 --- Train epoch-3, step-976 ---
2023-07-06 07:51:51 loss: 0.0419
2023-07-06 07:51:51 --- Eval epoch-3, step-976 ---
2023-07-06 07:51:51 accuracy: 0.9227
2023-07-06 07:51:51 pr_auc: 0.1223
2023-07-06 07:51:51 roc_auc: 0.5655
2023-07-06 07:51:51 f1: 0.1013
2023-07-06 07:51:51 loss: 0.3501
2023-07-06 07:51:51 New best roc_auc score (0.5655) at epoch-3, step-976
2023-07-06 07:51:51 
2023-07-06 07:51:52 --- Train epoch-4, step-1220 ---
2023-07-06 07:51:52 loss: 0.0136
2023-07-06 07:51:53 --- Eval epoch-4, step-1220 ---
2023-07-06 07:51:53 accuracy: 0.9238
2023-07-06 07:51:53 pr_auc: 0.1176
2023-07-06 07:51:53 roc_auc: 0.5646
2023-07-06 07:51:53 f1: 0.0541
2023-07-06 07:51:53 loss: 0.4195
2023-07-06 07:51:53 
2023-07-06 07:51:53 --- Train epoch-5, step-1464 ---
2023-07-06 07:51:53 loss: 0.0051
2023-07-06 07:51:54 --- Eval epoch-5, step-1464 ---
2023-07-06 07:51:54 accuracy: 0.9238
2023-07-06 07:51:54 pr_auc: 0.1185
2023-07-06 07:51:54 roc_auc: 0.5529
2023-07-06 07:51:54 f1: 0.0278
2023-07-06 07:51:54 loss: 0.4878
2023-07-06 07:51:54 
2023-07-06 07:51:55 --- Train epoch-6, step-1708 ---
2023-07-06 07:51:55 loss: 0.0029
2023-07-06 07:51:55 --- Eval epoch-6, step-1708 ---
2023-07-06 07:51:55 accuracy: 0.9238
2023-07-06 07:51:55 pr_auc: 0.1189
2023-07-06 07:51:55 roc_auc: 0.5567
2023-07-06 07:51:55 f1: 0.0278
2023-07-06 07:51:55 loss: 0.5030
2023-07-06 07:51:55 
2023-07-06 07:51:56 --- Train epoch-7, step-1952 ---
2023-07-06 07:51:56 loss: 0.0019
2023-07-06 07:51:56 --- Eval epoch-7, step-1952 ---
2023-07-06 07:51:56 accuracy: 0.9227
2023-07-06 07:51:56 pr_auc: 0.1143
2023-07-06 07:51:56 roc_auc: 0.5565
2023-07-06 07:51:56 f1: 0.0274
2023-07-06 07:51:56 loss: 0.5271
2023-07-06 07:51:56 
2023-07-06 07:51:57 --- Train epoch-8, step-2196 ---
2023-07-06 07:51:57 loss: 0.0014
2023-07-06 07:51:57 --- Eval epoch-8, step-2196 ---
2023-07-06 07:51:57 accuracy: 0.9227
2023-07-06 07:51:57 pr_auc: 0.1132
2023-07-06 07:51:57 roc_auc: 0.5557
2023-07-06 07:51:57 f1: 0.0274
2023-07-06 07:51:57 loss: 0.5432
2023-07-06 07:51:57 
2023-07-06 07:51:58 --- Train epoch-9, step-2440 ---
2023-07-06 07:51:58 loss: 0.0010
2023-07-06 07:51:58 --- Eval epoch-9, step-2440 ---
2023-07-06 07:51:58 accuracy: 0.9238
2023-07-06 07:51:58 pr_auc: 0.1175
2023-07-06 07:51:58 roc_auc: 0.5568
2023-07-06 07:51:58 f1: 0.0278
2023-07-06 07:51:58 loss: 0.5672
2023-07-06 07:51:58 
2023-07-06 07:51:59 --- Train epoch-10, step-2684 ---
2023-07-06 07:51:59 loss: 0.0008
2023-07-06 07:51:59 --- Eval epoch-10, step-2684 ---
2023-07-06 07:51:59 accuracy: 0.9238
2023-07-06 07:51:59 pr_auc: 0.1166
2023-07-06 07:51:59 roc_auc: 0.5560
2023-07-06 07:51:59 f1: 0.0278
2023-07-06 07:51:59 loss: 0.5865
2023-07-06 07:51:59 
2023-07-06 07:52:00 --- Train epoch-11, step-2928 ---
2023-07-06 07:52:00 loss: 0.0006
2023-07-06 07:52:00 --- Eval epoch-11, step-2928 ---
2023-07-06 07:52:00 accuracy: 0.9238
2023-07-06 07:52:00 pr_auc: 0.1144
2023-07-06 07:52:00 roc_auc: 0.5555
2023-07-06 07:52:00 f1: 0.0278
2023-07-06 07:52:00 loss: 0.5978
2023-07-06 07:52:00 
2023-07-06 07:52:01 --- Train epoch-12, step-3172 ---
2023-07-06 07:52:01 loss: 0.0005
2023-07-06 07:52:01 --- Eval epoch-12, step-3172 ---
2023-07-06 07:52:01 accuracy: 0.9238
2023-07-06 07:52:01 pr_auc: 0.1172
2023-07-06 07:52:01 roc_auc: 0.5567
2023-07-06 07:52:01 f1: 0.0278
2023-07-06 07:52:01 loss: 0.6216
2023-07-06 07:52:01 
2023-07-06 07:52:02 --- Train epoch-13, step-3416 ---
2023-07-06 07:52:02 loss: 0.0004
2023-07-06 07:52:02 --- Eval epoch-13, step-3416 ---
2023-07-06 07:52:02 accuracy: 0.9238
2023-07-06 07:52:02 pr_auc: 0.1151
2023-07-06 07:52:02 roc_auc: 0.5568
2023-07-06 07:52:02 f1: 0.0278
2023-07-06 07:52:02 loss: 0.6295
2023-07-06 07:52:02 
2023-07-06 07:52:03 --- Train epoch-14, step-3660 ---
2023-07-06 07:52:03 loss: 0.0003
2023-07-06 07:52:03 --- Eval epoch-14, step-3660 ---
2023-07-06 07:52:03 accuracy: 0.9238
2023-07-06 07:52:03 pr_auc: 0.1145
2023-07-06 07:52:03 roc_auc: 0.5564
2023-07-06 07:52:03 f1: 0.0278
2023-07-06 07:52:03 loss: 0.6525
2023-07-06 07:52:03 
2023-07-06 07:52:04 --- Train epoch-15, step-3904 ---
2023-07-06 07:52:04 loss: 0.0003
2023-07-06 07:52:04 --- Eval epoch-15, step-3904 ---
2023-07-06 07:52:04 accuracy: 0.9238
2023-07-06 07:52:04 pr_auc: 0.1158
2023-07-06 07:52:04 roc_auc: 0.5566
2023-07-06 07:52:04 f1: 0.0278
2023-07-06 07:52:04 loss: 0.6555
2023-07-06 07:52:04 
2023-07-06 07:52:05 --- Train epoch-16, step-4148 ---
2023-07-06 07:52:05 loss: 0.0002
2023-07-06 07:52:05 --- Eval epoch-16, step-4148 ---
2023-07-06 07:52:05 accuracy: 0.9238
2023-07-06 07:52:05 pr_auc: 0.1158
2023-07-06 07:52:05 roc_auc: 0.5576
2023-07-06 07:52:05 f1: 0.0278
2023-07-06 07:52:05 loss: 0.6701
2023-07-06 07:52:05 
2023-07-06 07:52:06 --- Train epoch-17, step-4392 ---
2023-07-06 07:52:06 loss: 0.0002
2023-07-06 07:52:06 --- Eval epoch-17, step-4392 ---
2023-07-06 07:52:06 accuracy: 0.9227
2023-07-06 07:52:06 pr_auc: 0.1147
2023-07-06 07:52:06 roc_auc: 0.5562
2023-07-06 07:52:06 f1: 0.0000
2023-07-06 07:52:06 loss: 0.7035
2023-07-06 07:52:06 
2023-07-06 07:52:07 --- Train epoch-18, step-4636 ---
2023-07-06 07:52:07 loss: 0.0002
2023-07-06 07:52:07 --- Eval epoch-18, step-4636 ---
2023-07-06 07:52:07 accuracy: 0.9238
2023-07-06 07:52:07 pr_auc: 0.1158
2023-07-06 07:52:07 roc_auc: 0.5568
2023-07-06 07:52:07 f1: 0.0278
2023-07-06 07:52:07 loss: 0.7038
2023-07-06 07:52:07 
2023-07-06 07:52:08 --- Train epoch-19, step-4880 ---
2023-07-06 07:52:08 loss: 0.0001
2023-07-06 07:52:09 --- Eval epoch-19, step-4880 ---
2023-07-06 07:52:09 accuracy: 0.9238
2023-07-06 07:52:09 pr_auc: 0.1168
2023-07-06 07:52:09 roc_auc: 0.5576
2023-07-06 07:52:09 f1: 0.0278
2023-07-06 07:52:09 loss: 0.7109
2023-07-06 07:52:09 Loaded best model
2023-07-06 07:52:16 Deepr(
  (embeddings): ModuleDict(
    (conditions): Embedding(6665, 128, padding_idx=0)
    (procedures): Embedding(1981, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (cnn): ModuleDict(
    (conditions): DeeprLayer(
      (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,))
    )
    (procedures): DeeprLayer(
      (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,))
    )
  )
  (fc): Linear(in_features=256, out_features=10, bias=True)
)
2023-07-06 07:52:16 Metrics: ['accuracy', 'f1_macro', 'roc_auc_weighted_ovo']
2023-07-06 07:52:16 Device: cuda
2023-07-06 07:52:16 
2023-07-06 07:52:16 Training:
2023-07-06 07:52:16 Batch size: 32
2023-07-06 07:52:16 Optimizer: <class 'torch.optim.adam.Adam'>
2023-07-06 07:52:16 Optimizer params: {'lr': 0.001}
2023-07-06 07:52:16 Weight decay: 0.0
2023-07-06 07:52:16 Max grad norm: None
2023-07-06 07:52:16 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fd721f89ed0>
2023-07-06 07:52:16 Monitor: accuracy
2023-07-06 07:52:16 Monitor criterion: max
2023-07-06 07:52:16 Epochs: 20
2023-07-06 07:52:16 
2023-07-06 07:52:21 --- Train epoch-0, step-1111 ---
2023-07-06 07:52:21 loss: 1.7193
2023-07-06 07:52:22 --- Eval epoch-0, step-1111 ---
2023-07-06 07:52:22 accuracy: 0.3912
2023-07-06 07:52:22 f1_macro: 0.2436
2023-07-06 07:52:22 roc_auc_weighted_ovo: 0.7785
2023-07-06 07:52:22 loss: 1.6604
2023-07-06 07:52:22 New best accuracy score (0.3912) at epoch-0, step-1111
2023-07-06 07:52:22 
2023-07-06 07:52:26 --- Train epoch-1, step-2222 ---
2023-07-06 07:52:26 loss: 1.5250
2023-07-06 07:52:27 --- Eval epoch-1, step-2222 ---
2023-07-06 07:52:27 accuracy: 0.4040
2023-07-06 07:52:27 f1_macro: 0.2466
2023-07-06 07:52:27 roc_auc_weighted_ovo: 0.7840
2023-07-06 07:52:27 loss: 1.6548
2023-07-06 07:52:27 New best accuracy score (0.4040) at epoch-1, step-2222
2023-07-06 07:52:27 
2023-07-06 07:52:32 --- Train epoch-2, step-3333 ---
2023-07-06 07:52:32 loss: 1.3666
2023-07-06 07:52:32 --- Eval epoch-2, step-3333 ---
2023-07-06 07:52:32 accuracy: 0.3753
2023-07-06 07:52:32 f1_macro: 0.2740
2023-07-06 07:52:32 roc_auc_weighted_ovo: 0.7762
2023-07-06 07:52:32 loss: 1.7250
2023-07-06 07:52:32 
2023-07-06 07:52:38 --- Train epoch-3, step-4444 ---
2023-07-06 07:52:38 loss: 1.1773
2023-07-06 07:52:38 --- Eval epoch-3, step-4444 ---
2023-07-06 07:52:38 accuracy: 0.3562
2023-07-06 07:52:38 f1_macro: 0.2462
2023-07-06 07:52:38 roc_auc_weighted_ovo: 0.7636
2023-07-06 07:52:38 loss: 1.8816
2023-07-06 07:52:38 
2023-07-06 07:52:44 --- Train epoch-4, step-5555 ---
2023-07-06 07:52:44 loss: 0.9772
2023-07-06 07:52:44 --- Eval epoch-4, step-5555 ---
2023-07-06 07:52:44 accuracy: 0.3451
2023-07-06 07:52:44 f1_macro: 0.2512
2023-07-06 07:52:44 roc_auc_weighted_ovo: 0.7541
2023-07-06 07:52:44 loss: 2.0619
2023-07-06 07:52:44 
