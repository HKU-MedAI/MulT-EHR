#### general settings
name: HGT
train_type: baseline
eval_type: homo-graph
gpu_ids: '0'

#### datasets
datasets:
  dataset_path: "./data/mimic3_objects/mimic3_dpl.pkl"

#### Checkpoint settings
checkpoint:
  path: "./checkpoints/AdaCare/"
  save_checkpoint_freq: 2

#### Optimizer settings
optimizer:
  opt_method: "ADAM"
  lr: 0.0005 # Learning rate
  weight_decay: 0.001

#### training settings: learning rate scheme, loss, optimizer
train:
  num_epochs: 50
  batch_size: 4096
  baseline_name: "AdaCare"
  task: "readm"
